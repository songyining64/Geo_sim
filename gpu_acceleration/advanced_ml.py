"""
é«˜çº§æœºå™¨å­¦ä¹ åŠ é€Ÿæ•°å€¼æ¨¡æ‹Ÿæ¡†æ¶
"""

import numpy as np
import time
import warnings
from typing import Dict, List, Tuple, Optional, Callable, Union, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod

# æ·±åº¦å­¦ä¹ ç›¸å…³ä¾èµ–
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    from torch.utils.data import DataLoader, TensorDataset
    from torch.nn.parameter import Parameter
    HAS_PYTORCH = True
except ImportError:
    HAS_PYTORCH = False
    torch = None
    nn = None
    optim = None
    warnings.warn("PyTorch not available. Advanced ML features will be limited.")

try:
    import sklearn
    from sklearn.gaussian_process import GaussianProcessRegressor
    from sklearn.gaussian_process.kernels import RBF, ConstantKernel, Matern
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.neural_network import MLPRegressor
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    warnings.warn("scikit-learn not available. Advanced ML features will be limited.")

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    warnings.warn("matplotlib not available. Visualization features will be limited.")


@dataclass
class PhysicsConfig:
    """ç‰©ç†é…ç½®ç±»"""
    # åœ°è´¨æ¨¡æ‹Ÿå¸¸ç”¨å‚æ•°
    rayleigh_number: float = 1e6
    prandtl_number: float = 1.0
    gravity: float = 9.81
    thermal_expansion: float = 3e-5
    thermal_diffusivity: float = 1e-6
    reference_density: float = 3300.0
    reference_viscosity: float = 1e21
    
    # è¾¹ç•Œæ¡ä»¶
    boundary_conditions: Dict = None
    
    def __post_init__(self):
        if self.boundary_conditions is None:
            self.boundary_conditions = {
                'temperature': {'top': 0, 'bottom': 1},
                'velocity': {'top': 'free_slip', 'bottom': 'free_slip'},
                'pressure': {'top': 'free', 'bottom': 'free'}
            }


class BaseSolver(ABC):
    """åŸºç¡€æ±‚è§£å™¨æŠ½è±¡ç±» - å®ç°ä»£ç å¤ç”¨"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.is_trained = False
        self.training_history = {}
    
    @abstractmethod
    def train(self, X: np.ndarray, y: np.ndarray, **kwargs) -> dict:
        """è®­ç»ƒæ¨¡å‹"""
        pass
    
    @abstractmethod
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        pass
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        try:
            if hasattr(self, 'state_dict'):
                torch.save(self.state_dict(), filepath)
                print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜: {filepath}")
            else:
                import pickle
                with open(filepath, 'wb') as f:
                    pickle.dump(self, f)
                print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜: {filepath}")
        except Exception as e:
            raise RuntimeError(f"ä¿å­˜æ¨¡å‹å¤±è´¥ï¼š{str(e)}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        try:
            if hasattr(self, 'load_state_dict'):
                self.load_state_dict(torch.load(filepath, map_location=self.device))
                print(f"ğŸ“ æ¨¡å‹å·²åŠ è½½: {filepath}")
            else:
                import pickle
                with open(filepath, 'rb') as f:
                    loaded_model = pickle.load(f)
                    self.__dict__.update(loaded_model.__dict__)
                print(f"ğŸ“ æ¨¡å‹å·²åŠ è½½: {filepath}")
        except FileNotFoundError:
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼š{filepath}")
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æ¨¡å‹å¤±è´¥ï¼š{str(e)}")


class PhysicsInformedNeuralNetwork(BaseSolver, nn.Module):
    """
    ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNï¼‰ - é«˜çº§ç‰ˆæœ¬
    
    æ ¸å¿ƒæ€æƒ³ï¼šå°†ç‰©ç†æ–¹ç¨‹ä½œä¸ºè½¯çº¦æŸåµŒå…¥ç¥ç»ç½‘ç»œçš„æŸå¤±å‡½æ•°ï¼Œ
    å¼ºåˆ¶æ¨¡å‹è¾“å‡ºæ»¡è¶³ç‰©ç†è§„å¾‹ï¼Œå®ç°"å°æ•°æ®+ç‰©ç†çŸ¥è¯†"çš„è”åˆå­¦ä¹ 
    """
    
    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int,
                 physics_equations: List[Callable] = None,
                 boundary_conditions: List[Callable] = None,
                 physics_config: PhysicsConfig = None):
        BaseSolver.__init__(self)
        nn.Module.__init__(self)
        
        if not HAS_PYTORCH:
            raise ImportError("éœ€è¦å®‰è£…PyTorchæ¥ä½¿ç”¨PINN")
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.output_dim = output_dim
        self.physics_equations = physics_equations or []
        self.boundary_conditions = boundary_conditions or []
        self.physics_config = physics_config or PhysicsConfig()
        
        # æ„å»ºç½‘ç»œ - ä½¿ç”¨æ®‹å·®è¿æ¥å’Œæ‰¹å½’ä¸€åŒ–
        self.layers = nn.ModuleList()
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            self.layers.append(nn.Linear(prev_dim, hidden_dim))
            self.layers.append(nn.BatchNorm1d(hidden_dim))
            self.layers.append(nn.Tanh())  # PINNé€šå¸¸ä½¿ç”¨Tanhæ¿€æ´»å‡½æ•°
            prev_dim = hidden_dim
        
        self.output_layer = nn.Linear(prev_dim, output_dim)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
        self.optimizer = None
        self.to(self.device)
        
        print(f"ğŸ”„ é«˜çº§PINNåˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}")
        print(f"   ç½‘ç»œç»“æ„: {input_dim} -> {hidden_dims} -> {output_dim}")
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for layer in self.layers:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_normal_(layer.weight)
                nn.init.constant_(layer.bias, 0)
        nn.init.xavier_normal_(self.output_layer.weight)
        nn.init.constant_(self.output_layer.bias, 0)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        for layer in self.layers:
            x = layer(x)
        return self.output_layer(x)
    
    def setup_training(self, learning_rate: float = 0.001, weight_decay: float = 1e-5):
        """è®¾ç½®è®­ç»ƒå‚æ•°"""
        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=weight_decay)
    
    def compute_physics_loss(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—ç‰©ç†çº¦æŸæŸå¤±"""
        if not self.physics_equations:
            return torch.tensor(0.0, device=self.device)
        
        total_loss = torch.tensor(0.0, device=self.device)
        
        for equation in self.physics_equations:
            # è®¡ç®—ç‰©ç†æ–¹ç¨‹çš„æ®‹å·®
            residual = equation(x, y)
            total_loss += torch.mean(residual ** 2)
        
        return total_loss
    
    def compute_boundary_loss(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—è¾¹ç•Œæ¡ä»¶æŸå¤±"""
        if not self.boundary_conditions:
            return torch.tensor(0.0, device=self.device)
        
        total_loss = torch.tensor(0.0, device=self.device)
        
        for bc in self.boundary_conditions:
            # è®¡ç®—è¾¹ç•Œæ¡ä»¶çš„æ®‹å·®
            residual = bc(x, y)
            total_loss += torch.mean(residual ** 2)
        
        return total_loss
    
    def train(self, X: np.ndarray, y: np.ndarray, 
              physics_points: np.ndarray = None,
              boundary_points: np.ndarray = None,
              epochs: int = 1000, 
              physics_weight: float = 1.0,
              boundary_weight: float = 1.0,
              batch_size: int = 32,
              validation_split: float = 0.2) -> dict:
        """è®­ç»ƒPINNæ¨¡å‹"""
        if self.optimizer is None:
            self.setup_training()
        
        # éªŒè¯è¾“å…¥æ•°æ®
        if X.shape[0] != y.shape[0]:
            raise ValueError(f"Xå’Œyæ ·æœ¬æ•°ä¸åŒ¹é…ï¼š{X.shape[0]} vs {y.shape[0]}")
        
        # æ•°æ®åˆ†å‰²
        if validation_split > 0:
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=validation_split, random_state=42
            )
        else:
            X_train, y_train = X, y
            X_val, y_val = None, None
        
        # è½¬æ¢ä¸ºå¼ é‡
        X_train_tensor = torch.FloatTensor(X_train).to(self.device)
        y_train_tensor = torch.FloatTensor(y_train).to(self.device)
        
        if X_val is not None:
            X_val_tensor = torch.FloatTensor(X_val).to(self.device)
            y_val_tensor = torch.FloatTensor(y_val).to(self.device)
        
        if physics_points is not None:
            physics_tensor = torch.FloatTensor(physics_points).to(self.device)
        
        if boundary_points is not None:
            boundary_tensor = torch.FloatTensor(boundary_points).to(self.device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        history = {
            'total_loss': [], 'data_loss': [], 'physics_loss': [], 
            'boundary_loss': [], 'val_loss': [], 'train_time': 0.0
        }
        start_time = time.time()
        
        best_val_loss = float('inf')
        patience = 50
        patience_counter = 0
        
        for epoch in range(epochs):
            self.train()
            epoch_losses = {'total': 0, 'data': 0, 'physics': 0, 'boundary': 0}
            
            for batch_X, batch_y in train_loader:
                self.optimizer.zero_grad()
                
                # æ•°æ®æŸå¤±
                outputs = self(batch_X)
                data_loss = F.mse_loss(outputs, batch_y)
                
                # ç‰©ç†æŸå¤±
                if physics_points is not None and self.physics_equations:
                    physics_outputs = self(physics_tensor)
                    physics_loss = self.compute_physics_loss(physics_tensor, physics_outputs)
                else:
                    physics_loss = torch.tensor(0.0, device=self.device)
                
                # è¾¹ç•ŒæŸå¤±
                if boundary_points is not None and self.boundary_conditions:
                    boundary_outputs = self(boundary_tensor)
                    boundary_loss = self.compute_boundary_loss(boundary_tensor, boundary_outputs)
                else:
                    boundary_loss = torch.tensor(0.0, device=self.device)
                
                # æ€»æŸå¤±
                total_loss = data_loss + physics_weight * physics_loss + boundary_weight * boundary_loss
                
                total_loss.backward()
                self.optimizer.step()
                
                # ç´¯ç§¯æŸå¤±
                epoch_losses['total'] += total_loss.item()
                epoch_losses['data'] += data_loss.item()
                epoch_losses['physics'] += physics_loss.item()
                epoch_losses['boundary'] += boundary_loss.item()
            
            # è®¡ç®—å¹³å‡æŸå¤±
            num_batches = len(train_loader)
            for key in epoch_losses:
                epoch_losses[key] /= num_batches
            
            # éªŒè¯æŸå¤±
            val_loss = None
            if X_val is not None:
                self.eval()
                with torch.no_grad():
                    val_outputs = self(X_val_tensor)
                    val_loss = F.mse_loss(val_outputs, y_val_tensor).item()
                    history['val_loss'].append(val_loss)
                    
                    # æ—©åœ
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        patience_counter = 0
                    else:
                        patience_counter += 1
                    
                    if patience_counter >= patience:
                        print(f"   æ—©åœåœ¨ç¬¬ {epoch+1} è½®")
                        break
            
            # è®°å½•å†å²
            history['total_loss'].append(epoch_losses['total'])
            history['data_loss'].append(epoch_losses['data'])
            history['physics_loss'].append(epoch_losses['physics'])
            history['boundary_loss'].append(epoch_losses['boundary'])
            
            if (epoch + 1) % 100 == 0:
                val_info = f", val_loss={val_loss:.6f}" if val_loss is not None else ""
                print(f"   Epoch {epoch+1}/{epochs}: total_loss={epoch_losses['total']:.6f}, "
                      f"data_loss={epoch_losses['data']:.6f}, physics_loss={epoch_losses['physics']:.6f}, "
                      f"boundary_loss={epoch_losses['boundary']:.6f}{val_info}")
        
        history['train_time'] = time.time() - start_time
        self.is_trained = True
        self.training_history = history
        return history
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        self.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)
            outputs = self(X_tensor)
            return outputs.cpu().numpy()


class SurrogateModelAdvanced(BaseSolver):
    """
    é«˜çº§ä»£ç†æ¨¡å‹ - æ”¯æŒå¤šç§ç®—æ³•
    
    æ ¸å¿ƒæ€æƒ³ï¼šç”¨ä¼ ç»Ÿæ•°å€¼æ¨¡æ‹Ÿç”Ÿæˆ"è¾“å…¥å‚æ•°â†’è¾“å‡ºç‰©ç†åœº"çš„æ•°æ®é›†ï¼Œ
    è®­ç»ƒDLæ¨¡å‹å­¦ä¹ è¿™ç§æ˜ å°„ï¼Œåç»­ç”¨æ¨¡å‹ç›´æ¥é¢„æµ‹ï¼Œæ›¿ä»£å®Œæ•´æ¨¡æ‹Ÿæµç¨‹
    """
    
    def __init__(self, model_type: str = 'gaussian_process', **kwargs):
        super().__init__()
        self.model_type = model_type
        self.model = None
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.kwargs = kwargs
        
        if model_type == 'gaussian_process' and not HAS_SKLEARN:
            raise ImportError("éœ€è¦å®‰è£…scikit-learnæ¥ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹å›å½’")
    
    def train(self, X: np.ndarray, y: np.ndarray, **kwargs) -> dict:
        """è®­ç»ƒä»£ç†æ¨¡å‹"""
        # éªŒè¯è¾“å…¥æ•°æ®åˆæ³•æ€§
        if X.shape[0] != y.shape[0]:
            raise ValueError(f"Xå’Œyæ ·æœ¬æ•°ä¸åŒ¹é…ï¼š{X.shape[0]} vs {y.shape[0]}")
        
        start_time = time.time()
        
        # æ•°æ®æ ‡å‡†åŒ–
        X_scaled = self.scaler_X.fit_transform(X)
        y_scaled = self.scaler_y.fit_transform(y.reshape(-1, 1)).flatten()
        
        if self.model_type == 'gaussian_process':
            # é«˜æ–¯è¿‡ç¨‹å›å½’
            kernel = self.kwargs.get('kernel', RBF(length_scale=1.0) + ConstantKernel())
            self.model = GaussianProcessRegressor(kernel=kernel, random_state=42)
            self.model.fit(X_scaled, y_scaled)
        
        elif self.model_type == 'random_forest':
            # éšæœºæ£®æ—å›å½’
            n_estimators = self.kwargs.get('n_estimators', 100)
            if not isinstance(n_estimators, int) or n_estimators <= 0:
                raise ValueError(f"n_estimatorså¿…é¡»ä¸ºæ­£æ•´æ•°ï¼Œå®é™…ä¸ºï¼š{n_estimators}")
            
            max_depth = self.kwargs.get('max_depth', None)
            self.model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
            self.model.fit(X_scaled, y_scaled)
        
        elif self.model_type == 'gradient_boosting':
            # æ¢¯åº¦æå‡å›å½’
            n_estimators = self.kwargs.get('n_estimators', 100)
            if not isinstance(n_estimators, int) or n_estimators <= 0:
                raise ValueError(f"n_estimatorså¿…é¡»ä¸ºæ­£æ•´æ•°ï¼Œå®é™…ä¸ºï¼š{n_estimators}")
            
            learning_rate = self.kwargs.get('learning_rate', 0.1)
            if not isinstance(learning_rate, (int, float)) or learning_rate <= 0:
                raise ValueError(f"learning_rateå¿…é¡»ä¸ºæ­£æ•°ï¼Œå®é™…ä¸ºï¼š{learning_rate}")
            
            self.model = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, random_state=42)
            self.model.fit(X_scaled, y_scaled)
        
        elif self.model_type == 'neural_network':
            # ç¥ç»ç½‘ç»œå›å½’
            hidden_layer_sizes = self.kwargs.get('hidden_layer_sizes', (100, 50))
            max_iter = self.kwargs.get('max_iter', 1000)
            if not isinstance(max_iter, int) or max_iter <= 0:
                raise ValueError(f"max_iterå¿…é¡»ä¸ºæ­£æ•´æ•°ï¼Œå®é™…ä¸ºï¼š{max_iter}")
            
            self.model = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, max_iter=max_iter, random_state=42)
            self.model.fit(X_scaled, y_scaled)
        
        self.is_trained = True
        training_time = time.time() - start_time
        
        self.training_history = {
            'training_time': training_time,
            'model_type': self.model_type,
            'n_samples': len(X),
            'n_features': X.shape[1]
        }
        
        return self.training_history
    
    def predict(self, X: np.ndarray, return_std: bool = False) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """é¢„æµ‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self.scaler_X.transform(X)
        
        if self.model_type == 'gaussian_process':
            if return_std:
                y_pred_scaled, std = self.model.predict(X_scaled, return_std=True)
                y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
                return y_pred, std
            else:
                y_pred_scaled = self.model.predict(X_scaled)
                y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
                return y_pred
        
        elif self.model_type in ['random_forest', 'gradient_boosting', 'neural_network']:
            y_pred_scaled = self.model.predict(X_scaled)
            y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
            
            if return_std:
                # éšæœºæ£®æ—ï¼šé€šè¿‡æ ‘çš„é¢„æµ‹å·®å¼‚ä¼°è®¡æ ‡å‡†å·®
                if self.model_type == 'random_forest':
                    tree_preds = np.array([tree.predict(X_scaled) for tree in self.model.estimators_])
                    std = np.std(tree_preds, axis=0)  # æ ‘é—´é¢„æµ‹æ ‡å‡†å·®
                    std = self.scaler_y.inverse_transform(std.reshape(-1, 1)).flatten()  # åå½’ä¸€åŒ–
                else:
                    std = np.zeros_like(y_pred)  # å…¶ä»–æ¨¡å‹æš‚ä¸æ”¯æŒ
                return y_pred, std
            return y_pred
        
        return y_pred


class MultiScaleMLBridge:
    """
    å¤šå°ºåº¦MLæ¡¥æ¥å™¨ - ç”¨äºè·¨å°ºåº¦æ¨¡æ‹Ÿ
    
    æ ¸å¿ƒæ€æƒ³ï¼šåœ¨è·¨å°ºåº¦æ¨¡æ‹Ÿä¸­ï¼ˆå¦‚ä»æ¿å—è¿åŠ¨åˆ°å²©çŸ³å˜å½¢ï¼‰ï¼Œ
    ç”¨MLæ¨¡å‹æ›¿ä»£å°å°ºåº¦ç²¾ç»†æ¨¡æ‹Ÿï¼Œå°†å°å°ºåº¦ç»“æœ"æ‰“åŒ…"ä¸ºå¤§å°ºåº¦æ¨¡å‹çš„å‚æ•°
    """
    
    def __init__(self, fine_scale_model: Callable = None, coarse_scale_model: Callable = None):
        self.fine_scale_model = fine_scale_model
        self.coarse_scale_model = coarse_scale_model
        self.bridge_model = None
        self.is_trained = False
        self.scale_ratio = 1.0
        self.bridge_type = 'neural_network'
    
    def setup_bridge_model(self, input_dim: int, output_dim: int, model_type: str = 'neural_network'):
        """è®¾ç½®æ¡¥æ¥æ¨¡å‹"""
        self.bridge_type = model_type
        
        if model_type == 'neural_network' and HAS_PYTORCH:
            self.bridge_model = PhysicsInformedNeuralNetwork(
                input_dim, [128, 64, 32], output_dim
            )
        elif model_type == 'surrogate':
            self.bridge_model = SurrogateModelAdvanced('gaussian_process')
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¡¥æ¥æ¨¡å‹ç±»å‹: {model_type}")
    
    def train_bridge(self, fine_data: np.ndarray, coarse_data: np.ndarray, **kwargs) -> dict:
        """è®­ç»ƒæ¡¥æ¥æ¨¡å‹"""
        if self.bridge_model is None:
            raise ValueError("æ¡¥æ¥æ¨¡å‹å°šæœªè®¾ç½®")
        
        if isinstance(self.bridge_model, PhysicsInformedNeuralNetwork):
            self.bridge_model.setup_training()
            result = self.bridge_model.train(fine_data, coarse_data, **kwargs)
        else:
            result = self.bridge_model.train(fine_data, coarse_data, **kwargs)
        
        self.is_trained = True
        return result
    
    def predict_coarse_from_fine(self, fine_data: np.ndarray) -> np.ndarray:
        """ä»ç»†å°ºåº¦æ•°æ®é¢„æµ‹ç²—å°ºåº¦æ•°æ®"""
        if not self.is_trained:
            raise ValueError("æ¡¥æ¥æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        return self.bridge_model.predict(fine_data)
    
    def predict_fine_from_coarse(self, coarse_data: np.ndarray) -> np.ndarray:
        """ä»ç²—å°ºåº¦æ•°æ®é¢„æµ‹ç»†å°ºåº¦æ•°æ®"""
        if not self.is_trained:
            raise ValueError("æ¡¥æ¥æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        # è¿™é‡Œéœ€è¦å®ç°åå‘æ˜ å°„
        # ç®€åŒ–å®ç°ï¼šè¿”å›ç²—å°ºåº¦æ•°æ®çš„æ’å€¼
        return coarse_data
    
    def set_scale_ratio(self, ratio: float):
        """è®¾ç½®å°ºåº¦æ¯”ä¾‹"""
        self.scale_ratio = ratio


class HybridMLAccelerator:
    """
    æ··åˆMLåŠ é€Ÿå™¨ - ç»“åˆä¼ ç»Ÿæ±‚è§£å™¨å’ŒML
    
    æ ¸å¿ƒæ€æƒ³ï¼šæ— æ³•å®Œå…¨æ›¿ä»£ä¼ ç»Ÿæ±‚è§£å™¨ï¼ˆéœ€é«˜ç²¾åº¦ï¼‰ï¼Œ
    ä½†å¯åŠ é€Ÿå…¶ä¸­è€—æ—¶æ­¥éª¤ï¼ˆå¦‚è¿­ä»£æ±‚è§£ã€ç½‘æ ¼è‡ªé€‚åº”ï¼‰
    """
    
    def __init__(self, traditional_solver: Callable = None):
        self.traditional_solver = traditional_solver
        self.ml_models = {}
        self.performance_stats = {
            'traditional_time': 0.0,
            'ml_time': 0.0,
            'speedup': 0.0,
            'accuracy_loss': 0.0,
            'total_calls': 0
        }
        self.acceleration_strategies = {
            'initial_guess': None,
            'preconditioner': None,
            'adaptive_mesh': None,
            'multiscale': None
        }
    
    def add_ml_model(self, name: str, model: Union[PhysicsInformedNeuralNetwork, SurrogateModelAdvanced]):
        """æ·»åŠ MLæ¨¡å‹"""
        self.ml_models[name] = model
        print(f"âœ… æ·»åŠ MLæ¨¡å‹: {name}")
    
    def setup_acceleration_strategy(self, strategy: str, model_name: str):
        """è®¾ç½®åŠ é€Ÿç­–ç•¥"""
        if model_name in self.ml_models:
            self.acceleration_strategies[strategy] = model_name
            print(f"âœ… è®¾ç½®åŠ é€Ÿç­–ç•¥: {strategy} -> {model_name}")
        else:
            raise ValueError(f"MLæ¨¡å‹ {model_name} ä¸å­˜åœ¨")
    
    def solve_hybrid(self, problem_data: Dict, use_ml: bool = True, 
                    ml_model_name: str = None) -> Dict:
        """æ··åˆæ±‚è§£"""
        start_time = time.time()
        self.performance_stats['total_calls'] += 1
        
        if use_ml and ml_model_name and ml_model_name in self.ml_models:
            # ä½¿ç”¨MLæ¨¡å‹
            ml_model = self.ml_models[ml_model_name]
            result = ml_model.predict(problem_data['input'])
            ml_time = time.time() - start_time
            
            self.performance_stats['ml_time'] += ml_time
            
            return {
                'solution': result,
                'method': 'ml',
                'time': ml_time,
                'model_name': ml_model_name
            }
        
        else:
            # ä½¿ç”¨ä¼ ç»Ÿæ±‚è§£å™¨
            if self.traditional_solver is None:
                raise ValueError("ä¼ ç»Ÿæ±‚è§£å™¨æœªè®¾ç½®")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰MLåŠ é€Ÿç­–ç•¥
            if self.acceleration_strategies['initial_guess']:
                # ä½¿ç”¨MLé¢„æµ‹åˆå§‹çŒœæµ‹
                initial_guess = self.ml_models[self.acceleration_strategies['initial_guess']].predict(
                    problem_data['input']
                )
                problem_data['initial_guess'] = initial_guess
            
            result = self.traditional_solver(problem_data)
            traditional_time = time.time() - start_time
            
            self.performance_stats['traditional_time'] += traditional_time
            
            return {
                'solution': result,
                'method': 'traditional',
                'time': traditional_time
            }
    
    def compare_methods(self, problem_data: Dict, ml_model_name: str = None) -> Dict:
        """æ¯”è¾ƒä¼ ç»Ÿæ–¹æ³•å’ŒMLæ–¹æ³•"""
        # ä¼ ç»Ÿæ–¹æ³•
        traditional_result = self.solve_hybrid(problem_data, use_ml=False)
        
        # MLæ–¹æ³•
        if ml_model_name and ml_model_name in self.ml_models:
            ml_result = self.solve_hybrid(problem_data, use_ml=True, ml_model_name=ml_model_name)
            
            # è®¡ç®—åŠ é€Ÿæ¯”
            speedup = traditional_result['time'] / ml_result['time']
            
            # è®¡ç®—ç²¾åº¦æŸå¤±ï¼ˆç®€åŒ–ï¼‰
            accuracy_loss = np.mean(np.abs(
                traditional_result['solution'] - ml_result['solution']
            ))
            
            self.performance_stats['speedup'] = speedup
            self.performance_stats['accuracy_loss'] = accuracy_loss
            
            return {
                'traditional': traditional_result,
                'ml': ml_result,
                'speedup': speedup,
                'accuracy_loss': accuracy_loss
            }
        
        return {'traditional': traditional_result}
    
    def get_performance_stats(self) -> dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        stats = self.performance_stats.copy()
        if stats['total_calls'] > 0:
            stats['avg_traditional_time'] = stats['traditional_time'] / stats['total_calls']
            stats['avg_ml_time'] = stats['ml_time'] / stats['total_calls']
        return stats


class AdaptiveMLSolver:
    """
    è‡ªé€‚åº”MLæ±‚è§£å™¨ - æ ¹æ®é—®é¢˜ç‰¹å¾è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹æ³•
    
    æ ¸å¿ƒæ€æƒ³ï¼šæ ¹æ®é—®é¢˜è§„æ¨¡ã€ç²¾åº¦è¦æ±‚ã€è®¡ç®—èµ„æºç­‰ç‰¹å¾ï¼Œ
    è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„æ±‚è§£æ–¹æ³•ï¼ˆä¼ ç»Ÿæ•°å€¼æ–¹æ³• vs MLåŠ é€Ÿæ–¹æ³•ï¼‰
    """
    
    def __init__(self):
        self.solvers = {}
        self.performance_history = []
        self.adaptive_rules = {}
        self.selection_strategy = 'performance_based'
        
        # å¯é…ç½®è¯„åˆ†æƒé‡
        self.score_weights = {
            'problem_feature': 1.0,
            'accuracy': 0.5,
            'speed': 0.5,
            'priority': 0.1
        }
    
    def add_solver(self, name: str, solver: Callable, 
                  conditions: Dict = None, priority: int = 1):
        """æ·»åŠ æ±‚è§£å™¨"""
        self.solvers[name] = {
            'solver': solver,
            'conditions': conditions or {},
            'priority': priority,
            'performance': {'accuracy': 0.0, 'speed': 0.0, 'usage_count': 0}
        }
        print(f"âœ… æ·»åŠ æ±‚è§£å™¨: {name}")
    
    def set_selection_strategy(self, strategy: str):
        """è®¾ç½®é€‰æ‹©ç­–ç•¥"""
        valid_strategies = ['performance_based', 'rule_based', 'hybrid']
        if strategy in valid_strategies:
            self.selection_strategy = strategy
            print(f"âœ… è®¾ç½®é€‰æ‹©ç­–ç•¥: {strategy}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é€‰æ‹©ç­–ç•¥: {strategy}")
    
    def set_score_weights(self, weights: Dict[str, float]):
        """è®¾ç½®è¯„åˆ†æƒé‡"""
        for key, value in weights.items():
            if key in self.score_weights:
                self.score_weights[key] = value
        print(f"âœ… è¯„åˆ†æƒé‡å·²æ›´æ–°: {self.score_weights}")
    
    def select_best_solver(self, problem_data: Dict) -> str:
        """é€‰æ‹©æœ€ä½³æ±‚è§£å™¨"""
        if self.selection_strategy == 'performance_based':
            return self._select_by_performance(problem_data)
        elif self.selection_strategy == 'rule_based':
            return self._select_by_rules(problem_data)
        elif self.selection_strategy == 'hybrid':
            return self._select_hybrid(problem_data)
        
        return None
    
    def _select_by_performance(self, problem_data: Dict) -> str:
        """åŸºäºæ€§èƒ½é€‰æ‹©æ±‚è§£å™¨"""
        best_solver = None
        best_score = -1
        
        for name, solver_info in self.solvers.items():
            score = self._evaluate_solver_performance(name, solver_info, problem_data)
            if score > best_score:
                best_score = score
                best_solver = name
        
        return best_solver
    
    def _select_by_rules(self, problem_data: Dict) -> str:
        """åŸºäºè§„åˆ™é€‰æ‹©æ±‚è§£å™¨"""
        for name, solver_info in self.solvers.items():
            if self._check_conditions(solver_info['conditions'], problem_data):
                return name
        return None
    
    def _select_hybrid(self, problem_data: Dict) -> str:
        """æ··åˆé€‰æ‹©ç­–ç•¥"""
        # å…ˆæ£€æŸ¥è§„åˆ™
        rule_based = self._select_by_rules(problem_data)
        if rule_based:
            return rule_based
        
        # å†åŸºäºæ€§èƒ½é€‰æ‹©
        return self._select_by_performance(problem_data)
    
    def _evaluate_solver_performance(self, name: str, solver_info: Dict, problem_data: Dict) -> float:
        """è¯„ä¼°æ±‚è§£å™¨æ€§èƒ½"""
        score = 0.0
        weights = self.score_weights
        
        # æ‰©å±•é—®é¢˜ç‰¹å¾è¯„ä¼°ï¼ˆæ”¯æŒç²¾åº¦è¦æ±‚ï¼‰
        if 'size' in problem_data:
            if problem_data['size'] < 1000 and solver_info['conditions'].get('small_problems', False):
                score += weights['problem_feature']
            elif problem_data['size'] >= 1000 and solver_info['conditions'].get('large_problems', False):
                score += weights['problem_feature']
        
        # æ”¯æŒç²¾åº¦è¦æ±‚åŒ¹é…ï¼ˆæ–°å¢ï¼‰
        if 'accuracy_requirement' in problem_data:
            req = problem_data['accuracy_requirement']
            if solver_info['performance']['accuracy'] >= req:
                score += weights['problem_feature'] * 0.5  # é¢å¤–åŠ åˆ†
        
        # æ”¯æŒè®¡ç®—èµ„æºé™åˆ¶ï¼ˆæ–°å¢ï¼‰
        if 'compute_resource' in problem_data:
            resource = problem_data['compute_resource']
            if solver_info['conditions'].get('resource_requirement', 'low') == resource:
                score += weights['problem_feature'] * 0.3
        
        # åŠ¨æ€æƒé‡è®¡ç®—
        performance = solver_info['performance']
        score += performance['accuracy'] * weights['accuracy'] + performance['speed'] * weights['speed']
        score += solver_info['priority'] * weights['priority']
        
        return score
    
    def _check_conditions(self, conditions: Dict, problem_data: Dict) -> bool:
        """æ£€æŸ¥æ¡ä»¶æ˜¯å¦æ»¡è¶³ - æ”¯æŒèŒƒå›´æ¡ä»¶"""
        for key, cond in conditions.items():
            if key not in problem_data:
                return False
            val = problem_data[key]
            
            # æ”¯æŒèŒƒå›´æ¡ä»¶ï¼ˆå¦‚ ('>', 1000)ã€('<=', 0.95)ï¼‰
            if isinstance(cond, tuple) and len(cond) == 2:
                op, threshold = cond
                if op == '>':
                    if not (val > threshold):
                        return False
                elif op == '<':
                    if not (val < threshold):
                        return False
                elif op == '>=':
                    if not (val >= threshold):
                        return False
                elif op == '<=':
                    if not (val <= threshold):
                        return False
                else:
                    return False
            # åŸé€»è¾‘ï¼šæ”¯æŒç­‰å€¼æˆ–åˆ—è¡¨åŒ…å«
            elif isinstance(cond, (list, tuple)):
                if val not in cond:
                    return False
            else:
                if val != cond:
                    return False
        return True
    
    def solve(self, problem_data: Dict) -> Dict:
        """è‡ªé€‚åº”æ±‚è§£"""
        # é€‰æ‹©æœ€ä½³æ±‚è§£å™¨
        best_solver_name = self.select_best_solver(problem_data)
        
        if best_solver_name is None:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ±‚è§£å™¨")
        
        # æ‰§è¡Œæ±‚è§£
        solver_info = self.solvers[best_solver_name]
        solver = solver_info['solver']
        
        start_time = time.time()
        result = solver(problem_data)
        solve_time = time.time() - start_time
        
        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        solver_info['performance']['usage_count'] += 1
        solver_info['performance']['speed'] = 1.0 / solve_time
        
        # æ–°å¢ï¼šè‹¥æä¾›å‚è€ƒè§£ï¼Œè®¡ç®—ç²¾åº¦
        if 'reference_solution' in problem_data:
            mse = np.mean((result - problem_data['reference_solution'])**2)
            accuracy = 1.0 / (1.0 + mse)  # å½’ä¸€åŒ–åˆ° [0,1]
            solver_info['performance']['accuracy'] = accuracy
        
        # è®°å½•å†å²
        self.performance_history.append({
            'solver': best_solver_name,
            'time': solve_time,
            'problem_size': problem_data.get('size', 0),
            'timestamp': time.time()
        })
        
        return {
            'solution': result,
            'solver_used': best_solver_name,
            'time': solve_time
        }
    
    def get_performance_summary(self) -> Dict:
        """è·å–æ€§èƒ½æ€»ç»“"""
        summary = {}
        
        for name, solver_info in self.solvers.items():
            performance = solver_info['performance']
            summary[name] = {
                'usage_count': performance['usage_count'],
                'avg_speed': performance['speed'],
                'avg_accuracy': performance['accuracy']
            }
        
        return summary


class RLAgent(nn.Module):
    """
    å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ - ç”¨äºä¼˜åŒ–æ•°å€¼æ±‚è§£ç­–ç•¥
    
    æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„æ±‚è§£å‚æ•°ï¼ˆæ—¶é—´æ­¥é•¿ã€ç½‘æ ¼åŠ å¯†æ–¹æ¡ˆç­‰ï¼‰ï¼Œ
    å‡å°‘äººå·¥è°ƒå‚æˆæœ¬ï¼Œæå‡æ±‚è§£æ•ˆç‡
    """
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [128, 64]):
        super().__init__()
        
        if not HAS_PYTORCH:
            raise ImportError("éœ€è¦å®‰è£…PyTorchæ¥ä½¿ç”¨RLæ™ºèƒ½ä½“")
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Actorç½‘ç»œï¼ˆç­–ç•¥ç½‘ç»œï¼‰
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dims[0]),
            nn.ReLU(),
            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.ReLU(),
            nn.Linear(hidden_dims[1], action_dim),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´[-1, 1]
        )
        
        # Criticç½‘ç»œï¼ˆä»·å€¼ç½‘ç»œï¼‰
        self.critic = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dims[0]),
            nn.ReLU(),
            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.ReLU(),
            nn.Linear(hidden_dims[1], 1)
        )
        
        self.to(self.device)
        
        print(f"ğŸ”„ RLæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}")
        print(f"   çŠ¶æ€ç»´åº¦: {state_dim}, åŠ¨ä½œç»´åº¦: {action_dim}")
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ - è¿”å›åŠ¨ä½œ"""
        return self.actor(state)
    
    def get_action(self, state: np.ndarray, noise_scale: float = 0.1) -> np.ndarray:
        """è·å–åŠ¨ä½œï¼ˆå¸¦æ¢ç´¢å™ªå£°ï¼‰"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action = self.forward(state_tensor).squeeze(0)
            # æ·»åŠ æ¢ç´¢å™ªå£°
            noise = torch.randn_like(action) * noise_scale
            action = action + noise
            # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
            action = torch.clamp(action, -1.0, 1.0)
        
        return action.cpu().numpy()
    
    def get_value(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """è·å–çŠ¶æ€-åŠ¨ä½œä»·å€¼"""
        return self.critic(torch.cat([state, action], dim=1))


class SolverEnvironment:
    """
    æ±‚è§£å™¨ç¯å¢ƒ - æ¨¡æ‹Ÿæ•°å€¼æ±‚è§£è¿‡ç¨‹ï¼Œä¸ºRLæä¾›è®­ç»ƒç¯å¢ƒ
    
    æ ¸å¿ƒæ€æƒ³ï¼šå°†æ•°å€¼æ±‚è§£è¿‡ç¨‹å»ºæ¨¡ä¸ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œ
    æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜çš„æ±‚è§£ç­–ç•¥
    """
    
    def __init__(self, solver_config: Dict, physics_config: PhysicsConfig = None):
        self.solver_config = solver_config
        self.physics_config = physics_config or PhysicsConfig()
        self.max_steps = solver_config.get('max_steps', 100)
        self.current_step = 0
        self.convergence_history = []
        self.performance_metrics = {}
        
        # æ±‚è§£ç­–ç•¥å‚æ•°èŒƒå›´
        self.action_bounds = {
            'time_step': (0.001, 0.1),      # æ—¶é—´æ­¥é•¿
            'mesh_refinement': (0.1, 2.0),  # ç½‘æ ¼åŠ å¯†å› å­
            'tolerance': (1e-6, 1e-3),      # æ”¶æ•›å®¹å·®
            'max_iterations': (50, 500)      # æœ€å¤§è¿­ä»£æ¬¡æ•°
        }
        
        print(f"ğŸ”„ æ±‚è§£å™¨ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"   æœ€å¤§æ­¥æ•°: {self.max_steps}")
        print(f"   åŠ¨ä½œå‚æ•°: {list(self.action_bounds.keys())}")
    
    def reset(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒ"""
        self.current_step = 0
        self.convergence_history = []
        self.performance_metrics = {}
        
        # è¿”å›åˆå§‹çŠ¶æ€
        initial_state = self._get_state()
        return initial_state
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ"""
        if self.current_step >= self.max_steps:
            return self._get_state(), 0.0, True, {}
        
        # è§£æåŠ¨ä½œï¼ˆä»[-1,1]æ˜ å°„åˆ°å®é™…å‚æ•°èŒƒå›´ï¼‰
        solver_params = self._action_to_params(action)
        
        # æ¨¡æ‹Ÿæ±‚è§£è¿‡ç¨‹
        reward, metrics = self._simulate_solving(solver_params)
        
        # æ›´æ–°çŠ¶æ€
        self.current_step += 1
        self.convergence_history.append(metrics.get('convergence', 0.0))
        self.performance_metrics.update(metrics)
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
        done = self.current_step >= self.max_steps or metrics.get('converged', False)
        
        return self._get_state(), reward, done, metrics
    
    def _action_to_params(self, action: np.ndarray) -> Dict:
        """å°†åŠ¨ä½œè½¬æ¢ä¸ºæ±‚è§£å™¨å‚æ•°"""
        params = {}
        action_names = list(self.action_bounds.keys())
        
        for i, name in enumerate(action_names):
            if i < len(action):
                # å°†[-1,1]æ˜ å°„åˆ°å®é™…èŒƒå›´
                action_val = action[i]
                min_val, max_val = self.action_bounds[name]
                param_val = min_val + (action_val + 1) * (max_val - min_val) / 2
                params[name] = param_val
        
        return params
    
    def _simulate_solving(self, solver_params: Dict) -> Tuple[float, Dict]:
        """æ¨¡æ‹Ÿæ±‚è§£è¿‡ç¨‹ï¼Œè®¡ç®—å¥–åŠ±å’ŒæŒ‡æ ‡"""
        # æ¨¡æ‹Ÿæ±‚è§£æ—¶é—´ï¼ˆåŸºäºå‚æ•°ï¼‰
        time_step = solver_params.get('time_step', 0.01)
        mesh_refinement = solver_params.get('mesh_refinement', 1.0)
        tolerance = solver_params.get('tolerance', 1e-4)
        max_iterations = solver_params.get('max_iterations', 100)
        
        # æ¨¡æ‹Ÿæ”¶æ•›è¿‡ç¨‹
        convergence_rate = 1.0 / (1.0 + tolerance * 1000)  # å®¹å·®è¶Šå°ï¼Œæ”¶æ•›è¶Šå¿«
        mesh_efficiency = 1.0 / (1.0 + abs(mesh_refinement - 1.0))  # ç½‘æ ¼å› å­æ¥è¿‘1æ—¶æ•ˆç‡æœ€é«˜
        
        # æ¨¡æ‹Ÿè¿­ä»£æ¬¡æ•°
        actual_iterations = min(max_iterations, int(50 / convergence_rate))
        
        # è®¡ç®—å¥–åŠ±ï¼ˆç»¼åˆè€ƒè™‘æ•ˆç‡ã€ç²¾åº¦ã€ç¨³å®šæ€§ï¼‰
        efficiency_reward = 1.0 / (1.0 + time_step * 100)  # æ—¶é—´æ­¥é•¿è¶Šå°è¶Šå¥½
        accuracy_reward = 1.0 / (1.0 + tolerance * 1e6)    # å®¹å·®è¶Šå°è¶Šå¥½
        stability_reward = 1.0 / (1.0 + abs(mesh_refinement - 1.0))  # ç½‘æ ¼ç¨³å®šæ€§
        
        # æ”¶æ•›å¥–åŠ±
        converged = actual_iterations < max_iterations
        convergence_reward = 10.0 if converged else 0.0
        
        # æ€»å¥–åŠ±
        total_reward = (efficiency_reward + accuracy_reward + stability_reward + convergence_reward) / 4
        
        # æ€§èƒ½æŒ‡æ ‡
        metrics = {
            'convergence': convergence_rate,
            'mesh_efficiency': mesh_efficiency,
            'iterations': actual_iterations,
            'converged': converged,
            'time_step': time_step,
            'mesh_refinement': mesh_refinement,
            'tolerance': tolerance
        }
        
        return total_reward, metrics
    
    def _get_state(self) -> np.ndarray:
        """è·å–å½“å‰çŠ¶æ€"""
        state = []
        
        # å½“å‰æ­¥æ•°ï¼ˆå½’ä¸€åŒ–ï¼‰
        state.append(self.current_step / self.max_steps)
        
        # æ”¶æ•›å†å²ç»Ÿè®¡
        if self.convergence_history:
            state.extend([
                np.mean(self.convergence_history),
                np.std(self.convergence_history),
                self.convergence_history[-1] if self.convergence_history else 0.0
            ])
        else:
            state.extend([0.0, 0.0, 0.0])
        
        # æ€§èƒ½æŒ‡æ ‡
        for key in ['mesh_efficiency', 'iterations']:
            if key in self.performance_metrics:
                # å½’ä¸€åŒ–åˆ°[0,1]
                if key == 'iterations':
                    val = self.performance_metrics[key] / 500.0  # å‡è®¾æœ€å¤§500æ¬¡è¿­ä»£
                else:
                    val = self.performance_metrics[key]
                state.append(val)
            else:
                state.append(0.0)
        
        return np.array(state, dtype=np.float32)


class RLSolverOptimizer(BaseSolver):
    """
    å¼ºåŒ–å­¦ä¹ æ±‚è§£å™¨ä¼˜åŒ–å™¨ - ä½¿ç”¨RLè‡ªåŠ¨ä¼˜åŒ–æ•°å€¼æ±‚è§£ç­–ç•¥
    
    æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ™ºèƒ½ä½“ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„æ±‚è§£å‚æ•°ï¼Œ
    å®ç°"è‡ªå­¦ä¹ "çš„æ•°å€¼æ±‚è§£ä¼˜åŒ–
    """
    
    def __init__(self, state_dim: int, action_dim: int, solver_config: Dict = None):
        super().__init__()
        
        if not HAS_PYTORCH:
            raise ImportError("éœ€è¦å®‰è£…PyTorchæ¥ä½¿ç”¨RLæ±‚è§£å™¨ä¼˜åŒ–å™¨")
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.solver_config = solver_config or {}
        
        # åˆ›å»ºRLæ™ºèƒ½ä½“å’Œç¯å¢ƒ
        self.agent = RLAgent(state_dim, action_dim)
        self.environment = SolverEnvironment(solver_config)
        
        # è®­ç»ƒå‚æ•°
        self.learning_rate = 0.001
        self.gamma = 0.99  # æŠ˜æ‰£å› å­
        self.tau = 0.005   # è½¯æ›´æ–°å‚æ•°
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = []
        self.buffer_size = 10000
        self.batch_size = 64
        
        # ç›®æ ‡ç½‘ç»œï¼ˆç”¨äºç¨³å®šè®­ç»ƒï¼‰
        self.target_agent = RLAgent(state_dim, action_dim)
        self._update_target_network()
        
        self.optimizer_actor = optim.Adam(self.agent.actor.parameters(), lr=self.learning_rate)
        self.optimizer_critic = optim.Adam(self.agent.critic.parameters(), lr=self.learning_rate)
        
        print(f"ğŸ”„ RLæ±‚è§£å™¨ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   çŠ¶æ€ç»´åº¦: {state_dim}, åŠ¨ä½œç»´åº¦: {action_dim}")
    
    def _update_target_network(self):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(self.target_agent.parameters(), self.agent.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
    
    def _store_experience(self, state: np.ndarray, action: np.ndarray, 
                         reward: float, next_state: np.ndarray, done: bool):
        """å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº"""
        experience = (state, action, reward, next_state, done)
        self.replay_buffer.append(experience)
        
        # é™åˆ¶ç¼“å†²åŒºå¤§å°
        if len(self.replay_buffer) > self.buffer_size:
            self.replay_buffer.pop(0)
    
    def _sample_batch(self) -> List[Tuple]:
        """ä»å›æ”¾ç¼“å†²åŒºé‡‡æ ·æ‰¹æ¬¡"""
        if len(self.replay_buffer) < self.batch_size:
            return []
        
        indices = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)
        return [self.replay_buffer[i] for i in indices]
    
    def _update_networks(self, batch: List[Tuple]):
        """æ›´æ–°ç½‘ç»œå‚æ•°"""
        if not batch:
            return
        
        states = torch.FloatTensor(np.array([exp[0] for exp in batch])).to(self.device)
        actions = torch.FloatTensor(np.array([exp[1] for exp in batch])).to(self.device)
        rewards = torch.FloatTensor(np.array([exp[2] for exp in batch])).to(self.device)
        next_states = torch.FloatTensor(np.array([exp[3] for exp in batch])).to(self.device)
        dones = torch.BoolTensor(np.array([exp[4] for exp in batch])).to(self.device)
        
        # æ›´æ–°Criticç½‘ç»œ
        current_q_values = self.agent.get_value(states, actions)
        next_actions = self.target_agent(next_states)
        next_q_values = self.target_agent.get_value(next_states, next_actions)
        target_q_values = rewards + (self.gamma * next_q_values * (~dones).float())
        
        critic_loss = F.mse_loss(current_q_values, target_q_values.detach())
        
        self.optimizer_critic.zero_grad()
        critic_loss.backward()
        self.optimizer_critic.step()
        
        # æ›´æ–°Actorç½‘ç»œ
        actor_loss = -self.agent.get_value(states, self.agent(states)).mean()
        
        self.optimizer_actor.zero_grad()
        actor_loss.backward()
        self.optimizer_actor.step()
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self._update_target_network()
        
        return {
            'critic_loss': critic_loss.item(),
            'actor_loss': actor_loss.item()
        }
    
    def train(self, episodes: int = 1000, **kwargs) -> dict:
        """è®­ç»ƒRLæ™ºèƒ½ä½“"""
        print(f"ğŸ”„ å¼€å§‹è®­ç»ƒRLæ±‚è§£å™¨ä¼˜åŒ–å™¨ï¼Œæ€»è½®æ•°: {episodes}")
        
        episode_rewards = []
        episode_lengths = []
        training_losses = []
        
        for episode in range(episodes):
            state = self.environment.reset()
            episode_reward = 0.0
            episode_length = 0
            
            while True:
                # é€‰æ‹©åŠ¨ä½œ
                action = self.agent.get_action(state, noise_scale=max(0.01, 0.1 * (1 - episode / episodes)))
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = self.environment.step(action)
                
                # å­˜å‚¨ç»éªŒ
                self._store_experience(state, action, reward, next_state, done)
                
                # æ›´æ–°ç½‘ç»œ
                batch = self._sample_batch()
                if batch:
                    loss_info = self._update_networks(batch)
                    training_losses.append(loss_info)
                
                state = next_state
                episode_reward += reward
                episode_length += 1
                
                if done:
                    break
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                avg_length = np.mean(episode_lengths[-100:])
                print(f"   è½®æ•° {episode+1}/{episodes}: å¹³å‡å¥–åŠ±={avg_reward:.4f}, å¹³å‡é•¿åº¦={avg_length:.1f}")
        
        self.is_trained = True
        
        training_history = {
            'episodes': episodes,
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'training_losses': training_losses,
            'final_avg_reward': np.mean(episode_rewards[-100:]) if episode_rewards else 0.0
        }
        
        print(f"âœ… RLè®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆå¹³å‡å¥–åŠ±: {training_history['final_avg_reward']:.4f}")
        return training_history
    
    def optimize_solver_strategy(self, problem_state: np.ndarray) -> Dict:
        """ä¼˜åŒ–æ±‚è§£ç­–ç•¥"""
        if not self.is_trained:
            raise ValueError("RLæ™ºèƒ½ä½“å°šæœªè®­ç»ƒ")
        
        # ä½¿ç”¨è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
        optimal_action = self.agent.get_action(problem_state, noise_scale=0.0)
        
        # è½¬æ¢ä¸ºæ±‚è§£å™¨å‚æ•°
        solver_params = self.environment._action_to_params(optimal_action)
        
        print(f"ğŸ”§ RLä¼˜åŒ–æ±‚è§£ç­–ç•¥:")
        for param, value in solver_params.items():
            print(f"   {param}: {value:.6f}")
        
        return solver_params
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹æœ€ä¼˜æ±‚è§£ç­–ç•¥"""
        if not self.is_trained:
            raise ValueError("RLæ™ºèƒ½ä½“å°šæœªè®­ç»ƒ")
        
        strategies = []
        for state in X:
            action = self.agent.get_action(state, noise_scale=0.0)
            strategy = self.environment._action_to_params(action)
            strategies.append(list(strategy.values()))
        
        return np.array(strategies)
    
    def evaluate_strategy(self, strategy: Dict, problem_state: np.ndarray) -> Dict:
        """è¯„ä¼°æ±‚è§£ç­–ç•¥çš„æ€§èƒ½"""
        # å°†ç­–ç•¥è½¬æ¢ä¸ºåŠ¨ä½œ
        action = np.array([strategy.get(param, 0.0) for param in self.environment.action_bounds.keys()])
        
        # åœ¨ç¯å¢ƒä¸­æµ‹è¯•ç­–ç•¥
        state = problem_state
        total_reward = 0.0
        step_count = 0
        
        for _ in range(self.environment.max_steps):
            next_state, reward, done, info = self.environment.step(action)
            total_reward += reward
            step_count += 1
            
            if done:
                break
        
        return {
            'total_reward': total_reward,
            'step_count': step_count,
            'efficiency': total_reward / max(step_count, 1),
            'convergence': info.get('converged', False)
        }


def create_advanced_ml_system() -> Dict:
    """åˆ›å»ºé«˜çº§MLç³»ç»Ÿ"""
    system = {
        'pinn': PhysicsInformedNeuralNetwork,
        'surrogate': SurrogateModelAdvanced,
        'bridge': MultiScaleMLBridge,
        'hybrid': HybridMLAccelerator,
        'adaptive': AdaptiveMLSolver,
        'rl_agent': RLAgent,
        'rl_environment': SolverEnvironment,
        'rl_optimizer': RLSolverOptimizer
    }
    
    print("ğŸ”„ é«˜çº§MLç³»ç»Ÿåˆ›å»ºå®Œæˆ")
    return system


class RLAgent(nn.Module):
    """
    å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ - ç”¨äºä¼˜åŒ–æ•°å€¼æ±‚è§£ç­–ç•¥
    
    æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„æ±‚è§£å‚æ•°ï¼ˆæ—¶é—´æ­¥é•¿ã€ç½‘æ ¼åŠ å¯†æ–¹æ¡ˆç­‰ï¼‰ï¼Œ
    å‡å°‘äººå·¥è°ƒå‚æˆæœ¬ï¼Œæå‡æ±‚è§£æ•ˆç‡
    """
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [128, 64]):
        super().__init__()
        
        if not HAS_PYTORCH:
            raise ImportError("éœ€è¦å®‰è£…PyTorchæ¥ä½¿ç”¨RLæ™ºèƒ½ä½“")
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Actorç½‘ç»œï¼ˆç­–ç•¥ç½‘ç»œï¼‰
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dims[0]),
            nn.ReLU(),
            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.ReLU(),
            nn.Linear(hidden_dims[1], action_dim),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´[-1, 1]
        )
        
        # Criticç½‘ç»œï¼ˆä»·å€¼ç½‘ç»œï¼‰
        self.critic = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dims[0]),
            nn.ReLU(),
            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.ReLU(),
            nn.Linear(hidden_dims[1], 1)
        )
        
        self.to(self.device)
        
        print(f"ğŸ”„ RLæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}")
        print(f"   çŠ¶æ€ç»´åº¦: {state_dim}, åŠ¨ä½œç»´åº¦: {action_dim}")
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ - è¿”å›åŠ¨ä½œ"""
        return self.actor(state)
    
    def get_action(self, state: np.ndarray, noise_scale: float = 0.1) -> np.ndarray:
        """è·å–åŠ¨ä½œï¼ˆå¸¦æ¢ç´¢å™ªå£°ï¼‰"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action = self.forward(state_tensor).squeeze(0)
            # æ·»åŠ æ¢ç´¢å™ªå£°
            noise = torch.randn_like(action) * noise_scale
            action = action + noise
            # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
            action = torch.clamp(action, -1.0, 1.0)
        
        return action.cpu().numpy()
    
    def get_value(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """è·å–çŠ¶æ€-åŠ¨ä½œä»·å€¼"""
        return self.critic(torch.cat([state, action], dim=1))


class SolverEnvironment:
    """
    æ±‚è§£å™¨ç¯å¢ƒ - æ¨¡æ‹Ÿæ•°å€¼æ±‚è§£è¿‡ç¨‹ï¼Œä¸ºRLæä¾›è®­ç»ƒç¯å¢ƒ
    
    æ ¸å¿ƒæ€æƒ³ï¼šå°†æ•°å€¼æ±‚è§£è¿‡ç¨‹å»ºæ¨¡ä¸ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œ
    æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜çš„æ±‚è§£ç­–ç•¥
    """
    
    def __init__(self, solver_config: Dict, physics_config: PhysicsConfig = None):
        self.solver_config = solver_config
        self.physics_config = physics_config or PhysicsConfig()
        self.max_steps = solver_config.get('max_steps', 100)
        self.current_step = 0
        self.convergence_history = []
        self.performance_metrics = {}
        
        # æ±‚è§£ç­–ç•¥å‚æ•°èŒƒå›´
        self.action_bounds = {
            'time_step': (0.001, 0.1),      # æ—¶é—´æ­¥é•¿
            'mesh_refinement': (0.1, 2.0),  # ç½‘æ ¼åŠ å¯†å› å­
            'tolerance': (1e-6, 1e-3),      # æ”¶æ•›å®¹å·®
            'max_iterations': (50, 500)      # æœ€å¤§è¿­ä»£æ¬¡æ•°
        }
        
        print(f"ğŸ”„ æ±‚è§£å™¨ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"   æœ€å¤§æ­¥æ•°: {self.max_steps}")
        print(f"   åŠ¨ä½œå‚æ•°: {list(self.action_bounds.keys())}")
    
    def reset(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒ"""
        self.current_step = 0
        self.convergence_history = []
        self.performance_metrics = {}
        
        # è¿”å›åˆå§‹çŠ¶æ€
        initial_state = self._get_state()
        return initial_state
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ"""
        if self.current_step >= self.max_steps:
            return self._get_state(), 0.0, True, {}
        
        # è§£æåŠ¨ä½œï¼ˆä»[-1,1]æ˜ å°„åˆ°å®é™…å‚æ•°èŒƒå›´ï¼‰
        solver_params = self._action_to_params(action)
        
        # æ¨¡æ‹Ÿæ±‚è§£è¿‡ç¨‹
        reward, metrics = self._simulate_solving(solver_params)
        
        # æ›´æ–°çŠ¶æ€
        self.current_step += 1
        self.convergence_history.append(metrics.get('convergence', 0.0))
        self.performance_metrics.update(metrics)
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
        done = self.current_step >= self.max_steps or metrics.get('converged', False)
        
        return self._get_state(), reward, done, metrics
    
    def _action_to_params(self, action: np.ndarray) -> Dict:
        """å°†åŠ¨ä½œè½¬æ¢ä¸ºæ±‚è§£å™¨å‚æ•°"""
        params = {}
        action_names = list(self.action_bounds.keys())
        
        for i, name in enumerate(action_names):
            if i < len(action):
                # å°†[-1,1]æ˜ å°„åˆ°å®é™…èŒƒå›´
                action_val = action[i]
                min_val, max_val = self.action_bounds[name]
                param_val = min_val + (action_val + 1) * (max_val - min_val) / 2
                params[name] = param_val
        
        return params
    
    def _simulate_solving(self, solver_params: Dict) -> Tuple[float, Dict]:
        """æ¨¡æ‹Ÿæ±‚è§£è¿‡ç¨‹ï¼Œè®¡ç®—å¥–åŠ±å’ŒæŒ‡æ ‡"""
        # æ¨¡æ‹Ÿæ±‚è§£æ—¶é—´ï¼ˆåŸºäºå‚æ•°ï¼‰
        time_step = solver_params.get('time_step', 0.01)
        mesh_refinement = solver_params.get('mesh_refinement', 1.0)
        tolerance = solver_params.get('tolerance', 1e-4)
        max_iterations = solver_params.get('max_iterations', 100)
        
        # æ¨¡æ‹Ÿæ”¶æ•›è¿‡ç¨‹
        convergence_rate = 1.0 / (1.0 + tolerance * 1000)  # å®¹å·®è¶Šå°ï¼Œæ”¶æ•›è¶Šå¿«
        mesh_efficiency = 1.0 / (1.0 + abs(mesh_refinement - 1.0))  # ç½‘æ ¼å› å­æ¥è¿‘1æ—¶æ•ˆç‡æœ€é«˜
        
        # æ¨¡æ‹Ÿè¿­ä»£æ¬¡æ•°
        actual_iterations = min(max_iterations, int(50 / convergence_rate))
        
        # è®¡ç®—å¥–åŠ±ï¼ˆç»¼åˆè€ƒè™‘æ•ˆç‡ã€ç²¾åº¦ã€ç¨³å®šæ€§ï¼‰
        efficiency_reward = 1.0 / (1.0 + time_step * 100)  # æ—¶é—´æ­¥é•¿è¶Šå°è¶Šå¥½
        accuracy_reward = 1.0 / (1.0 + tolerance * 1e6)    # å®¹å·®è¶Šå°è¶Šå¥½
        stability_reward = 1.0 / (1.0 + abs(mesh_refinement - 1.0))  # ç½‘æ ¼ç¨³å®šæ€§
        
        # æ”¶æ•›å¥–åŠ±
        converged = actual_iterations < max_iterations
        convergence_reward = 10.0 if converged else 0.0
        
        # æ€»å¥–åŠ±
        total_reward = (efficiency_reward + accuracy_reward + stability_reward + convergence_reward) / 4
        
        # æ€§èƒ½æŒ‡æ ‡
        metrics = {
            'convergence': convergence_rate,
            'mesh_efficiency': mesh_efficiency,
            'iterations': actual_iterations,
            'converged': converged,
            'time_step': time_step,
            'mesh_refinement': mesh_refinement,
            'tolerance': tolerance
        }
        
        return total_reward, metrics
    
    def _get_state(self) -> np.ndarray:
        """è·å–å½“å‰çŠ¶æ€"""
        state = []
        
        # å½“å‰æ­¥æ•°ï¼ˆå½’ä¸€åŒ–ï¼‰
        state.append(self.current_step / self.max_steps)
        
        # æ”¶æ•›å†å²ç»Ÿè®¡
        if self.convergence_history:
            state.extend([
                np.mean(self.convergence_history),
                np.std(self.convergence_history),
                self.convergence_history[-1] if self.convergence_history else 0.0
            ])
        else:
            state.extend([0.0, 0.0, 0.0])
        
        # æ€§èƒ½æŒ‡æ ‡
        for key in ['mesh_efficiency', 'iterations']:
            if key in self.performance_metrics:
                # å½’ä¸€åŒ–åˆ°[0,1]
                if key == 'iterations':
                    val = self.performance_metrics[key] / 500.0  # å‡è®¾æœ€å¤§500æ¬¡è¿­ä»£
                else:
                    val = self.performance_metrics[key]
                state.append(val)
            else:
                state.append(0.0)
        
        return np.array(state, dtype=np.float32)


class RLSolverOptimizer(BaseSolver):
    """
    å¼ºåŒ–å­¦ä¹ æ±‚è§£å™¨ä¼˜åŒ–å™¨ - ä½¿ç”¨RLè‡ªåŠ¨ä¼˜åŒ–æ•°å€¼æ±‚è§£ç­–ç•¥
    
    æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ™ºèƒ½ä½“ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„æ±‚è§£å‚æ•°ï¼Œ
    å®ç°"è‡ªå­¦ä¹ "çš„æ•°å€¼æ±‚è§£ä¼˜åŒ–
    """
    
    def __init__(self, state_dim: int, action_dim: int, solver_config: Dict = None):
        super().__init__()
        
        if not HAS_PYTORCH:
            raise ImportError("éœ€è¦å®‰è£…PyTorchæ¥ä½¿ç”¨RLæ±‚è§£å™¨ä¼˜åŒ–å™¨")
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.solver_config = solver_config or {}
        
        # åˆ›å»ºRLæ™ºèƒ½ä½“å’Œç¯å¢ƒ
        self.agent = RLAgent(state_dim, action_dim)
        self.environment = SolverEnvironment(solver_config)
        
        # è®­ç»ƒå‚æ•°
        self.learning_rate = 0.001
        self.gamma = 0.99  # æŠ˜æ‰£å› å­
        self.tau = 0.005   # è½¯æ›´æ–°å‚æ•°
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = []
        self.buffer_size = 10000
        self.batch_size = 64
        
        # ç›®æ ‡ç½‘ç»œï¼ˆç”¨äºç¨³å®šè®­ç»ƒï¼‰
        self.target_agent = RLAgent(state_dim, action_dim)
        self._update_target_network()
        
        self.optimizer_actor = optim.Adam(self.agent.actor.parameters(), lr=self.learning_rate)
        self.optimizer_critic = optim.Adam(self.agent.critic.parameters(), lr=self.learning_rate)
        
        print(f"ğŸ”„ RLæ±‚è§£å™¨ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   çŠ¶æ€ç»´åº¦: {state_dim}, åŠ¨ä½œç»´åº¦: {action_dim}")
    
    def _update_target_network(self):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(self.target_agent.parameters(), self.agent.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
    
    def _store_experience(self, state: np.ndarray, action: np.ndarray, 
                         reward: float, next_state: np.ndarray, done: bool):
        """å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº"""
        experience = (state, action, reward, next_state, done)
        self.replay_buffer.append(experience)
        
        # é™åˆ¶ç¼“å†²åŒºå¤§å°
        if len(self.replay_buffer) > self.buffer_size:
            self.replay_buffer.pop(0)
    
    def _sample_batch(self) -> List[Tuple]:
        """ä»å›æ”¾ç¼“å†²åŒºé‡‡æ ·æ‰¹æ¬¡"""
        if len(self.replay_buffer) < self.batch_size:
            return []
        
        indices = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)
        return [self.replay_buffer[i] for i in indices]
    
    def _update_networks(self, batch: List[Tuple]):
        """æ›´æ–°ç½‘ç»œå‚æ•°"""
        if not batch:
            return
        
        states = torch.FloatTensor(np.array([exp[0] for exp in batch])).to(self.device)
        actions = torch.FloatTensor(np.array([exp[1] for exp in batch])).to(self.device)
        rewards = torch.FloatTensor(np.array([exp[2] for exp in batch])).to(self.device)
        next_states = torch.FloatTensor(np.array([exp[3] for exp in batch])).to(self.device)
        dones = torch.BoolTensor(np.array([exp[4] for exp in batch])).to(self.device)
        
        # æ›´æ–°Criticç½‘ç»œ
        current_q_values = self.agent.get_value(states, actions)
        next_actions = self.target_agent(next_states)
        next_q_values = self.target_agent.get_value(next_states, next_actions)
        target_q_values = rewards + (self.gamma * next_q_values * (~dones).float())
        
        critic_loss = F.mse_loss(current_q_values, target_q_values.detach())
        
        self.optimizer_critic.zero_grad()
        critic_loss.backward()
        self.optimizer_critic.step()
        
        # æ›´æ–°Actorç½‘ç»œ
        actor_loss = -self.agent.get_value(states, self.agent(states)).mean()
        
        self.optimizer_actor.zero_grad()
        actor_loss.backward()
        self.optimizer_actor.step()
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self._update_target_network()
        
        return {
            'critic_loss': critic_loss.item(),
            'actor_loss': actor_loss.item()
        }
    
    def train(self, episodes: int = 1000, **kwargs) -> dict:
        """è®­ç»ƒRLæ™ºèƒ½ä½“"""
        print(f"ğŸ”„ å¼€å§‹è®­ç»ƒRLæ±‚è§£å™¨ä¼˜åŒ–å™¨ï¼Œæ€»è½®æ•°: {episodes}")
        
        episode_rewards = []
        episode_lengths = []
        training_losses = []
        
        for episode in range(episodes):
            state = self.environment.reset()
            episode_reward = 0.0
            episode_length = 0
            
            while True:
                # é€‰æ‹©åŠ¨ä½œ
                action = self.agent.get_action(state, noise_scale=max(0.01, 0.1 * (1 - episode / episodes)))
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = self.environment.step(action)
                
                # å­˜å‚¨ç»éªŒ
                self._store_experience(state, action, reward, next_state, done)
                
                # æ›´æ–°ç½‘ç»œ
                batch = self._sample_batch()
                if batch:
                    loss_info = self._update_networks(batch)
                    training_losses.append(loss_info)
                
                state = next_state
                episode_reward += reward
                episode_length += 1
                
                if done:
                    break
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                avg_length = np.mean(episode_lengths[-100:])
                print(f"   è½®æ•° {episode+1}/{episodes}: å¹³å‡å¥–åŠ±={avg_reward:.4f}, å¹³å‡é•¿åº¦={avg_length:.1f}")
        
        self.is_trained = True
        
        training_history = {
            'episodes': episodes,
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'training_losses': training_losses,
            'final_avg_reward': np.mean(episode_rewards[-100:]) if episode_rewards else 0.0
        }
        
        print(f"âœ… RLè®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆå¹³å‡å¥–åŠ±: {training_history['final_avg_reward']:.4f}")
        return training_history
    
    def optimize_solver_strategy(self, problem_state: np.ndarray) -> Dict:
        """ä¼˜åŒ–æ±‚è§£ç­–ç•¥"""
        if not self.is_trained:
            raise ValueError("RLæ™ºèƒ½ä½“å°šæœªè®­ç»ƒ")
        
        # ä½¿ç”¨è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
        optimal_action = self.agent.get_action(problem_state, noise_scale=0.0)
        
        # è½¬æ¢ä¸ºæ±‚è§£å™¨å‚æ•°
        solver_params = self.environment._action_to_params(optimal_action)
        
        print(f"ğŸ”§ RLä¼˜åŒ–æ±‚è§£ç­–ç•¥:")
        for param, value in solver_params.items():
            print(f"   {param}: {value:.6f}")
        
        return solver_params
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹æœ€ä¼˜æ±‚è§£ç­–ç•¥"""
        if not self.is_trained:
            raise ValueError("RLæ™ºèƒ½ä½“å°šæœªè®­ç»ƒ")
        
        strategies = []
        for state in X:
            action = self.agent.get_action(state, noise_scale=0.0)
            strategy = self.environment._action_to_params(action)
            strategies.append(list(strategy.values()))
        
        return np.array(strategies)
    
    def evaluate_strategy(self, strategy: Dict, problem_state: np.ndarray) -> Dict:
        """è¯„ä¼°æ±‚è§£ç­–ç•¥çš„æ€§èƒ½"""
        # å°†ç­–ç•¥è½¬æ¢ä¸ºåŠ¨ä½œ
        action = np.array([strategy.get(param, 0.0) for param in self.environment.action_bounds.keys()])
        
        # åœ¨ç¯å¢ƒä¸­æµ‹è¯•ç­–ç•¥
        state = problem_state
        total_reward = 0.0
        step_count = 0
        
        for _ in range(self.environment.max_steps):
            next_state, reward, done, info = self.environment.step(action)
            total_reward += reward
            step_count += 1
            
            if done:
                break
        
        return {
            'total_reward': total_reward,
            'step_count': step_count,
            'efficiency': total_reward / max(step_count, 1),
            'convergence': info.get('converged', False)
        }


def create_rl_solver_system() -> Dict:
    """åˆ›å»ºRLæ±‚è§£å™¨ç³»ç»Ÿ"""
    system = {
        'agent': RLAgent,
        'environment': SolverEnvironment,
        'optimizer': RLSolverOptimizer
    }
    
    print("ğŸ”„ RLæ±‚è§£å™¨ç³»ç»Ÿåˆ›å»ºå®Œæˆ")
    return system


def demo_rl_solver_optimization():
    """æ¼”ç¤ºRLæ±‚è§£å™¨ä¼˜åŒ–"""
    print("ğŸ¤– å¼ºåŒ–å­¦ä¹ æ±‚è§£å™¨ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # åˆ›å»ºRLæ±‚è§£å™¨ç³»ç»Ÿ
        rl_system = create_rl_solver_system()
        
        # é…ç½®æ±‚è§£å™¨ç¯å¢ƒ
        solver_config = {
            'max_steps': 50,
            'convergence_threshold': 1e-6
        }
        
        # åˆ›å»ºç¯å¢ƒ
        env = rl_system['environment'](solver_config)
        state_dim = len(env.reset())
        action_dim = len(env.action_bounds)
        
        print(f"ğŸ“Š ç¯å¢ƒé…ç½®:")
        print(f"   çŠ¶æ€ç»´åº¦: {state_dim}")
        print(f"   åŠ¨ä½œç»´åº¦: {action_dim}")
        print(f"   æœ€å¤§æ­¥æ•°: {env.max_steps}")
        
        # åˆ›å»ºRLä¼˜åŒ–å™¨
        rl_optimizer = rl_system['optimizer'](state_dim, action_dim, solver_config)
        
        # è®­ç»ƒRLæ™ºèƒ½ä½“
        print("\nğŸ”§ è®­ç»ƒRLæ™ºèƒ½ä½“...")
        training_history = rl_optimizer.train(episodes=500)
        
        print(f"   è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆå¹³å‡å¥–åŠ±: {training_history['final_avg_reward']:.4f}")
        
        # æµ‹è¯•ä¼˜åŒ–åçš„ç­–ç•¥
        print("\nğŸ”§ æµ‹è¯•ä¼˜åŒ–åçš„æ±‚è§£ç­–ç•¥...")
        
        # æ¨¡æ‹Ÿé—®é¢˜çŠ¶æ€
        test_state = np.array([0.0, 0.5, 0.1, 0.8, 0.3])
        
        # è·å–æœ€ä¼˜ç­–ç•¥
        optimal_strategy = rl_optimizer.optimize_solver_strategy(test_state)
        
        # è¯„ä¼°ç­–ç•¥æ€§èƒ½
        performance = rl_optimizer.evaluate_strategy(optimal_strategy, test_state)
        
        print(f"   ç­–ç•¥æ€§èƒ½è¯„ä¼°:")
        print(f"     æ€»å¥–åŠ±: {performance['total_reward']:.4f}")
        print(f"     æ­¥æ•°: {performance['step_count']}")
        print(f"     æ•ˆç‡: {performance['efficiency']:.4f}")
        print(f"     æ”¶æ•›: {performance['convergence']}")
        
        print("\nâœ… RLæ±‚è§£å™¨ä¼˜åŒ–æ¼”ç¤ºå®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ RLæ±‚è§£å™¨ä¼˜åŒ–æ¼”ç¤ºå¤±è´¥: {e}")
        return False


def demo_advanced_ml():
    """æ¼”ç¤ºé«˜çº§MLåŠŸèƒ½"""
    print("ğŸ¤– é«˜çº§æœºå™¨å­¦ä¹ åŠ é€Ÿæ•°å€¼æ¨¡æ‹Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
    np.random.seed(42)
    if HAS_PYTORCH:
        torch.manual_seed(42)
    
    # åˆ›å»ºé«˜çº§MLç³»ç»Ÿ
    ml_system = create_advanced_ml_system()
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    n_samples = 1000
    input_dim = 5
    output_dim = 3
    
    X = np.random.randn(n_samples, input_dim)
    y = np.random.randn(n_samples, output_dim)
    
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®: {n_samples} æ ·æœ¬, è¾“å…¥ç»´åº¦: {input_dim}, è¾“å‡ºç»´åº¦: {output_dim}")
    
    # 1. æµ‹è¯•é«˜çº§PINN
    print("\nğŸ”§ æµ‹è¯•é«˜çº§PINN...")
    try:
        # å®šä¹‰ç‰©ç†æ–¹ç¨‹ï¼ˆç¤ºä¾‹ï¼šçƒ­ä¼ å¯¼æ–¹ç¨‹ï¼‰
        def heat_equation(x, y):
            return torch.mean(torch.abs(y - 0.1 * torch.sum(x, dim=1, keepdim=True)))
        
        # å®šä¹‰è¾¹ç•Œæ¡ä»¶
        def boundary_condition(x, y):
            return torch.mean(torch.abs(y[:, 0] - 0.0))  # ç¬¬ä¸€ä¸ªè¾“å‡ºåœ¨è¾¹ç•Œä¸Šä¸º0
        
        pinn = ml_system['pinn'](input_dim, [64, 32], output_dim, 
                                physics_equations=[heat_equation],
                                boundary_conditions=[boundary_condition])
        
        pinn.setup_training()
        result = pinn.train(X, y, epochs=200)
        print(f"   è®­ç»ƒæ—¶é—´: {result['train_time']:.4f} ç§’")
        print(f"   æœ€ç»ˆæ€»æŸå¤±: {result['total_loss'][-1]:.6f}")
        
    except Exception as e:
        print(f"   âŒ é«˜çº§PINNå¤±è´¥: {e}")
    
    # 2. æµ‹è¯•é«˜çº§ä»£ç†æ¨¡å‹
    print("\nğŸ”§ æµ‹è¯•é«˜çº§ä»£ç†æ¨¡å‹...")
    try:
        surrogate = ml_system['surrogate']('gaussian_process', kernel=RBF(length_scale=1.0) + Matern(length_scale=1.0))
        result = surrogate.train(X, y[:, 0])  # åªé¢„æµ‹ç¬¬ä¸€ä¸ªè¾“å‡º
        print(f"   è®­ç»ƒæ—¶é—´: {result['training_time']:.4f} ç§’")
        
        # é¢„æµ‹
        predictions, std = surrogate.predict(X[:10], return_std=True)
        print(f"   é¢„æµ‹å½¢çŠ¶: {predictions.shape}, æ ‡å‡†å·®å½¢çŠ¶: {std.shape}")
        
    except Exception as e:
        print(f"   âŒ é«˜çº§ä»£ç†æ¨¡å‹å¤±è´¥: {e}")
    
    # 3. æµ‹è¯•å¤šå°ºåº¦æ¡¥æ¥
    print("\nğŸ”§ æµ‹è¯•å¤šå°ºåº¦æ¡¥æ¥...")
    try:
        bridge = ml_system['bridge']()
        bridge.setup_bridge_model(input_dim, output_dim, 'neural_network')
        
        # æ¨¡æ‹Ÿç»†å°ºåº¦å’Œç²—å°ºåº¦æ•°æ®
        fine_data = X
        coarse_data = y
        
        result = bridge.train_bridge(fine_data, coarse_data, epochs=100)
        print(f"   æ¡¥æ¥æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # æµ‹è¯•æ¡¥æ¥
        coarse_pred = bridge.predict_coarse_from_fine(X[:10])
        print(f"   ç²—å°ºåº¦é¢„æµ‹å½¢çŠ¶: {coarse_pred.shape}")
        
    except Exception as e:
        print(f"   âŒ å¤šå°ºåº¦æ¡¥æ¥å¤±è´¥: {e}")
    
    # 4. æµ‹è¯•æ··åˆåŠ é€Ÿå™¨
    print("\nğŸ”§ æµ‹è¯•æ··åˆåŠ é€Ÿå™¨...")
    try:
        hybrid_accelerator = ml_system['hybrid']()
        
        # æ·»åŠ MLæ¨¡å‹
        surrogate_model = ml_system['surrogate']('random_forest')
        surrogate_model.train(X, y[:, 0])
        hybrid_accelerator.add_ml_model('surrogate', surrogate_model)
        
        # è®¾ç½®åŠ é€Ÿç­–ç•¥
        hybrid_accelerator.setup_acceleration_strategy('initial_guess', 'surrogate')
        
        # æµ‹è¯•æ··åˆæ±‚è§£
        problem_data = {'input': X[:10]}
        result = hybrid_accelerator.solve_hybrid(problem_data, use_ml=True, ml_model_name='surrogate')
        print(f"   æ··åˆæ±‚è§£å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {result['model_name']}")
        print(f"   æ±‚è§£æ—¶é—´: {result['time']:.4f} ç§’")
        
    except Exception as e:
        print(f"   âŒ æ··åˆåŠ é€Ÿå™¨å¤±è´¥: {e}")
    
    # 5. æµ‹è¯•è‡ªé€‚åº”æ±‚è§£å™¨
    print("\nğŸ”§ æµ‹è¯•è‡ªé€‚åº”æ±‚è§£å™¨...")
    try:
        adaptive_solver = ml_system['adaptive']()
        
        # æ·»åŠ ä¸åŒçš„æ±‚è§£å™¨
        def fast_solver(data):
            return np.random.randn(data.get('size', 100))
        
        def accurate_solver(data):
            time.sleep(0.01)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
            return np.random.randn(data.get('size', 100))
        
        # ä½¿ç”¨æ–°çš„æ¡ä»¶æ ¼å¼
        adaptive_solver.add_solver('fast', fast_solver, 
                                 conditions={'size': ('<', 1000)}, priority=1)
        adaptive_solver.add_solver('accurate', accurate_solver, 
                                 conditions={'size': ('>=', 1000)}, priority=2)
        
        # è®¾ç½®é€‰æ‹©ç­–ç•¥å’Œè¯„åˆ†æƒé‡
        adaptive_solver.set_selection_strategy('hybrid')
        adaptive_solver.set_score_weights({
            'problem_feature': 1.0,
            'accuracy': 0.6,
            'speed': 0.4,
            'priority': 0.1
        })
        
        # æµ‹è¯•æ±‚è§£
        problem_data = {'size': 500, 'accuracy_requirement': 0.8}
        result = adaptive_solver.solve(problem_data)
        print(f"   ä½¿ç”¨çš„æ±‚è§£å™¨: {result['solver_used']}")
        print(f"   æ±‚è§£æ—¶é—´: {result['time']:.4f} ç§’")
        
    except Exception as e:
        print(f"   âŒ è‡ªé€‚åº”æ±‚è§£å™¨å¤±è´¥: {e}")
    
    print("\nâœ… é«˜çº§æœºå™¨å­¦ä¹ åŠ é€Ÿæ•°å€¼æ¨¡æ‹Ÿæ¼”ç¤ºå®Œæˆ!")


def create_rl_solver_system() -> Dict:
    """åˆ›å»ºRLæ±‚è§£å™¨ç³»ç»Ÿ"""
    system = {
        'agent': RLAgent,
        'environment': SolverEnvironment,
        'optimizer': RLSolverOptimizer
    }
    
    print("ğŸ”„ RLæ±‚è§£å™¨ç³»ç»Ÿåˆ›å»ºå®Œæˆ")
    return system


def demo_rl_solver_optimization():
    """æ¼”ç¤ºRLæ±‚è§£å™¨ä¼˜åŒ–"""
    print("ğŸ¤– å¼ºåŒ–å­¦ä¹ æ±‚è§£å™¨ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # åˆ›å»ºRLæ±‚è§£å™¨ç³»ç»Ÿ
        rl_system = create_rl_solver_system()
        
        # é…ç½®æ±‚è§£å™¨ç¯å¢ƒ
        solver_config = {
            'max_steps': 50,
            'convergence_threshold': 1e-6
        }
        
        # åˆ›å»ºç¯å¢ƒ
        env = rl_system['environment'](solver_config)
        state_dim = len(env.reset())
        action_dim = len(env.action_bounds)
        
        print(f"ğŸ“Š ç¯å¢ƒé…ç½®:")
        print(f"   çŠ¶æ€ç»´åº¦: {state_dim}")
        print(f"   åŠ¨ä½œç»´åº¦: {action_dim}")
        print(f"   æœ€å¤§æ­¥æ•°: {env.max_steps}")
        
        # åˆ›å»ºRLä¼˜åŒ–å™¨
        rl_optimizer = rl_system['optimizer'](state_dim, action_dim, solver_config)
        
        # è®­ç»ƒRLæ™ºèƒ½ä½“
        print("\nğŸ”§ è®­ç»ƒRLæ™ºèƒ½ä½“...")
        training_history = rl_optimizer.train(episodes=500)
        
        print(f"   è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆå¹³å‡å¥–åŠ±: {training_history['final_avg_reward']:.4f}")
        
        # æµ‹è¯•ä¼˜åŒ–åçš„ç­–ç•¥
        print("\nğŸ”§ æµ‹è¯•ä¼˜åŒ–åçš„æ±‚è§£ç­–ç•¥...")
        
        # æ¨¡æ‹Ÿé—®é¢˜çŠ¶æ€
        test_state = np.array([0.0, 0.5, 0.1, 0.8, 0.3])
        
        # è·å–æœ€ä¼˜ç­–ç•¥
        optimal_strategy = rl_optimizer.optimize_solver_strategy(test_state)
        
        # è¯„ä¼°ç­–ç•¥æ€§èƒ½
        performance = rl_optimizer.evaluate_strategy(optimal_strategy, test_state)
        
        print(f"   ç­–ç•¥æ€§èƒ½è¯„ä¼°:")
        print(f"     æ€»å¥–åŠ±: {performance['total_reward']:.4f}")
        print(f"     æ­¥æ•°: {performance['step_count']}")
        print(f"     æ•ˆç‡: {performance['efficiency']:.4f}")
        print(f"     æ”¶æ•›: {performance['convergence']}")
        
        print("\nâœ… RLæ±‚è§£å™¨ä¼˜åŒ–æ¼”ç¤ºå®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ RLæ±‚è§£å™¨ä¼˜åŒ–æ¼”ç¤ºå¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    demo_advanced_ml()
    demo_rl_solver_optimization()
