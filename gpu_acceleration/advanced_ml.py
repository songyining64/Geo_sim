"""
È´òÁ∫ßÊú∫Âô®Â≠¶‰π†Âä†ÈÄüÊï∞ÂÄºÊ®°ÊãüÊ°ÜÊû∂
"""

import numpy as np
import time
import warnings
from typing import Dict, List, Tuple, Optional, Callable, Union, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod

# Ê∑±Â∫¶Â≠¶‰π†Áõ∏ÂÖ≥‰æùËµñ
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    from torch.utils.data import DataLoader, TensorDataset
    from torch.nn.parameter import Parameter
    HAS_PYTORCH = True
except ImportError:
    HAS_PYTORCH = False
    torch = None
    nn = None
    optim = None
    warnings.warn("PyTorch not available. Advanced ML features will be limited.")

try:
    import sklearn
    from sklearn.gaussian_process import GaussianProcessRegressor
    from sklearn.gaussian_process.kernels import RBF, ConstantKernel, Matern
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.neural_network import MLPRegressor
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    warnings.warn("scikit-learn not available. Advanced ML features will be limited.")

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    warnings.warn("matplotlib not available. Visualization features will be limited.")


@dataclass
class PhysicsConfig:
    """Áâ©ÁêÜÈÖçÁΩÆÁ±ª"""
    # Âú∞Ë¥®Ê®°ÊãüÂ∏∏Áî®ÂèÇÊï∞
    rayleigh_number: float = 1e6
    prandtl_number: float = 1.0
    gravity: float = 9.81
    thermal_expansion: float = 3e-5
    thermal_diffusivity: float = 1e-6
    reference_density: float = 3300.0
    reference_viscosity: float = 1e21
    
    # ËæπÁïåÊù°‰ª∂
    boundary_conditions: Dict = None
    
    def __post_init__(self):
        if self.boundary_conditions is None:
            self.boundary_conditions = {
                'temperature': {'top': 0, 'bottom': 1},
                'velocity': {'top': 'free_slip', 'bottom': 'free_slip'},
                'pressure': {'top': 'free', 'bottom': 'free'}
            }


class BaseSolver(ABC):
    """Âü∫Á°ÄÊ±ÇËß£Âô®ÊäΩË±°Á±ª - ÂÆûÁé∞‰ª£Á†ÅÂ§çÁî®"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.is_trained = False
        self.training_history = {}
    
    @abstractmethod
    def train(self, X: np.ndarray, y: np.ndarray, **kwargs) -> dict:
        """ËÆ≠ÁªÉÊ®°Âûã"""
        pass
    
    @abstractmethod
    def predict(self, X: np.ndarray) -> np.ndarray:
        """È¢ÑÊµã"""
        pass
    
    def save_model(self, filepath: str):
        """‰øùÂ≠òÊ®°Âûã"""
        try:
            if hasattr(self, 'state_dict'):
                torch.save(self.state_dict(), filepath)
                print(f"üìÅ Ê®°ÂûãÂ∑≤‰øùÂ≠ò: {filepath}")
            else:
                import pickle
                with open(filepath, 'wb') as f:
                    pickle.dump(self, f)
                print(f"üìÅ Ê®°ÂûãÂ∑≤‰øùÂ≠ò: {filepath}")
        except Exception as e:
            raise RuntimeError(f"‰øùÂ≠òÊ®°ÂûãÂ§±Ë¥•Ôºö{str(e)}")
    
    def load_model(self, filepath: str):
        """Âä†ËΩΩÊ®°Âûã"""
        try:
            if hasattr(self, 'load_state_dict'):
                self.load_state_dict(torch.load(filepath, map_location=self.device))
                print(f"üìÅ Ê®°ÂûãÂ∑≤Âä†ËΩΩ: {filepath}")
            else:
                import pickle
                with open(filepath, 'rb') as f:
                    loaded_model = pickle.load(f)
                    self.__dict__.update(loaded_model.__dict__)
                print(f"üìÅ Ê®°ÂûãÂ∑≤Âä†ËΩΩ: {filepath}")
        except FileNotFoundError:
            raise FileNotFoundError(f"Ê®°ÂûãÊñá‰ª∂‰∏çÂ≠òÂú®Ôºö{filepath}")
        except Exception as e:
            raise RuntimeError(f"Âä†ËΩΩÊ®°ÂûãÂ§±Ë¥•Ôºö{str(e)}")


class PhysicsInformedNeuralNetwork(BaseSolver, nn.Module):
    """
    Áâ©ÁêÜ‰ø°ÊÅØÁ•ûÁªèÁΩëÁªúÔºàPINNÔºâ - È´òÁ∫ßÁâàÊú¨
    
    Ê†∏ÂøÉÊÄùÊÉ≥ÔºöÂ∞ÜÁâ©ÁêÜÊñπÁ®ã‰Ωú‰∏∫ËΩØÁ∫¶ÊùüÂµåÂÖ•Á•ûÁªèÁΩëÁªúÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå
    Âº∫Âà∂Ê®°ÂûãËæìÂá∫Êª°Ë∂≥Áâ©ÁêÜËßÑÂæãÔºåÂÆûÁé∞"Â∞èÊï∞ÊçÆ+Áâ©ÁêÜÁü•ËØÜ"ÁöÑËÅîÂêàÂ≠¶‰π†
    """
    
    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int,
                 physics_equations: List[Callable] = None,
                 boundary_conditions: List[Callable] = None,
                 physics_config: PhysicsConfig = None):
        BaseSolver.__init__(self)
        nn.Module.__init__(self)
        
        if not HAS_PYTORCH:
            raise ImportError("ÈúÄË¶ÅÂÆâË£ÖPyTorchÊù•‰ΩøÁî®PINN")
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.output_dim = output_dim
        self.physics_equations = physics_equations or []
        self.boundary_conditions = boundary_conditions or []
        self.physics_config = physics_config or PhysicsConfig()
        
        # ÊûÑÂª∫ÁΩëÁªú - ‰ΩøÁî®ÊÆãÂ∑ÆËøûÊé•ÂíåÊâπÂΩí‰∏ÄÂåñ
        self.layers = nn.ModuleList()
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            self.layers.append(nn.Linear(prev_dim, hidden_dim))
            self.layers.append(nn.BatchNorm1d(hidden_dim))
            self.layers.append(nn.Tanh())  # PINNÈÄöÂ∏∏‰ΩøÁî®TanhÊøÄÊ¥ªÂáΩÊï∞
            prev_dim = hidden_dim
        
        self.output_layer = nn.Linear(prev_dim, output_dim)
        
        # ÂàùÂßãÂåñÊùÉÈáç
        self._initialize_weights()
        
        self.optimizer = None
        self.to(self.device)
        
        print(f"üîÑ È´òÁ∫ßPINNÂàùÂßãÂåñÂÆåÊàê - ËÆæÂ§á: {self.device}")
        print(f"   ÁΩëÁªúÁªìÊûÑ: {input_dim} -> {hidden_dims} -> {output_dim}")
    
    def _initialize_weights(self):
        """ÂàùÂßãÂåñÁΩëÁªúÊùÉÈáç"""
        for layer in self.layers:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_normal_(layer.weight)
                nn.init.constant_(layer.bias, 0)
        nn.init.xavier_normal_(self.output_layer.weight)
        nn.init.constant_(self.output_layer.bias, 0)
    
    def forward(self, x):
        """ÂâçÂêë‰º†Êí≠"""
        for layer in self.layers:
            x = layer(x)
        return self.output_layer(x)
    
    def setup_training(self, learning_rate: float = 0.001, weight_decay: float = 1e-5):
        """ËÆæÁΩÆËÆ≠ÁªÉÂèÇÊï∞"""
        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=weight_decay)
    
    def compute_physics_loss(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """ËÆ°ÁÆóÁâ©ÁêÜÁ∫¶ÊùüÊçüÂ§±"""
        if not self.physics_equations:
            return torch.tensor(0.0, device=self.device)
        
        total_loss = torch.tensor(0.0, device=self.device)
        
        for equation in self.physics_equations:
            # ËÆ°ÁÆóÁâ©ÁêÜÊñπÁ®ãÁöÑÊÆãÂ∑Æ
            residual = equation(x, y)
            total_loss += torch.mean(residual ** 2)
        
        return total_loss
    
    def compute_boundary_loss(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """ËÆ°ÁÆóËæπÁïåÊù°‰ª∂ÊçüÂ§±"""
        if not self.boundary_conditions:
            return torch.tensor(0.0, device=self.device)
        
        total_loss = torch.tensor(0.0, device=self.device)
        
        for bc in self.boundary_conditions:
            # ËÆ°ÁÆóËæπÁïåÊù°‰ª∂ÁöÑÊÆãÂ∑Æ
            residual = bc(x, y)
            total_loss += torch.mean(residual ** 2)
        
        return total_loss
    
    def train(self, X: np.ndarray, y: np.ndarray, 
              physics_points: np.ndarray = None,
              boundary_points: np.ndarray = None,
              epochs: int = 1000, 
              physics_weight: float = 1.0,
              boundary_weight: float = 1.0,
              batch_size: int = 32,
              validation_split: float = 0.2) -> dict:
        """ËÆ≠ÁªÉPINNÊ®°Âûã"""
        if self.optimizer is None:
            self.setup_training()
        
        # È™åËØÅËæìÂÖ•Êï∞ÊçÆ
        if X.shape[0] != y.shape[0]:
            raise ValueError(f"XÂíåyÊ†∑Êú¨Êï∞‰∏çÂåπÈÖçÔºö{X.shape[0]} vs {y.shape[0]}")
        
        # Êï∞ÊçÆÂàÜÂâ≤
        if validation_split > 0:
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=validation_split, random_state=42
            )
        else:
            X_train, y_train = X, y
            X_val, y_val = None, None
        
        # ËΩ¨Êç¢‰∏∫Âº†Èáè
        X_train_tensor = torch.FloatTensor(X_train).to(self.device)
        y_train_tensor = torch.FloatTensor(y_train).to(self.device)
        
        if X_val is not None:
            X_val_tensor = torch.FloatTensor(X_val).to(self.device)
            y_val_tensor = torch.FloatTensor(y_val).to(self.device)
        
        if physics_points is not None:
            physics_tensor = torch.FloatTensor(physics_points).to(self.device)
        
        if boundary_points is not None:
            boundary_tensor = torch.FloatTensor(boundary_points).to(self.device)
        
        # ÂàõÂª∫Êï∞ÊçÆÂä†ËΩΩÂô®
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        history = {
            'total_loss': [], 'data_loss': [], 'physics_loss': [], 
            'boundary_loss': [], 'val_loss': [], 'train_time': 0.0
        }
        start_time = time.time()
        
        best_val_loss = float('inf')
        patience = 50
        patience_counter = 0
        
        for epoch in range(epochs):
            self.train()
            epoch_losses = {'total': 0, 'data': 0, 'physics': 0, 'boundary': 0}
            
            for batch_X, batch_y in train_loader:
                self.optimizer.zero_grad()
                
                # Êï∞ÊçÆÊçüÂ§±
                outputs = self(batch_X)
                data_loss = F.mse_loss(outputs, batch_y)
                
                # Áâ©ÁêÜÊçüÂ§±
                if physics_points is not None and self.physics_equations:
                    physics_outputs = self(physics_tensor)
                    physics_loss = self.compute_physics_loss(physics_tensor, physics_outputs)
                else:
                    physics_loss = torch.tensor(0.0, device=self.device)
                
                # ËæπÁïåÊçüÂ§±
                if boundary_points is not None and self.boundary_conditions:
                    boundary_outputs = self(boundary_tensor)
                    boundary_loss = self.compute_boundary_loss(boundary_tensor, boundary_outputs)
                else:
                    boundary_loss = torch.tensor(0.0, device=self.device)
                
                # ÊÄªÊçüÂ§±
                total_loss = data_loss + physics_weight * physics_loss + boundary_weight * boundary_loss
                
                total_loss.backward()
                self.optimizer.step()
                
                # Á¥ØÁßØÊçüÂ§±
                epoch_losses['total'] += total_loss.item()
                epoch_losses['data'] += data_loss.item()
                epoch_losses['physics'] += physics_loss.item()
                epoch_losses['boundary'] += boundary_loss.item()
            
            # ËÆ°ÁÆóÂπ≥ÂùáÊçüÂ§±
            num_batches = len(train_loader)
            for key in epoch_losses:
                epoch_losses[key] /= num_batches
            
            # È™åËØÅÊçüÂ§±
            val_loss = None
            if X_val is not None:
                self.eval()
                with torch.no_grad():
                    val_outputs = self(X_val_tensor)
                    val_loss = F.mse_loss(val_outputs, y_val_tensor).item()
                    history['val_loss'].append(val_loss)
                    
                    # Êó©ÂÅú
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        patience_counter = 0
                    else:
                        patience_counter += 1
                    
                    if patience_counter >= patience:
                        print(f"   Êó©ÂÅúÂú®Á¨¨ {epoch+1} ËΩÆ")
                        break
            
            # ËÆ∞ÂΩïÂéÜÂè≤
            history['total_loss'].append(epoch_losses['total'])
            history['data_loss'].append(epoch_losses['data'])
            history['physics_loss'].append(epoch_losses['physics'])
            history['boundary_loss'].append(epoch_losses['boundary'])
            
            if (epoch + 1) % 100 == 0:
                val_info = f", val_loss={val_loss:.6f}" if val_loss is not None else ""
                print(f"   Epoch {epoch+1}/{epochs}: total_loss={epoch_losses['total']:.6f}, "
                      f"data_loss={epoch_losses['data']:.6f}, physics_loss={epoch_losses['physics']:.6f}, "
                      f"boundary_loss={epoch_losses['boundary']:.6f}{val_info}")
        
        history['train_time'] = time.time() - start_time
        self.is_trained = True
        self.training_history = history
        return history
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """È¢ÑÊµã"""
        if not self.is_trained:
            raise ValueError("Ê®°ÂûãÂ∞öÊú™ËÆ≠ÁªÉ")
        
        self.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)
            outputs = self(X_tensor)
            return outputs.cpu().numpy()


class SurrogateModelAdvanced(BaseSolver):
    """
    È´òÁ∫ß‰ª£ÁêÜÊ®°Âûã - ÊîØÊåÅÂ§öÁßçÁÆóÊ≥ï
    
    Ê†∏ÂøÉÊÄùÊÉ≥ÔºöÁî®‰º†ÁªüÊï∞ÂÄºÊ®°ÊãüÁîüÊàê"ËæìÂÖ•ÂèÇÊï∞‚ÜíËæìÂá∫Áâ©ÁêÜÂú∫"ÁöÑÊï∞ÊçÆÈõÜÔºå
    ËÆ≠ÁªÉDLÊ®°ÂûãÂ≠¶‰π†ËøôÁßçÊò†Â∞ÑÔºåÂêéÁª≠Áî®Ê®°ÂûãÁõ¥Êé•È¢ÑÊµãÔºåÊõø‰ª£ÂÆåÊï¥Ê®°ÊãüÊµÅÁ®ã
    """
    
    def __init__(self, model_type: str = 'gaussian_process', **kwargs):
        super().__init__()
        self.model_type = model_type
        self.model = None
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.kwargs = kwargs
        
        if model_type == 'gaussian_process' and not HAS_SKLEARN:
            raise ImportError("ÈúÄË¶ÅÂÆâË£Öscikit-learnÊù•‰ΩøÁî®È´òÊñØËøáÁ®ãÂõûÂΩí")
    
    def train(self, X: np.ndarray, y: np.ndarray, **kwargs) -> dict:
        """ËÆ≠ÁªÉ‰ª£ÁêÜÊ®°Âûã"""
        # È™åËØÅËæìÂÖ•Êï∞ÊçÆÂêàÊ≥ïÊÄß
        if X.shape[0] != y.shape[0]:
            raise ValueError(f"XÂíåyÊ†∑Êú¨Êï∞‰∏çÂåπÈÖçÔºö{X.shape[0]} vs {y.shape[0]}")
        
        start_time = time.time()
        
        # Êï∞ÊçÆÊ†áÂáÜÂåñ
        X_scaled = self.scaler_X.fit_transform(X)
        y_scaled = self.scaler_y.fit_transform(y.reshape(-1, 1)).flatten()
        
        if self.model_type == 'gaussian_process':
            # È´òÊñØËøáÁ®ãÂõûÂΩí
            kernel = self.kwargs.get('kernel', RBF(length_scale=1.0) + ConstantKernel())
            self.model = GaussianProcessRegressor(kernel=kernel, random_state=42)
            self.model.fit(X_scaled, y_scaled)
        
        elif self.model_type == 'random_forest':
            # ÈöèÊú∫Ê£ÆÊûóÂõûÂΩí
            n_estimators = self.kwargs.get('n_estimators', 100)
            if not isinstance(n_estimators, int) or n_estimators <= 0:
                raise ValueError(f"n_estimatorsÂøÖÈ°ª‰∏∫Ê≠£Êï¥Êï∞ÔºåÂÆûÈôÖ‰∏∫Ôºö{n_estimators}")
            
            max_depth = self.kwargs.get('max_depth', None)
            self.model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
            self.model.fit(X_scaled, y_scaled)
        
        elif self.model_type == 'gradient_boosting':
            # Ê¢ØÂ∫¶ÊèêÂçáÂõûÂΩí
            n_estimators = self.kwargs.get('n_estimators', 100)
            if not isinstance(n_estimators, int) or n_estimators <= 0:
                raise ValueError(f"n_estimatorsÂøÖÈ°ª‰∏∫Ê≠£Êï¥Êï∞ÔºåÂÆûÈôÖ‰∏∫Ôºö{n_estimators}")
            
            learning_rate = self.kwargs.get('learning_rate', 0.1)
            if not isinstance(learning_rate, (int, float)) or learning_rate <= 0:
                raise ValueError(f"learning_rateÂøÖÈ°ª‰∏∫Ê≠£Êï∞ÔºåÂÆûÈôÖ‰∏∫Ôºö{learning_rate}")
            
            self.model = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, random_state=42)
            self.model.fit(X_scaled, y_scaled)
        
        elif self.model_type == 'neural_network':
            # Á•ûÁªèÁΩëÁªúÂõûÂΩí
            hidden_layer_sizes = self.kwargs.get('hidden_layer_sizes', (100, 50))
            max_iter = self.kwargs.get('max_iter', 1000)
            if not isinstance(max_iter, int) or max_iter <= 0:
                raise ValueError(f"max_iterÂøÖÈ°ª‰∏∫Ê≠£Êï¥Êï∞ÔºåÂÆûÈôÖ‰∏∫Ôºö{max_iter}")
            
            self.model = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, max_iter=max_iter, random_state=42)
            self.model.fit(X_scaled, y_scaled)
        
        self.is_trained = True
        training_time = time.time() - start_time
        
        self.training_history = {
            'training_time': training_time,
            'model_type': self.model_type,
            'n_samples': len(X),
            'n_features': X.shape[1]
        }
        
        return self.training_history
    
    def predict(self, X: np.ndarray, return_std: bool = False) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """È¢ÑÊµã"""
        if not self.is_trained:
            raise ValueError("Ê®°ÂûãÂ∞öÊú™ËÆ≠ÁªÉ")
        
        X_scaled = self.scaler_X.transform(X)
        
        if self.model_type == 'gaussian_process':
            if return_std:
                y_pred_scaled, std = self.model.predict(X_scaled, return_std=True)
                y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
                return y_pred, std
            else:
                y_pred_scaled = self.model.predict(X_scaled)
                y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
                return y_pred
        
        elif self.model_type in ['random_forest', 'gradient_boosting', 'neural_network']:
            y_pred_scaled = self.model.predict(X_scaled)
            y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
            
            if return_std:
                # ÈöèÊú∫Ê£ÆÊûóÔºöÈÄöËøáÊ†ëÁöÑÈ¢ÑÊµãÂ∑ÆÂºÇ‰º∞ËÆ°Ê†áÂáÜÂ∑Æ
                if self.model_type == 'random_forest':
                    tree_preds = np.array([tree.predict(X_scaled) for tree in self.model.estimators_])
                    std = np.std(tree_preds, axis=0)  # Ê†ëÈó¥È¢ÑÊµãÊ†áÂáÜÂ∑Æ
                    std = self.scaler_y.inverse_transform(std.reshape(-1, 1)).flatten()  # ÂèçÂΩí‰∏ÄÂåñ
                else:
                    std = np.zeros_like(y_pred)  # ÂÖ∂‰ªñÊ®°ÂûãÊöÇ‰∏çÊîØÊåÅ
                return y_pred, std
            return y_pred
        
        return y_pred


class MultiScaleMLBridge:
    """
    Â§öÂ∞∫Â∫¶MLÊ°•Êé•Âô® - Áî®‰∫éË∑®Â∞∫Â∫¶Ê®°Êãü
    
    Ê†∏ÂøÉÊÄùÊÉ≥ÔºöÂú®Ë∑®Â∞∫Â∫¶Ê®°Êãü‰∏≠ÔºàÂ¶Ç‰ªéÊùøÂùóËøêÂä®Âà∞Â≤©Áü≥ÂèòÂΩ¢ÔºâÔºå
    Áî®MLÊ®°ÂûãÊõø‰ª£Â∞èÂ∞∫Â∫¶Á≤æÁªÜÊ®°ÊãüÔºåÂ∞ÜÂ∞èÂ∞∫Â∫¶ÁªìÊûú"ÊâìÂåÖ"‰∏∫Â§ßÂ∞∫Â∫¶Ê®°ÂûãÁöÑÂèÇÊï∞
    """
    
    def __init__(self, fine_scale_model: Callable = None, coarse_scale_model: Callable = None):
        self.fine_scale_model = fine_scale_model
        self.coarse_scale_model = coarse_scale_model
        self.bridge_model = None
        self.is_trained = False
        self.scale_ratio = 1.0
        self.bridge_type = 'neural_network'
    
    def setup_bridge_model(self, input_dim: int, output_dim: int, model_type: str = 'neural_network'):
        """ËÆæÁΩÆÊ°•Êé•Ê®°Âûã"""
        self.bridge_type = model_type
        
        if model_type == 'neural_network' and HAS_PYTORCH:
            self.bridge_model = PhysicsInformedNeuralNetwork(
                input_dim, [128, 64, 32], output_dim
            )
        elif model_type == 'surrogate':
            self.bridge_model = SurrogateModelAdvanced('gaussian_process')
        else:
            raise ValueError(f"‰∏çÊîØÊåÅÁöÑÊ°•Êé•Ê®°ÂûãÁ±ªÂûã: {model_type}")
    
    def train_bridge(self, fine_data: np.ndarray, coarse_data: np.ndarray, **kwargs) -> dict:
        """ËÆ≠ÁªÉÊ°•Êé•Ê®°Âûã"""
        if self.bridge_model is None:
            raise ValueError("Ê°•Êé•Ê®°ÂûãÂ∞öÊú™ËÆæÁΩÆ")
        
        if isinstance(self.bridge_model, PhysicsInformedNeuralNetwork):
            self.bridge_model.setup_training()
            result = self.bridge_model.train(fine_data, coarse_data, **kwargs)
        else:
            result = self.bridge_model.train(fine_data, coarse_data, **kwargs)
        
        self.is_trained = True
        return result
    
    def predict_coarse_from_fine(self, fine_data: np.ndarray) -> np.ndarray:
        """‰ªéÁªÜÂ∞∫Â∫¶Êï∞ÊçÆÈ¢ÑÊµãÁ≤óÂ∞∫Â∫¶Êï∞ÊçÆ"""
        if not self.is_trained:
            raise ValueError("Ê°•Êé•Ê®°ÂûãÂ∞öÊú™ËÆ≠ÁªÉ")
        
        return self.bridge_model.predict(fine_data)
    
    def predict_fine_from_coarse(self, coarse_data: np.ndarray) -> np.ndarray:
        """‰ªéÁ≤óÂ∞∫Â∫¶Êï∞ÊçÆÈ¢ÑÊµãÁªÜÂ∞∫Â∫¶Êï∞ÊçÆ"""
        if not self.is_trained:
            raise ValueError("Ê°•Êé•Ê®°ÂûãÂ∞öÊú™ËÆ≠ÁªÉ")
        
        # ËøôÈáåÈúÄË¶ÅÂÆûÁé∞ÂèçÂêëÊò†Â∞Ñ
        # ÁÆÄÂåñÂÆûÁé∞ÔºöËøîÂõûÁ≤óÂ∞∫Â∫¶Êï∞ÊçÆÁöÑÊèíÂÄº
        return coarse_data
    
    def set_scale_ratio(self, ratio: float):
        """ËÆæÁΩÆÂ∞∫Â∫¶ÊØî‰æã"""
        self.scale_ratio = ratio


class HybridMLAccelerator:
    """
    Ê∑∑ÂêàMLÂä†ÈÄüÂô® - ÁªìÂêà‰º†ÁªüÊ±ÇËß£Âô®ÂíåML
    
    Ê†∏ÂøÉÊÄùÊÉ≥ÔºöÊó†Ê≥ïÂÆåÂÖ®Êõø‰ª£‰º†ÁªüÊ±ÇËß£Âô®ÔºàÈúÄÈ´òÁ≤æÂ∫¶ÔºâÔºå
    ‰ΩÜÂèØÂä†ÈÄüÂÖ∂‰∏≠ËÄóÊó∂Ê≠•È™§ÔºàÂ¶ÇËø≠‰ª£Ê±ÇËß£„ÄÅÁΩëÊ†ºËá™ÈÄÇÂ∫îÔºâ
    """
    
    def __init__(self, traditional_solver: Callable = None):
        self.traditional_solver = traditional_solver
        self.ml_models = {}
        self.performance_stats = {
            'traditional_time': 0.0,
            'ml_time': 0.0,
            'speedup': 0.0,
            'accuracy_loss': 0.0,
            'total_calls': 0
        }
        self.acceleration_strategies = {
            'initial_guess': None,
            'preconditioner': None,
            'adaptive_mesh': None,
            'multiscale': None
        }
    
    def add_ml_model(self, name: str, model: Union[PhysicsInformedNeuralNetwork, SurrogateModelAdvanced]):
        """Ê∑ªÂä†MLÊ®°Âûã"""
        self.ml_models[name] = model
        print(f"‚úÖ Ê∑ªÂä†MLÊ®°Âûã: {name}")
    
    def setup_acceleration_strategy(self, strategy: str, model_name: str):
        """ËÆæÁΩÆÂä†ÈÄüÁ≠ñÁï•"""
        if model_name in self.ml_models:
            self.acceleration_strategies[strategy] = model_name
            print(f"‚úÖ ËÆæÁΩÆÂä†ÈÄüÁ≠ñÁï•: {strategy} -> {model_name}")
        else:
            raise ValueError(f"MLÊ®°Âûã {model_name} ‰∏çÂ≠òÂú®")
    
    def solve_hybrid(self, problem_data: Dict, use_ml: bool = True, 
                    ml_model_name: str = None) -> Dict:
        """Ê∑∑ÂêàÊ±ÇËß£"""
        start_time = time.time()
        self.performance_stats['total_calls'] += 1
        
        if use_ml and ml_model_name and ml_model_name in self.ml_models:
            # ‰ΩøÁî®MLÊ®°Âûã
            ml_model = self.ml_models[ml_model_name]
            result = ml_model.predict(problem_data['input'])
            ml_time = time.time() - start_time
            
            self.performance_stats['ml_time'] += ml_time
            
            return {
                'solution': result,
                'method': 'ml',
                'time': ml_time,
                'model_name': ml_model_name
            }
        
        else:
            # ‰ΩøÁî®‰º†ÁªüÊ±ÇËß£Âô®
            if self.traditional_solver is None:
                raise ValueError("‰º†ÁªüÊ±ÇËß£Âô®Êú™ËÆæÁΩÆ")
            
            # Ê£ÄÊü•ÊòØÂê¶ÊúâMLÂä†ÈÄüÁ≠ñÁï•
            if self.acceleration_strategies['initial_guess']:
                # ‰ΩøÁî®MLÈ¢ÑÊµãÂàùÂßãÁåúÊµã
                initial_guess = self.ml_models[self.acceleration_strategies['initial_guess']].predict(
                    problem_data['input']
                )
                problem_data['initial_guess'] = initial_guess
            
            result = self.traditional_solver(problem_data)
            traditional_time = time.time() - start_time
            
            self.performance_stats['traditional_time'] += traditional_time
            
            return {
                'solution': result,
                'method': 'traditional',
                'time': traditional_time
            }
    
    def compare_methods(self, problem_data: Dict, ml_model_name: str = None) -> Dict:
        """ÊØîËæÉ‰º†ÁªüÊñπÊ≥ïÂíåMLÊñπÊ≥ï"""
        # ‰º†ÁªüÊñπÊ≥ï
        traditional_result = self.solve_hybrid(problem_data, use_ml=False)
        
        # MLÊñπÊ≥ï
        if ml_model_name and ml_model_name in self.ml_models:
            ml_result = self.solve_hybrid(problem_data, use_ml=True, ml_model_name=ml_model_name)
            
            # ËÆ°ÁÆóÂä†ÈÄüÊØî
            speedup = traditional_result['time'] / ml_result['time']
            
            # ËÆ°ÁÆóÁ≤æÂ∫¶ÊçüÂ§±ÔºàÁÆÄÂåñÔºâ
            accuracy_loss = np.mean(np.abs(
                traditional_result['solution'] - ml_result['solution']
            ))
            
            self.performance_stats['speedup'] = speedup
            self.performance_stats['accuracy_loss'] = accuracy_loss
            
            return {
                'traditional': traditional_result,
                'ml': ml_result,
                'speedup': speedup,
                'accuracy_loss': accuracy_loss
            }
        
        return {'traditional': traditional_result}
    
    def get_performance_stats(self) -> dict:
        """Ëé∑ÂèñÊÄßËÉΩÁªüËÆ°"""
        stats = self.performance_stats.copy()
        if stats['total_calls'] > 0:
            stats['avg_traditional_time'] = stats['traditional_time'] / stats['total_calls']
            stats['avg_ml_time'] = stats['ml_time'] / stats['total_calls']
        return stats


class AdaptiveMLSolver:
    """
    Ëá™ÈÄÇÂ∫îMLÊ±ÇËß£Âô® - Ê†πÊçÆÈóÆÈ¢òÁâπÂæÅËá™Âä®ÈÄâÊã©ÊúÄ‰Ω≥ÊñπÊ≥ï
    
    Ê†∏ÂøÉÊÄùÊÉ≥ÔºöÊ†πÊçÆÈóÆÈ¢òËßÑÊ®°„ÄÅÁ≤æÂ∫¶Ë¶ÅÊ±Ç„ÄÅËÆ°ÁÆóËµÑÊ∫êÁ≠âÁâπÂæÅÔºå
    Ëá™Âä®ÈÄâÊã©ÊúÄÈÄÇÂêàÁöÑÊ±ÇËß£ÊñπÊ≥ïÔºà‰º†ÁªüÊï∞ÂÄºÊñπÊ≥ï vs MLÂä†ÈÄüÊñπÊ≥ïÔºâ
    """
    
    def __init__(self):
        self.solvers = {}
        self.performance_history = []
        self.adaptive_rules = {}
        self.selection_strategy = 'performance_based'
        
        # ÂèØÈÖçÁΩÆËØÑÂàÜÊùÉÈáç
        self.score_weights = {
            'problem_feature': 1.0,
            'accuracy': 0.5,
            'speed': 0.5,
            'priority': 0.1
        }
    
    def add_solver(self, name: str, solver: Callable, 
                  conditions: Dict = None, priority: int = 1):
        """Ê∑ªÂä†Ê±ÇËß£Âô®"""
        self.solvers[name] = {
            'solver': solver,
            'conditions': conditions or {},
            'priority': priority,
            'performance': {'accuracy': 0.0, 'speed': 0.0, 'usage_count': 0}
        }
        print(f"‚úÖ Ê∑ªÂä†Ê±ÇËß£Âô®: {name}")
    
    def set_selection_strategy(self, strategy: str):
        """ËÆæÁΩÆÈÄâÊã©Á≠ñÁï•"""
        valid_strategies = ['performance_based', 'rule_based', 'hybrid']
        if strategy in valid_strategies:
            self.selection_strategy = strategy
            print(f"‚úÖ ËÆæÁΩÆÈÄâÊã©Á≠ñÁï•: {strategy}")
        else:
            raise ValueError(f"‰∏çÊîØÊåÅÁöÑÈÄâÊã©Á≠ñÁï•: {strategy}")
    
    def set_score_weights(self, weights: Dict[str, float]):
        """ËÆæÁΩÆËØÑÂàÜÊùÉÈáç"""
        for key, value in weights.items():
            if key in self.score_weights:
                self.score_weights[key] = value
        print(f"‚úÖ ËØÑÂàÜÊùÉÈáçÂ∑≤Êõ¥Êñ∞: {self.score_weights}")
    
    def select_best_solver(self, problem_data: Dict) -> str:
        """ÈÄâÊã©ÊúÄ‰Ω≥Ê±ÇËß£Âô®"""
        if self.selection_strategy == 'performance_based':
            return self._select_by_performance(problem_data)
        elif self.selection_strategy == 'rule_based':
            return self._select_by_rules(problem_data)
        elif self.selection_strategy == 'hybrid':
            return self._select_hybrid(problem_data)
        
        return None
    
    def _select_by_performance(self, problem_data: Dict) -> str:
        """Âü∫‰∫éÊÄßËÉΩÈÄâÊã©Ê±ÇËß£Âô®"""
        best_solver = None
        best_score = -1
        
        for name, solver_info in self.solvers.items():
            score = self._evaluate_solver_performance(name, solver_info, problem_data)
            if score > best_score:
                best_score = score
                best_solver = name
        
        return best_solver
    
    def _select_by_rules(self, problem_data: Dict) -> str:
        """Âü∫‰∫éËßÑÂàôÈÄâÊã©Ê±ÇËß£Âô®"""
        for name, solver_info in self.solvers.items():
            if self._check_conditions(solver_info['conditions'], problem_data):
                return name
        return None
    
    def _select_hybrid(self, problem_data: Dict) -> str:
        """Ê∑∑ÂêàÈÄâÊã©Á≠ñÁï•"""
        # ÂÖàÊ£ÄÊü•ËßÑÂàô
        rule_based = self._select_by_rules(problem_data)
        if rule_based:
            return rule_based
        
        # ÂÜçÂü∫‰∫éÊÄßËÉΩÈÄâÊã©
        return self._select_by_performance(problem_data)
    
    def _evaluate_solver_performance(self, name: str, solver_info: Dict, problem_data: Dict) -> float:
        """ËØÑ‰º∞Ê±ÇËß£Âô®ÊÄßËÉΩ"""
        score = 0.0
        weights = self.score_weights
        
        # Êâ©Â±ïÈóÆÈ¢òÁâπÂæÅËØÑ‰º∞ÔºàÊîØÊåÅÁ≤æÂ∫¶Ë¶ÅÊ±ÇÔºâ
        if 'size' in problem_data:
            if problem_data['size'] < 1000 and solver_info['conditions'].get('small_problems', False):
                score += weights['problem_feature']
            elif problem_data['size'] >= 1000 and solver_info['conditions'].get('large_problems', False):
                score += weights['problem_feature']
        
        # ÊîØÊåÅÁ≤æÂ∫¶Ë¶ÅÊ±ÇÂåπÈÖçÔºàÊñ∞Â¢ûÔºâ
        if 'accuracy_requirement' in problem_data:
            req = problem_data['accuracy_requirement']
            if solver_info['performance']['accuracy'] >= req:
                score += weights['problem_feature'] * 0.5  # È¢ùÂ§ñÂä†ÂàÜ
        
        # ÊîØÊåÅËÆ°ÁÆóËµÑÊ∫êÈôêÂà∂ÔºàÊñ∞Â¢ûÔºâ
        if 'compute_resource' in problem_data:
            resource = problem_data['compute_resource']
            if solver_info['conditions'].get('resource_requirement', 'low') == resource:
                score += weights['problem_feature'] * 0.3
        
        # Âä®ÊÄÅÊùÉÈáçËÆ°ÁÆó
        performance = solver_info['performance']
        score += performance['accuracy'] * weights['accuracy'] + performance['speed'] * weights['speed']
        score += solver_info['priority'] * weights['priority']
        
        return score
    
    def _check_conditions(self, conditions: Dict, problem_data: Dict) -> bool:
        """Ê£ÄÊü•Êù°‰ª∂ÊòØÂê¶Êª°Ë∂≥ - ÊîØÊåÅËåÉÂõ¥Êù°‰ª∂"""
        for key, cond in conditions.items():
            if key not in problem_data:
                return False
            val = problem_data[key]
            
            # ÊîØÊåÅËåÉÂõ¥Êù°‰ª∂ÔºàÂ¶Ç ('>', 1000)„ÄÅ('<=', 0.95)Ôºâ
            if isinstance(cond, tuple) and len(cond) == 2:
                op, threshold = cond
                if op == '>':
                    if not (val > threshold):
                        return False
                elif op == '<':
                    if not (val < threshold):
                        return False
                elif op == '>=':
                    if not (val >= threshold):
                        return False
                elif op == '<=':
                    if not (val <= threshold):
                        return False
                else:
                    return False
            # ÂéüÈÄªËæëÔºöÊîØÊåÅÁ≠âÂÄºÊàñÂàóË°®ÂåÖÂê´
            elif isinstance(cond, (list, tuple)):
                if val not in cond:
                    return False
            else:
                if val != cond:
                    return False
        return True
    
    def solve(self, problem_data: Dict) -> Dict:
        """Ëá™ÈÄÇÂ∫îÊ±ÇËß£"""
        # ÈÄâÊã©ÊúÄ‰Ω≥Ê±ÇËß£Âô®
        best_solver_name = self.select_best_solver(problem_data)
        
        if best_solver_name is None:
            raise ValueError("Ê≤°ÊúâÂèØÁî®ÁöÑÊ±ÇËß£Âô®")
        
        # ÊâßË°åÊ±ÇËß£
        solver_info = self.solvers[best_solver_name]
        solver = solver_info['solver']
        
        start_time = time.time()
        result = solver(problem_data)
        solve_time = time.time() - start_time
        
        # Êõ¥Êñ∞ÊÄßËÉΩÁªüËÆ°
        solver_info['performance']['usage_count'] += 1
        solver_info['performance']['speed'] = 1.0 / solve_time
        
        # Êñ∞Â¢ûÔºöËã•Êèê‰æõÂèÇËÄÉËß£ÔºåËÆ°ÁÆóÁ≤æÂ∫¶
        if 'reference_solution' in problem_data:
            mse = np.mean((result - problem_data['reference_solution'])**2)
            accuracy = 1.0 / (1.0 + mse)  # ÂΩí‰∏ÄÂåñÂà∞ [0,1]
            solver_info['performance']['accuracy'] = accuracy
        
        # ËÆ∞ÂΩïÂéÜÂè≤
        self.performance_history.append({
            'solver': best_solver_name,
            'time': solve_time,
            'problem_size': problem_data.get('size', 0),
            'timestamp': time.time()
        })
        
        return {
            'solution': result,
            'solver_used': best_solver_name,
            'time': solve_time
        }
    
    def get_performance_summary(self) -> Dict:
        """Ëé∑ÂèñÊÄßËÉΩÊÄªÁªì"""
        summary = {}
        
        for name, solver_info in self.solvers.items():
            performance = solver_info['performance']
            summary[name] = {
                'usage_count': performance['usage_count'],
                'avg_speed': performance['speed'],
                'avg_accuracy': performance['accuracy']
            }
        
        return summary


class RLAgent(nn.Module):
    """
    Âº∫ÂåñÂ≠¶‰π†Êô∫ËÉΩ‰Ωì - Áî®‰∫é‰ºòÂåñÊï∞ÂÄºÊ±ÇËß£Á≠ñÁï•
    
    Ê†∏ÂøÉÊÄùÊÉ≥ÔºöÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Ëá™Âä®ÈÄâÊã©ÊúÄ‰ºòÁöÑÊ±ÇËß£ÂèÇÊï∞ÔºàÊó∂Èó¥Ê≠•Èïø„ÄÅÁΩëÊ†ºÂä†ÂØÜÊñπÊ°àÁ≠âÔºâÔºå
    ÂáèÂ∞ë‰∫∫Â∑•Ë∞ÉÂèÇÊàêÊú¨ÔºåÊèêÂçáÊ±ÇËß£ÊïàÁéá
    """
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [128, 64]):
        super().__init__()
        
        if not HAS_PYTORCH:
            raise ImportError("ÈúÄË¶ÅÂÆâË£ÖPyTorchÊù•‰ΩøÁî®RLÊô∫ËÉΩ‰Ωì")
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ActorÁΩëÁªúÔºàÁ≠ñÁï•ÁΩëÁªúÔºâ
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dims[0]),
            nn.ReLU(),
            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.ReLU(),
            nn.Linear(hidden_dims[1], action_dim),
            nn.Tanh()  # ËæìÂá∫ËåÉÂõ¥[-1, 1]
        )
        
        # CriticÁΩëÁªúÔºà‰ª∑ÂÄºÁΩëÁªúÔºâ
        self.critic = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dims[0]),
            nn.ReLU(),
            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.ReLU(),
            nn.Linear(hidden_dims[1], 1)
        )
        
        self.to(self.device)
        
        print(f"üîÑ RLÊô∫ËÉΩ‰ΩìÂàùÂßãÂåñÂÆåÊàê - ËÆæÂ§á: {self.device}")
        print(f"   Áä∂ÊÄÅÁª¥Â∫¶: {state_dim}, Âä®‰ΩúÁª¥Â∫¶: {action_dim}")
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """ÂâçÂêë‰º†Êí≠ - ËøîÂõûÂä®‰Ωú"""
        return self.actor(state)
    
    def get_action(self, state: np.ndarray, noise_scale: float = 0.1) -> np.ndarray:
        """Ëé∑ÂèñÂä®‰ΩúÔºàÂ∏¶Êé¢Á¥¢Âô™Â£∞Ôºâ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action = self.forward(state_tensor).squeeze(0)
            # Ê∑ªÂä†Êé¢Á¥¢Âô™Â£∞
            noise = torch.randn_like(action) * noise_scale
            action = action + noise
            # Ë£ÅÂâ™Âà∞ÊúâÊïàËåÉÂõ¥
            action = torch.clamp(action, -1.0, 1.0)
        
        return action.cpu().numpy()
    
    def get_value(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """Ëé∑ÂèñÁä∂ÊÄÅ-Âä®‰Ωú‰ª∑ÂÄº"""
        return self.critic(torch.cat([state, action], dim=1))


class SolverEnvironment:
    """
    Ê±ÇËß£Âô®ÁéØÂ¢É - Ê®°ÊãüÊï∞ÂÄºÊ±ÇËß£ËøáÁ®ãÔºå‰∏∫RLÊèê‰æõËÆ≠ÁªÉÁéØÂ¢É
    
    Ê†∏ÂøÉÊÄùÊÉ≥ÔºöÂ∞ÜÊï∞ÂÄºÊ±ÇËß£ËøáÁ®ãÂª∫Ê®°‰∏∫Âº∫ÂåñÂ≠¶‰π†ÁéØÂ¢ÉÔºå
    Êô∫ËÉΩ‰ΩìÈÄöËøá‰∏éÁéØÂ¢É‰∫§‰∫íÂ≠¶‰π†ÊúÄ‰ºòÁöÑÊ±ÇËß£Á≠ñÁï•
    """
    
    def __init__(self, solver_config: Dict, physics_config: PhysicsConfig = None):
        self.solver_config = solver_config
        self.physics_config = physics_config or PhysicsConfig()
        self.max_steps = solver_config.get('max_steps', 100)
        self.current_step = 0
        self.convergence_history = []
        self.performance_metrics = {}
        
        # Ê±ÇËß£Á≠ñÁï•ÂèÇÊï∞ËåÉÂõ¥
        self.action_bounds = {
            'time_step': (0.001, 0.1),      # Êó∂Èó¥Ê≠•Èïø
            'mesh_refinement': (0.1, 2.0),  # ÁΩëÊ†ºÂä†ÂØÜÂõ†Â≠ê
            'tolerance': (1e-6, 1e-3),      # Êî∂ÊïõÂÆπÂ∑Æ
            'max_iterations': (50, 500)      # ÊúÄÂ§ßËø≠‰ª£Ê¨°Êï∞
        }
        
        print(f"üîÑ Ê±ÇËß£Âô®ÁéØÂ¢ÉÂàùÂßãÂåñÂÆåÊàê")
        print(f"   ÊúÄÂ§ßÊ≠•Êï∞: {self.max_steps}")
        print(f"   Âä®‰ΩúÂèÇÊï∞: {list(self.action_bounds.keys())}")
    
    def reset(self) -> np.ndarray:
        """ÈáçÁΩÆÁéØÂ¢É"""
        self.current_step = 0
        self.convergence_history = []
        self.performance_metrics = {}
        
        # ËøîÂõûÂàùÂßãÁä∂ÊÄÅ
        initial_state = self._get_state()
        return initial_state
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """ÊâßË°å‰∏ÄÊ≠•Âä®‰Ωú"""
        if self.current_step >= self.max_steps:
            return self._get_state(), 0.0, True, {}
        
        # Ëß£ÊûêÂä®‰ΩúÔºà‰ªé[-1,1]Êò†Â∞ÑÂà∞ÂÆûÈôÖÂèÇÊï∞ËåÉÂõ¥Ôºâ
        solver_params = self._action_to_params(action)
        
        # Ê®°ÊãüÊ±ÇËß£ËøáÁ®ã
        reward, metrics = self._simulate_solving(solver_params)
        
        # Êõ¥Êñ∞Áä∂ÊÄÅ
        self.current_step += 1
        self.convergence_history.append(metrics.get('convergence', 0.0))
        self.performance_metrics.update(metrics)
        
        # Ê£ÄÊü•ÊòØÂê¶ÂÆåÊàê
        done = self.current_step >= self.max_steps or metrics.get('converged', False)
        
        return self._get_state(), reward, done, metrics
    
    def _action_to_params(self, action: np.ndarray) -> Dict:
        """Â∞ÜÂä®‰ΩúËΩ¨Êç¢‰∏∫Ê±ÇËß£Âô®ÂèÇÊï∞"""
        params = {}
        action_names = list(self.action_bounds.keys())
        
        for i, name in enumerate(action_names):
            if i < len(action):
                # Â∞Ü[-1,1]Êò†Â∞ÑÂà∞ÂÆûÈôÖËåÉÂõ¥
                action_val = action[i]
                min_val, max_val = self.action_bounds[name]
                param_val = min_val + (action_val + 1) * (max_val - min_val) / 2
                params[name] = param_val
        
        return params
    
    def _simulate_solving(self, solver_params: Dict) -> Tuple[float, Dict]:
        """Ê®°ÊãüÊ±ÇËß£ËøáÁ®ãÔºåËÆ°ÁÆóÂ•ñÂä±ÂíåÊåáÊ†á"""
        # Ê®°ÊãüÊ±ÇËß£Êó∂Èó¥ÔºàÂü∫‰∫éÂèÇÊï∞Ôºâ
        time_step = solver_params.get('time_step', 0.01)
        mesh_refinement = solver_params.get('mesh_refinement', 1.0)
        tolerance = solver_params.get('tolerance', 1e-4)
        max_iterations = solver_params.get('max_iterations', 100)
        
        # Ê®°ÊãüÊî∂ÊïõËøáÁ®ã
        convergence_rate = 1.0 / (1.0 + tolerance * 1000)  # ÂÆπÂ∑ÆË∂äÂ∞èÔºåÊî∂ÊïõË∂äÂø´
        mesh_efficiency = 1.0 / (1.0 + abs(mesh_refinement - 1.0))  # ÁΩëÊ†ºÂõ†Â≠êÊé•Ëøë1Êó∂ÊïàÁéáÊúÄÈ´ò
        
        # Ê®°ÊãüËø≠‰ª£Ê¨°Êï∞
        actual_iterations = min(max_iterations, int(50 / convergence_rate))
        
        # ËÆ°ÁÆóÂ•ñÂä±ÔºàÁªºÂêàËÄÉËôëÊïàÁéá„ÄÅÁ≤æÂ∫¶„ÄÅÁ®≥ÂÆöÊÄßÔºâ
        efficiency_reward = 1.0 / (1.0 + time_step * 100)  # Êó∂Èó¥Ê≠•ÈïøË∂äÂ∞èË∂äÂ•Ω
        accuracy_reward = 1.0 / (1.0 + tolerance * 1e6)    # ÂÆπÂ∑ÆË∂äÂ∞èË∂äÂ•Ω
        stability_reward = 1.0 / (1.0 + abs(mesh_refinement - 1.0))  # ÁΩëÊ†ºÁ®≥ÂÆöÊÄß
        
        # Êî∂ÊïõÂ•ñÂä±
        converged = actual_iterations < max_iterations
        convergence_reward = 10.0 if converged else 0.0
        
        # ÊÄªÂ•ñÂä±
        total_reward = (efficiency_reward + accuracy_reward + stability_reward + convergence_reward) / 4
        
        # ÊÄßËÉΩÊåáÊ†á
        metrics = {
            'convergence': convergence_rate,
            'mesh_efficiency': mesh_efficiency,
            'iterations': actual_iterations,
            'converged': converged,
            'time_step': time_step,
            'mesh_refinement': mesh_refinement,
            'tolerance': tolerance
        }
        
        return total_reward, metrics
    
    def _get_state(self) -> np.ndarray:
        """Ëé∑ÂèñÂΩìÂâçÁä∂ÊÄÅ"""
        state = []
        
        # ÂΩìÂâçÊ≠•Êï∞ÔºàÂΩí‰∏ÄÂåñÔºâ
        state.append(self.current_step / self.max_steps)
        
        # Êî∂ÊïõÂéÜÂè≤ÁªüËÆ°
        if self.convergence_history:
            state.extend([
                np.mean(self.convergence_history),
                np.std(self.convergence_history),
                self.convergence_history[-1] if self.convergence_history else 0.0
            ])
        else:
            state.extend([0.0, 0.0, 0.0])
        
        # ÊÄßËÉΩÊåáÊ†á
        for key in ['mesh_efficiency', 'iterations']:
            if key in self.performance_metrics:
                # ÂΩí‰∏ÄÂåñÂà∞[0,1]
                if key == 'iterations':
                    val = self.performance_metrics[key] / 500.0  # ÂÅáËÆæÊúÄÂ§ß500Ê¨°Ëø≠‰ª£
                else:
                    val = self.performance_metrics[key]
                state.append(val)
            else:
                state.append(0.0)
        
        return np.array(state, dtype=np.float32)


class RLSolverOptimizer(BaseSolver):
    """
    Âº∫ÂåñÂ≠¶‰π†Ê±ÇËß£Âô®‰ºòÂåñÂô® - ‰ΩøÁî®RLËá™Âä®‰ºòÂåñÊï∞ÂÄºÊ±ÇËß£Á≠ñÁï•
    
    Ê†∏ÂøÉÊÄùÊÉ≥ÔºöÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÊô∫ËÉΩ‰ΩìÔºåËá™Âä®ÈÄâÊã©ÊúÄ‰ºòÁöÑÊ±ÇËß£ÂèÇÊï∞Ôºå
    ÂÆûÁé∞"Ëá™Â≠¶‰π†"ÁöÑÊï∞ÂÄºÊ±ÇËß£‰ºòÂåñ
    """
    
    def __init__(self, state_dim: int, action_dim: int, solver_config: Dict = None):
        super().__init__()
        
        if not HAS_PYTORCH:
            raise ImportError("ÈúÄË¶ÅÂÆâË£ÖPyTorchÊù•‰ΩøÁî®RLÊ±ÇËß£Âô®‰ºòÂåñÂô®")
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.solver_config = solver_config or {}
        
        # ÂàõÂª∫RLÊô∫ËÉΩ‰ΩìÂíåÁéØÂ¢É
        self.agent = RLAgent(state_dim, action_dim)
        self.environment = SolverEnvironment(solver_config)
        
        # ËÆ≠ÁªÉÂèÇÊï∞
        self.learning_rate = 0.001
        self.gamma = 0.99  # ÊäòÊâ£Âõ†Â≠ê
        self.tau = 0.005   # ËΩØÊõ¥Êñ∞ÂèÇÊï∞
        
        # ÁªèÈ™åÂõûÊîæÁºìÂÜ≤Âå∫
        self.replay_buffer = []
        self.buffer_size = 10000
        self.batch_size = 64
        
        # ÁõÆÊ†áÁΩëÁªúÔºàÁî®‰∫éÁ®≥ÂÆöËÆ≠ÁªÉÔºâ
        self.target_agent = RLAgent(state_dim, action_dim)
        self._update_target_network()
        
        self.optimizer_actor = optim.Adam(self.agent.actor.parameters(), lr=self.learning_rate)
        self.optimizer_critic = optim.Adam(self.agent.critic.parameters(), lr=self.learning_rate)
        
        print(f"üîÑ RLÊ±ÇËß£Âô®‰ºòÂåñÂô®ÂàùÂßãÂåñÂÆåÊàê")
        print(f"   Áä∂ÊÄÅÁª¥Â∫¶: {state_dim}, Âä®‰ΩúÁª¥Â∫¶: {action_dim}")
    
    def _update_target_network(self):
        """ËΩØÊõ¥Êñ∞ÁõÆÊ†áÁΩëÁªú"""
        for target_param, param in zip(self.target_agent.parameters(), self.agent.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
    
    def _store_experience(self, state: np.ndarray, action: np.ndarray, 
                         reward: float, next_state: np.ndarray, done: bool):
        """Â≠òÂÇ®ÁªèÈ™åÂà∞ÂõûÊîæÁºìÂÜ≤Âå∫"""
        experience = (state, action, reward, next_state, done)
        self.replay_buffer.append(experience)
        
        # ÈôêÂà∂ÁºìÂÜ≤Âå∫Â§ßÂ∞è
        if len(self.replay_buffer) > self.buffer_size:
            self.replay_buffer.pop(0)
    
    def _sample_batch(self) -> List[Tuple]:
        """‰ªéÂõûÊîæÁºìÂÜ≤Âå∫ÈááÊ†∑ÊâπÊ¨°"""
        if len(self.replay_buffer) < self.batch_size:
            return []
        
        indices = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)
        return [self.replay_buffer[i] for i in indices]
    
    def _update_networks(self, batch: List[Tuple]):
        """Êõ¥Êñ∞ÁΩëÁªúÂèÇÊï∞"""
        if not batch:
            return
        
        states = torch.FloatTensor(np.array([exp[0] for exp in batch])).to(self.device)
        actions = torch.FloatTensor(np.array([exp[1] for exp in batch])).to(self.device)
        rewards = torch.FloatTensor(np.array([exp[2] for exp in batch])).to(self.device)
        next_states = torch.FloatTensor(np.array([exp[3] for exp in batch])).to(self.device)
        dones = torch.BoolTensor(np.array([exp[4] for exp in batch])).to(self.device)
        
        # Êõ¥Êñ∞CriticÁΩëÁªú
        current_q_values = self.agent.get_value(states, actions)
        next_actions = self.target_agent(next_states)
        next_q_values = self.target_agent.get_value(next_states, next_actions)
        target_q_values = rewards + (self.gamma * next_q_values * (~dones).float())
        
        critic_loss = F.mse_loss(current_q_values, target_q_values.detach())
        
        self.optimizer_critic.zero_grad()
        critic_loss.backward()
        self.optimizer_critic.step()
        
        # Êõ¥Êñ∞ActorÁΩëÁªú
        actor_loss = -self.agent.get_value(states, self.agent(states)).mean()
        
        self.optimizer_actor.zero_grad()
        actor_loss.backward()
        self.optimizer_actor.step()
        
        # ËΩØÊõ¥Êñ∞ÁõÆÊ†áÁΩëÁªú
        self._update_target_network()
        
        return {
            'critic_loss': critic_loss.item(),
            'actor_loss': actor_loss.item()
        }
    
    def train(self, episodes: int = 1000, **kwargs) -> dict:
        """ËÆ≠ÁªÉRLÊô∫ËÉΩ‰Ωì"""
        print(f"üîÑ ÂºÄÂßãËÆ≠ÁªÉRLÊ±ÇËß£Âô®‰ºòÂåñÂô®ÔºåÊÄªËΩÆÊï∞: {episodes}")
        
        episode_rewards = []
        episode_lengths = []
        training_losses = []
        
        for episode in range(episodes):
            state = self.environment.reset()
            episode_reward = 0.0
            episode_length = 0
            
            while True:
                # ÈÄâÊã©Âä®‰Ωú
                action = self.agent.get_action(state, noise_scale=max(0.01, 0.1 * (1 - episode / episodes)))
                
                # ÊâßË°åÂä®‰Ωú
                next_state, reward, done, info = self.environment.step(action)
                
                # Â≠òÂÇ®ÁªèÈ™å
                self._store_experience(state, action, reward, next_state, done)
                
                # Êõ¥Êñ∞ÁΩëÁªú
                batch = self._sample_batch()
                if batch:
                    loss_info = self._update_networks(batch)
                    training_losses.append(loss_info)
                
                state = next_state
                episode_reward += reward
                episode_length += 1
                
                if done:
                    break
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                avg_length = np.mean(episode_lengths[-100:])
                print(f"   ËΩÆÊï∞ {episode+1}/{episodes}: Âπ≥ÂùáÂ•ñÂä±={avg_reward:.4f}, Âπ≥ÂùáÈïøÂ∫¶={avg_length:.1f}")
        
        self.is_trained = True
        
        training_history = {
            'episodes': episodes,
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'training_losses': training_losses,
            'final_avg_reward': np.mean(episode_rewards[-100:]) if episode_rewards else 0.0
        }
        
        print(f"‚úÖ RLËÆ≠ÁªÉÂÆåÊàêÔºåÊúÄÁªàÂπ≥ÂùáÂ•ñÂä±: {training_history['final_avg_reward']:.4f}")
        return training_history
    
    def optimize_solver_strategy(self, problem_state: np.ndarray) -> Dict:
        """‰ºòÂåñÊ±ÇËß£Á≠ñÁï•"""
        if not self.is_trained:
            raise ValueError("RLÊô∫ËÉΩ‰ΩìÂ∞öÊú™ËÆ≠ÁªÉ")
        
        # ‰ΩøÁî®ËÆ≠ÁªÉÂ•ΩÁöÑÊô∫ËÉΩ‰ΩìÈÄâÊã©ÊúÄ‰ºòÂä®‰Ωú
        optimal_action = self.agent.get_action(problem_state, noise_scale=0.0)
        
        # ËΩ¨Êç¢‰∏∫Ê±ÇËß£Âô®ÂèÇÊï∞
        solver_params = self.environment._action_to_params(optimal_action)
        
        print(f"üîß RL‰ºòÂåñÊ±ÇËß£Á≠ñÁï•:")
        for param, value in solver_params.items():
            print(f"   {param}: {value:.6f}")
        
        return solver_params
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """È¢ÑÊµãÊúÄ‰ºòÊ±ÇËß£Á≠ñÁï•"""
        if not self.is_trained:
            raise ValueError("RLÊô∫ËÉΩ‰ΩìÂ∞öÊú™ËÆ≠ÁªÉ")
        
        strategies = []
        for state in X:
            action = self.agent.get_action(state, noise_scale=0.0)
            strategy = self.environment._action_to_params(action)
            strategies.append(list(strategy.values()))
        
        return np.array(strategies)
    
    def evaluate_strategy(self, strategy: Dict, problem_state: np.ndarray) -> Dict:
        """ËØÑ‰º∞Ê±ÇËß£Á≠ñÁï•ÁöÑÊÄßËÉΩ"""
        # Â∞ÜÁ≠ñÁï•ËΩ¨Êç¢‰∏∫Âä®‰Ωú
        action = np.array([strategy.get(param, 0.0) for param in self.environment.action_bounds.keys()])
        
        # Âú®ÁéØÂ¢É‰∏≠ÊµãËØïÁ≠ñÁï•
        state = problem_state
        total_reward = 0.0
        step_count = 0
        
        for _ in range(self.environment.max_steps):
            next_state, reward, done, info = self.environment.step(action)
            total_reward += reward
            step_count += 1
            
            if done:
                break
        
        return {
            'total_reward': total_reward,
            'step_count': step_count,
            'efficiency': total_reward / max(step_count, 1),
            'convergence': info.get('converged', False)
        }


def create_advanced_ml_system() -> Dict:
    """ÂàõÂª∫È´òÁ∫ßMLÁ≥ªÁªü"""
    system = {
        'pinn': PhysicsInformedNeuralNetwork,
        'surrogate': SurrogateModelAdvanced,
        'bridge': MultiScaleMLBridge,
        'hybrid': HybridMLAccelerator,
        'adaptive': AdaptiveMLSolver,
        'rl_agent': RLAgent,
        'rl_environment': SolverEnvironment,
        'rl_optimizer': RLSolverOptimizer
    }
    
    print("üîÑ È´òÁ∫ßMLÁ≥ªÁªüÂàõÂª∫ÂÆåÊàê")
    return system


class RLAgent(nn.Module):
    """
    Âº∫ÂåñÂ≠¶‰π†Êô∫ËÉΩ‰Ωì - Áî®‰∫é‰ºòÂåñÊï∞ÂÄºÊ±ÇËß£Á≠ñÁï•
    
    Ê†∏ÂøÉÊÄùÊÉ≥ÔºöÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Ëá™Âä®ÈÄâÊã©ÊúÄ‰ºòÁöÑÊ±ÇËß£ÂèÇÊï∞ÔºàÊó∂Èó¥Ê≠•Èïø„ÄÅÁΩëÊ†ºÂä†ÂØÜÊñπÊ°àÁ≠âÔºâÔºå
    ÂáèÂ∞ë‰∫∫Â∑•Ë∞ÉÂèÇÊàêÊú¨ÔºåÊèêÂçáÊ±ÇËß£ÊïàÁéá
    """
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [128, 64]):
        super().__init__()
        
        if not HAS_PYTORCH:
            raise ImportError("ÈúÄË¶ÅÂÆâË£ÖPyTorchÊù•‰ΩøÁî®RLÊô∫ËÉΩ‰Ωì")
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ActorÁΩëÁªúÔºàÁ≠ñÁï•ÁΩëÁªúÔºâ
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dims[0]),
            nn.ReLU(),
            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.ReLU(),
            nn.Linear(hidden_dims[1], action_dim),
            nn.Tanh()  # ËæìÂá∫ËåÉÂõ¥[-1, 1]
        )
        
        # CriticÁΩëÁªúÔºà‰ª∑ÂÄºÁΩëÁªúÔºâ
        self.critic = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dims[0]),
            nn.ReLU(),
            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.ReLU(),
            nn.Linear(hidden_dims[1], 1)
        )
        
        self.to(self.device)
        
        print(f"üîÑ RLÊô∫ËÉΩ‰ΩìÂàùÂßãÂåñÂÆåÊàê - ËÆæÂ§á: {self.device}")
        print(f"   Áä∂ÊÄÅÁª¥Â∫¶: {state_dim}, Âä®‰ΩúÁª¥Â∫¶: {action_dim}")
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """ÂâçÂêë‰º†Êí≠ - ËøîÂõûÂä®‰Ωú"""
        return self.actor(state)
    
    def get_action(self, state: np.ndarray, noise_scale: float = 0.1) -> np.ndarray:
        """Ëé∑ÂèñÂä®‰ΩúÔºàÂ∏¶Êé¢Á¥¢Âô™Â£∞Ôºâ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action = self.forward(state_tensor).squeeze(0)
            # Ê∑ªÂä†Êé¢Á¥¢Âô™Â£∞
            noise = torch.randn_like(action) * noise_scale
            action = action + noise
            # Ë£ÅÂâ™Âà∞ÊúâÊïàËåÉÂõ¥
            action = torch.clamp(action, -1.0, 1.0)
        
        return action.cpu().numpy()
    
    def get_value(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """Ëé∑ÂèñÁä∂ÊÄÅ-Âä®‰Ωú‰ª∑ÂÄº"""
        return self.critic(torch.cat([state, action], dim=1))


class SolverEnvironment:
    """
    Ê±ÇËß£Âô®ÁéØÂ¢É - Ê®°ÊãüÊï∞ÂÄºÊ±ÇËß£ËøáÁ®ãÔºå‰∏∫RLÊèê‰æõËÆ≠ÁªÉÁéØÂ¢É
    
    Ê†∏ÂøÉÊÄùÊÉ≥ÔºöÂ∞ÜÊï∞ÂÄºÊ±ÇËß£ËøáÁ®ãÂª∫Ê®°‰∏∫Âº∫ÂåñÂ≠¶‰π†ÁéØÂ¢ÉÔºå
    Êô∫ËÉΩ‰ΩìÈÄöËøá‰∏éÁéØÂ¢É‰∫§‰∫íÂ≠¶‰π†ÊúÄ‰ºòÁöÑÊ±ÇËß£Á≠ñÁï•
    """
    
    def __init__(self, solver_config: Dict, physics_config: PhysicsConfig = None):
        self.solver_config = solver_config
        self.physics_config = physics_config or PhysicsConfig()
        self.max_steps = solver_config.get('max_steps', 100)
        self.current_step = 0
        self.convergence_history = []
        self.performance_metrics = {}
        
        # Ê±ÇËß£Á≠ñÁï•ÂèÇÊï∞ËåÉÂõ¥
        self.action_bounds = {
            'time_step': (0.001, 0.1),      # Êó∂Èó¥Ê≠•Èïø
            'mesh_refinement': (0.1, 2.0),  # ÁΩëÊ†ºÂä†ÂØÜÂõ†Â≠ê
            'tolerance': (1e-6, 1e-3),      # Êî∂ÊïõÂÆπÂ∑Æ
            'max_iterations': (50, 500)      # ÊúÄÂ§ßËø≠‰ª£Ê¨°Êï∞
        }
        
        print(f"üîÑ Ê±ÇËß£Âô®ÁéØÂ¢ÉÂàùÂßãÂåñÂÆåÊàê")
        print(f"   ÊúÄÂ§ßÊ≠•Êï∞: {self.max_steps}")
        print(f"   Âä®‰ΩúÂèÇÊï∞: {list(self.action_bounds.keys())}")
    
    def reset(self) -> np.ndarray:
        """ÈáçÁΩÆÁéØÂ¢É"""
        self.current_step = 0
        self.convergence_history = []
        self.performance_metrics = {}
        
        # ËøîÂõûÂàùÂßãÁä∂ÊÄÅ
        initial_state = self._get_state()
        return initial_state
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """ÊâßË°å‰∏ÄÊ≠•Âä®‰Ωú"""
        if self.current_step >= self.max_steps:
            return self._get_state(), 0.0, True, {}
        
        # Ëß£ÊûêÂä®‰ΩúÔºà‰ªé[-1,1]Êò†Â∞ÑÂà∞ÂÆûÈôÖÂèÇÊï∞ËåÉÂõ¥Ôºâ
        solver_params = self._action_to_params(action)
        
        # Ê®°ÊãüÊ±ÇËß£ËøáÁ®ã
        reward, metrics = self._simulate_solving(solver_params)
        
        # Êõ¥Êñ∞Áä∂ÊÄÅ
        self.current_step += 1
        self.convergence_history.append(metrics.get('convergence', 0.0))
        self.performance_metrics.update(metrics)
        
        # Ê£ÄÊü•ÊòØÂê¶ÂÆåÊàê
        done = self.current_step >= self.max_steps or metrics.get('converged', False)
        
        return self._get_state(), reward, done, metrics
    
    def _action_to_params(self, action: np.ndarray) -> Dict:
        """Â∞ÜÂä®‰ΩúËΩ¨Êç¢‰∏∫Ê±ÇËß£Âô®ÂèÇÊï∞"""
        params = {}
        action_names = list(self.action_bounds.keys())
        
        for i, name in enumerate(action_names):
            if i < len(action):
                # Â∞Ü[-1,1]Êò†Â∞ÑÂà∞ÂÆûÈôÖËåÉÂõ¥
                action_val = action[i]
                min_val, max_val = self.action_bounds[name]
                param_val = min_val + (action_val + 1) * (max_val - min_val) / 2
                params[name] = param_val
        
        return params
    
    def _simulate_solving(self, solver_params: Dict) -> Tuple[float, Dict]:
        """Ê®°ÊãüÊ±ÇËß£ËøáÁ®ãÔºåËÆ°ÁÆóÂ•ñÂä±ÂíåÊåáÊ†á"""
        # Ê®°ÊãüÊ±ÇËß£Êó∂Èó¥ÔºàÂü∫‰∫éÂèÇÊï∞Ôºâ
        time_step = solver_params.get('time_step', 0.01)
        mesh_refinement = solver_params.get('mesh_refinement', 1.0)
        tolerance = solver_params.get('tolerance', 1e-4)
        max_iterations = solver_params.get('max_iterations', 100)
        
        # Ê®°ÊãüÊî∂ÊïõËøáÁ®ã
        convergence_rate = 1.0 / (1.0 + tolerance * 1000)  # ÂÆπÂ∑ÆË∂äÂ∞èÔºåÊî∂ÊïõË∂äÂø´
        mesh_efficiency = 1.0 / (1.0 + abs(mesh_refinement - 1.0))  # ÁΩëÊ†ºÂõ†Â≠êÊé•Ëøë1Êó∂ÊïàÁéáÊúÄÈ´ò
        
        # Ê®°ÊãüËø≠‰ª£Ê¨°Êï∞
        actual_iterations = min(max_iterations, int(50 / convergence_rate))
        
        # ËÆ°ÁÆóÂ•ñÂä±ÔºàÁªºÂêàËÄÉËôëÊïàÁéá„ÄÅÁ≤æÂ∫¶„ÄÅÁ®≥ÂÆöÊÄßÔºâ
        efficiency_reward = 1.0 / (1.0 + time_step * 100)  # Êó∂Èó¥Ê≠•ÈïøË∂äÂ∞èË∂äÂ•Ω
        accuracy_reward = 1.0 / (1.0 + tolerance * 1e6)    # ÂÆπÂ∑ÆË∂äÂ∞èË∂äÂ•Ω
        stability_reward = 1.0 / (1.0 + abs(mesh_refinement - 1.0))  # ÁΩëÊ†ºÁ®≥ÂÆöÊÄß
        
        # Êî∂ÊïõÂ•ñÂä±
        converged = actual_iterations < max_iterations
        convergence_reward = 10.0 if converged else 0.0
        
        # ÊÄªÂ•ñÂä±
        total_reward = (efficiency_reward + accuracy_reward + stability_reward + convergence_reward) / 4
        
        # ÊÄßËÉΩÊåáÊ†á
        metrics = {
            'convergence': convergence_rate,
            'mesh_efficiency': mesh_efficiency,
            'iterations': actual_iterations,
            'converged': converged,
            'time_step': time_step,
            'mesh_refinement': mesh_refinement,
            'tolerance': tolerance
        }
        
        return total_reward, metrics
    
    def _get_state(self) -> np.ndarray:
        """Ëé∑ÂèñÂΩìÂâçÁä∂ÊÄÅ"""
        state = []
        
        # ÂΩìÂâçÊ≠•Êï∞ÔºàÂΩí‰∏ÄÂåñÔºâ
        state.append(self.current_step / self.max_steps)
        
        # Êî∂ÊïõÂéÜÂè≤ÁªüËÆ°
        if self.convergence_history:
            state.extend([
                np.mean(self.convergence_history),
                np.std(self.convergence_history),
                self.convergence_history[-1] if self.convergence_history else 0.0
            ])
        else:
            state.extend([0.0, 0.0, 0.0])
        
        # ÊÄßËÉΩÊåáÊ†á
        for key in ['mesh_efficiency', 'iterations']:
            if key in self.performance_metrics:
                # ÂΩí‰∏ÄÂåñÂà∞[0,1]
                if key == 'iterations':
                    val = self.performance_metrics[key] / 500.0  # ÂÅáËÆæÊúÄÂ§ß500Ê¨°Ëø≠‰ª£
                else:
                    val = self.performance_metrics[key]
                state.append(val)
            else:
                state.append(0.0)
        
        return np.array(state, dtype=np.float32)


class RLSolverOptimizer(BaseSolver):
    """
    Âº∫ÂåñÂ≠¶‰π†Ê±ÇËß£Âô®‰ºòÂåñÂô® - ‰ΩøÁî®RLËá™Âä®‰ºòÂåñÊï∞ÂÄºÊ±ÇËß£Á≠ñÁï•
    
    Ê†∏ÂøÉÊÄùÊÉ≥ÔºöÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÊô∫ËÉΩ‰ΩìÔºåËá™Âä®ÈÄâÊã©ÊúÄ‰ºòÁöÑÊ±ÇËß£ÂèÇÊï∞Ôºå
    ÂÆûÁé∞"Ëá™Â≠¶‰π†"ÁöÑÊï∞ÂÄºÊ±ÇËß£‰ºòÂåñ
    """
    
    def __init__(self, state_dim: int, action_dim: int, solver_config: Dict = None):
        super().__init__()
        
        if not HAS_PYTORCH:
            raise ImportError("ÈúÄË¶ÅÂÆâË£ÖPyTorchÊù•‰ΩøÁî®RLÊ±ÇËß£Âô®‰ºòÂåñÂô®")
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.solver_config = solver_config or {}
        
        # ÂàõÂª∫RLÊô∫ËÉΩ‰ΩìÂíåÁéØÂ¢É
        self.agent = RLAgent(state_dim, action_dim)
        self.environment = SolverEnvironment(solver_config)
        
        # ËÆ≠ÁªÉÂèÇÊï∞
        self.learning_rate = 0.001
        self.gamma = 0.99  # ÊäòÊâ£Âõ†Â≠ê
        self.tau = 0.005   # ËΩØÊõ¥Êñ∞ÂèÇÊï∞
        
        # ÁªèÈ™åÂõûÊîæÁºìÂÜ≤Âå∫
        self.replay_buffer = []
        self.buffer_size = 10000
        self.batch_size = 64
        
        # ÁõÆÊ†áÁΩëÁªúÔºàÁî®‰∫éÁ®≥ÂÆöËÆ≠ÁªÉÔºâ
        self.target_agent = RLAgent(state_dim, action_dim)
        self._update_target_network()
        
        self.optimizer_actor = optim.Adam(self.agent.actor.parameters(), lr=self.learning_rate)
        self.optimizer_critic = optim.Adam(self.agent.critic.parameters(), lr=self.learning_rate)
        
        print(f"üîÑ RLÊ±ÇËß£Âô®‰ºòÂåñÂô®ÂàùÂßãÂåñÂÆåÊàê")
        print(f"   Áä∂ÊÄÅÁª¥Â∫¶: {state_dim}, Âä®‰ΩúÁª¥Â∫¶: {action_dim}")
    
    def _update_target_network(self):
        """ËΩØÊõ¥Êñ∞ÁõÆÊ†áÁΩëÁªú"""
        for target_param, param in zip(self.target_agent.parameters(), self.agent.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
    
    def _store_experience(self, state: np.ndarray, action: np.ndarray, 
                         reward: float, next_state: np.ndarray, done: bool):
        """Â≠òÂÇ®ÁªèÈ™åÂà∞ÂõûÊîæÁºìÂÜ≤Âå∫"""
        experience = (state, action, reward, next_state, done)
        self.replay_buffer.append(experience)
        
        # ÈôêÂà∂ÁºìÂÜ≤Âå∫Â§ßÂ∞è
        if len(self.replay_buffer) > self.buffer_size:
            self.replay_buffer.pop(0)
    
    def _sample_batch(self) -> List[Tuple]:
        """‰ªéÂõûÊîæÁºìÂÜ≤Âå∫ÈááÊ†∑ÊâπÊ¨°"""
        if len(self.replay_buffer) < self.batch_size:
            return []
        
        indices = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)
        return [self.replay_buffer[i] for i in indices]
    
    def _update_networks(self, batch: List[Tuple]):
        """Êõ¥Êñ∞ÁΩëÁªúÂèÇÊï∞"""
        if not batch:
            return
        
        states = torch.FloatTensor(np.array([exp[0] for exp in batch])).to(self.device)
        actions = torch.FloatTensor(np.array([exp[1] for exp in batch])).to(self.device)
        rewards = torch.FloatTensor(np.array([exp[2] for exp in batch])).to(self.device)
        next_states = torch.FloatTensor(np.array([exp[3] for exp in batch])).to(self.device)
        dones = torch.BoolTensor(np.array([exp[4] for exp in batch])).to(self.device)
        
        # Êõ¥Êñ∞CriticÁΩëÁªú
        current_q_values = self.agent.get_value(states, actions)
        next_actions = self.target_agent(next_states)
        next_q_values = self.target_agent.get_value(next_states, next_actions)
        target_q_values = rewards + (self.gamma * next_q_values * (~dones).float())
        
        critic_loss = F.mse_loss(current_q_values, target_q_values.detach())
        
        self.optimizer_critic.zero_grad()
        critic_loss.backward()
        self.optimizer_critic.step()
        
        # Êõ¥Êñ∞ActorÁΩëÁªú
        actor_loss = -self.agent.get_value(states, self.agent(states)).mean()
        
        self.optimizer_actor.zero_grad()
        actor_loss.backward()
        self.optimizer_actor.step()
        
        # ËΩØÊõ¥Êñ∞ÁõÆÊ†áÁΩëÁªú
        self._update_target_network()
        
        return {
            'critic_loss': critic_loss.item(),
            'actor_loss': actor_loss.item()
        }
    
    def train(self, episodes: int = 1000, **kwargs) -> dict:
        """ËÆ≠ÁªÉRLÊô∫ËÉΩ‰Ωì"""
        print(f"üîÑ ÂºÄÂßãËÆ≠ÁªÉRLÊ±ÇËß£Âô®‰ºòÂåñÂô®ÔºåÊÄªËΩÆÊï∞: {episodes}")
        
        episode_rewards = []
        episode_lengths = []
        training_losses = []
        
        for episode in range(episodes):
            state = self.environment.reset()
            episode_reward = 0.0
            episode_length = 0
            
            while True:
                # ÈÄâÊã©Âä®‰Ωú
                action = self.agent.get_action(state, noise_scale=max(0.01, 0.1 * (1 - episode / episodes)))
                
                # ÊâßË°åÂä®‰Ωú
                next_state, reward, done, info = self.environment.step(action)
                
                # Â≠òÂÇ®ÁªèÈ™å
                self._store_experience(state, action, reward, next_state, done)
                
                # Êõ¥Êñ∞ÁΩëÁªú
                batch = self._sample_batch()
                if batch:
                    loss_info = self._update_networks(batch)
                    training_losses.append(loss_info)
                
                state = next_state
                episode_reward += reward
                episode_length += 1
                
                if done:
                    break
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                avg_length = np.mean(episode_lengths[-100:])
                print(f"   ËΩÆÊï∞ {episode+1}/{episodes}: Âπ≥ÂùáÂ•ñÂä±={avg_reward:.4f}, Âπ≥ÂùáÈïøÂ∫¶={avg_length:.1f}")
        
        self.is_trained = True
        
        training_history = {
            'episodes': episodes,
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'training_losses': training_losses,
            'final_avg_reward': np.mean(episode_rewards[-100:]) if episode_rewards else 0.0
        }
        
        print(f"‚úÖ RLËÆ≠ÁªÉÂÆåÊàêÔºåÊúÄÁªàÂπ≥ÂùáÂ•ñÂä±: {training_history['final_avg_reward']:.4f}")
        return training_history
    
    def optimize_solver_strategy(self, problem_state: np.ndarray) -> Dict:
        """‰ºòÂåñÊ±ÇËß£Á≠ñÁï•"""
        if not self.is_trained:
            raise ValueError("RLÊô∫ËÉΩ‰ΩìÂ∞öÊú™ËÆ≠ÁªÉ")
        
        # ‰ΩøÁî®ËÆ≠ÁªÉÂ•ΩÁöÑÊô∫ËÉΩ‰ΩìÈÄâÊã©ÊúÄ‰ºòÂä®‰Ωú
        optimal_action = self.agent.get_action(problem_state, noise_scale=0.0)
        
        # ËΩ¨Êç¢‰∏∫Ê±ÇËß£Âô®ÂèÇÊï∞
        solver_params = self.environment._action_to_params(optimal_action)
        
        print(f"üîß RL‰ºòÂåñÊ±ÇËß£Á≠ñÁï•:")
        for param, value in solver_params.items():
            print(f"   {param}: {value:.6f}")
        
        return solver_params
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """È¢ÑÊµãÊúÄ‰ºòÊ±ÇËß£Á≠ñÁï•"""
        if not self.is_trained:
            raise ValueError("RLÊô∫ËÉΩ‰ΩìÂ∞öÊú™ËÆ≠ÁªÉ")
        
        strategies = []
        for state in X:
            action = self.agent.get_action(state, noise_scale=0.0)
            strategy = self.environment._action_to_params(action)
            strategies.append(list(strategy.values()))
        
        return np.array(strategies)
    
    def evaluate_strategy(self, strategy: Dict, problem_state: np.ndarray) -> Dict:
        """ËØÑ‰º∞Ê±ÇËß£Á≠ñÁï•ÁöÑÊÄßËÉΩ"""
        # Â∞ÜÁ≠ñÁï•ËΩ¨Êç¢‰∏∫Âä®‰Ωú
        action = np.array([strategy.get(param, 0.0) for param in self.environment.action_bounds.keys()])
        
        # Âú®ÁéØÂ¢É‰∏≠ÊµãËØïÁ≠ñÁï•
        state = problem_state
        total_reward = 0.0
        step_count = 0
        
        for _ in range(self.environment.max_steps):
            next_state, reward, done, info = self.environment.step(action)
            total_reward += reward
            step_count += 1
            
            if done:
                break
        
        return {
            'total_reward': total_reward,
            'step_count': step_count,
            'efficiency': total_reward / max(step_count, 1),
            'convergence': info.get('converged', False)
        }


def create_rl_solver_system() -> Dict:
    """ÂàõÂª∫RLÊ±ÇËß£Âô®Á≥ªÁªü"""
    system = {
        'agent': RLAgent,
        'environment': SolverEnvironment,
        'optimizer': RLSolverOptimizer
    }
    
    print("üîÑ RLÊ±ÇËß£Âô®Á≥ªÁªüÂàõÂª∫ÂÆåÊàê")
    return system


def demo_rl_solver_optimization():
    """ÊºîÁ§∫RLÊ±ÇËß£Âô®‰ºòÂåñ"""
    print("ü§ñ Âº∫ÂåñÂ≠¶‰π†Ê±ÇËß£Âô®‰ºòÂåñÊºîÁ§∫")
    print("=" * 60)
    
    try:
        # ÂàõÂª∫RLÊ±ÇËß£Âô®Á≥ªÁªü
        rl_system = create_rl_solver_system()
        
        # ÈÖçÁΩÆÊ±ÇËß£Âô®ÁéØÂ¢É
        solver_config = {
            'max_steps': 50,
            'convergence_threshold': 1e-6
        }
        
        # ÂàõÂª∫ÁéØÂ¢É
        env = rl_system['environment'](solver_config)
        state_dim = len(env.reset())
        action_dim = len(env.action_bounds)
        
        print(f"üìä ÁéØÂ¢ÉÈÖçÁΩÆ:")
        print(f"   Áä∂ÊÄÅÁª¥Â∫¶: {state_dim}")
        print(f"   Âä®‰ΩúÁª¥Â∫¶: {action_dim}")
        print(f"   ÊúÄÂ§ßÊ≠•Êï∞: {env.max_steps}")
        
        # ÂàõÂª∫RL‰ºòÂåñÂô®
        rl_optimizer = rl_system['optimizer'](state_dim, action_dim, solver_config)
        
        # ËÆ≠ÁªÉRLÊô∫ËÉΩ‰Ωì
        print("\nüîß ËÆ≠ÁªÉRLÊô∫ËÉΩ‰Ωì...")
        training_history = rl_optimizer.train(episodes=500)
        
        print(f"   ËÆ≠ÁªÉÂÆåÊàêÔºåÊúÄÁªàÂπ≥ÂùáÂ•ñÂä±: {training_history['final_avg_reward']:.4f}")
        
        # ÊµãËØï‰ºòÂåñÂêéÁöÑÁ≠ñÁï•
        print("\nüîß ÊµãËØï‰ºòÂåñÂêéÁöÑÊ±ÇËß£Á≠ñÁï•...")
        
        # Ê®°ÊãüÈóÆÈ¢òÁä∂ÊÄÅ
        test_state = np.array([0.0, 0.5, 0.1, 0.8, 0.3])
        
        # Ëé∑ÂèñÊúÄ‰ºòÁ≠ñÁï•
        optimal_strategy = rl_optimizer.optimize_solver_strategy(test_state)
        
        # ËØÑ‰º∞Á≠ñÁï•ÊÄßËÉΩ
        performance = rl_optimizer.evaluate_strategy(optimal_strategy, test_state)
        
        print(f"   Á≠ñÁï•ÊÄßËÉΩËØÑ‰º∞:")
        print(f"     ÊÄªÂ•ñÂä±: {performance['total_reward']:.4f}")
        print(f"     Ê≠•Êï∞: {performance['step_count']}")
        print(f"     ÊïàÁéá: {performance['efficiency']:.4f}")
        print(f"     Êî∂Êïõ: {performance['convergence']}")
        
        print("\n‚úÖ RLÊ±ÇËß£Âô®‰ºòÂåñÊºîÁ§∫ÂÆåÊàê!")
        return True
        
    except Exception as e:
        print(f"‚ùå RLÊ±ÇËß£Âô®‰ºòÂåñÊºîÁ§∫Â§±Ë¥•: {e}")
        return False


def demo_advanced_ml():
    """ÊºîÁ§∫È´òÁ∫ßMLÂäüËÉΩ"""
    print("ü§ñ È´òÁ∫ßÊú∫Âô®Â≠¶‰π†Âä†ÈÄüÊï∞ÂÄºÊ®°ÊãüÊºîÁ§∫")
    print("=" * 60)
    
    # Âõ∫ÂÆöÈöèÊú∫ÁßçÂ≠êÔºåÁ°Æ‰øùÁªìÊûúÂèØÂ§çÁé∞
    np.random.seed(42)
    if HAS_PYTORCH:
        torch.manual_seed(42)
    
    # ÂàõÂª∫È´òÁ∫ßMLÁ≥ªÁªü
    ml_system = create_advanced_ml_system()
    
    # ÁîüÊàêÊµãËØïÊï∞ÊçÆ
    n_samples = 1000
    input_dim = 5
    output_dim = 3
    
    X = np.random.randn(n_samples, input_dim)
    y = np.random.randn(n_samples, output_dim)
    
    print(f"üìä ÊµãËØïÊï∞ÊçÆ: {n_samples} Ê†∑Êú¨, ËæìÂÖ•Áª¥Â∫¶: {input_dim}, ËæìÂá∫Áª¥Â∫¶: {output_dim}")
    
    # 1. ÊµãËØïÈ´òÁ∫ßPINN
    print("\nüîß ÊµãËØïÈ´òÁ∫ßPINN...")
    try:
        # ÂÆö‰πâÁâ©ÁêÜÊñπÁ®ãÔºàÁ§∫‰æãÔºöÁÉ≠‰º†ÂØºÊñπÁ®ãÔºâ
        def heat_equation(x, y):
            return torch.mean(torch.abs(y - 0.1 * torch.sum(x, dim=1, keepdim=True)))
        
        # ÂÆö‰πâËæπÁïåÊù°‰ª∂
        def boundary_condition(x, y):
            return torch.mean(torch.abs(y[:, 0] - 0.0))  # Á¨¨‰∏Ä‰∏™ËæìÂá∫Âú®ËæπÁïå‰∏ä‰∏∫0
        
        pinn = ml_system['pinn'](input_dim, [64, 32], output_dim, 
                                physics_equations=[heat_equation],
                                boundary_conditions=[boundary_condition])
        
        pinn.setup_training()
        result = pinn.train(X, y, epochs=200)
        print(f"   ËÆ≠ÁªÉÊó∂Èó¥: {result['train_time']:.4f} Áßí")
        print(f"   ÊúÄÁªàÊÄªÊçüÂ§±: {result['total_loss'][-1]:.6f}")
        
    except Exception as e:
        print(f"   ‚ùå È´òÁ∫ßPINNÂ§±Ë¥•: {e}")
    
    # 2. ÊµãËØïÈ´òÁ∫ß‰ª£ÁêÜÊ®°Âûã
    print("\nüîß ÊµãËØïÈ´òÁ∫ß‰ª£ÁêÜÊ®°Âûã...")
    try:
        surrogate = ml_system['surrogate']('gaussian_process', kernel=RBF(length_scale=1.0) + Matern(length_scale=1.0))
        result = surrogate.train(X, y[:, 0])  # Âè™È¢ÑÊµãÁ¨¨‰∏Ä‰∏™ËæìÂá∫
        print(f"   ËÆ≠ÁªÉÊó∂Èó¥: {result['training_time']:.4f} Áßí")
        
        # È¢ÑÊµã
        predictions, std = surrogate.predict(X[:10], return_std=True)
        print(f"   È¢ÑÊµãÂΩ¢Áä∂: {predictions.shape}, Ê†áÂáÜÂ∑ÆÂΩ¢Áä∂: {std.shape}")
        
    except Exception as e:
        print(f"   ‚ùå È´òÁ∫ß‰ª£ÁêÜÊ®°ÂûãÂ§±Ë¥•: {e}")
    
    # 3. ÊµãËØïÂ§öÂ∞∫Â∫¶Ê°•Êé•
    print("\nüîß ÊµãËØïÂ§öÂ∞∫Â∫¶Ê°•Êé•...")
    try:
        bridge = ml_system['bridge']()
        bridge.setup_bridge_model(input_dim, output_dim, 'neural_network')
        
        # Ê®°ÊãüÁªÜÂ∞∫Â∫¶ÂíåÁ≤óÂ∞∫Â∫¶Êï∞ÊçÆ
        fine_data = X
        coarse_data = y
        
        result = bridge.train_bridge(fine_data, coarse_data, epochs=100)
        print(f"   Ê°•Êé•Ê®°ÂûãËÆ≠ÁªÉÂÆåÊàê")
        
        # ÊµãËØïÊ°•Êé•
        coarse_pred = bridge.predict_coarse_from_fine(X[:10])
        print(f"   Á≤óÂ∞∫Â∫¶È¢ÑÊµãÂΩ¢Áä∂: {coarse_pred.shape}")
        
    except Exception as e:
        print(f"   ‚ùå Â§öÂ∞∫Â∫¶Ê°•Êé•Â§±Ë¥•: {e}")
    
    # 4. ÊµãËØïÊ∑∑ÂêàÂä†ÈÄüÂô®
    print("\nüîß ÊµãËØïÊ∑∑ÂêàÂä†ÈÄüÂô®...")
    try:
        hybrid_accelerator = ml_system['hybrid']()
        
        # Ê∑ªÂä†MLÊ®°Âûã
        surrogate_model = ml_system['surrogate']('random_forest')
        surrogate_model.train(X, y[:, 0])
        hybrid_accelerator.add_ml_model('surrogate', surrogate_model)
        
        # ËÆæÁΩÆÂä†ÈÄüÁ≠ñÁï•
        hybrid_accelerator.setup_acceleration_strategy('initial_guess', 'surrogate')
        
        # ÊµãËØïÊ∑∑ÂêàÊ±ÇËß£
        problem_data = {'input': X[:10]}
        result = hybrid_accelerator.solve_hybrid(problem_data, use_ml=True, ml_model_name='surrogate')
        print(f"   Ê∑∑ÂêàÊ±ÇËß£ÂÆåÊàêÔºå‰ΩøÁî®Ê®°Âûã: {result['model_name']}")
        print(f"   Ê±ÇËß£Êó∂Èó¥: {result['time']:.4f} Áßí")
        
    except Exception as e:
        print(f"   ‚ùå Ê∑∑ÂêàÂä†ÈÄüÂô®Â§±Ë¥•: {e}")
    
    # 5. ÊµãËØïËá™ÈÄÇÂ∫îÊ±ÇËß£Âô®
    print("\nüîß ÊµãËØïËá™ÈÄÇÂ∫îÊ±ÇËß£Âô®...")
    try:
        adaptive_solver = ml_system['adaptive']()
        
        # Ê∑ªÂä†‰∏çÂêåÁöÑÊ±ÇËß£Âô®
        def fast_solver(data):
            return np.random.randn(data.get('size', 100))
        
        def accurate_solver(data):
            time.sleep(0.01)  # Ê®°ÊãüËÆ°ÁÆóÊó∂Èó¥
            return np.random.randn(data.get('size', 100))
        
        # ‰ΩøÁî®Êñ∞ÁöÑÊù°‰ª∂Ê†ºÂºè
        adaptive_solver.add_solver('fast', fast_solver, 
                                 conditions={'size': ('<', 1000)}, priority=1)
        adaptive_solver.add_solver('accurate', accurate_solver, 
                                 conditions={'size': ('>=', 1000)}, priority=2)
        
        # ËÆæÁΩÆÈÄâÊã©Á≠ñÁï•ÂíåËØÑÂàÜÊùÉÈáç
        adaptive_solver.set_selection_strategy('hybrid')
        adaptive_solver.set_score_weights({
            'problem_feature': 1.0,
            'accuracy': 0.6,
            'speed': 0.4,
            'priority': 0.1
        })
        
        # ÊµãËØïÊ±ÇËß£
        problem_data = {'size': 500, 'accuracy_requirement': 0.8}
        result = adaptive_solver.solve(problem_data)
        print(f"   ‰ΩøÁî®ÁöÑÊ±ÇËß£Âô®: {result['solver_used']}")
        print(f"   Ê±ÇËß£Êó∂Èó¥: {result['time']:.4f} Áßí")
        
    except Exception as e:
        print(f"   ‚ùå Ëá™ÈÄÇÂ∫îÊ±ÇËß£Âô®Â§±Ë¥•: {e}")
    
    print("\n‚úÖ È´òÁ∫ßÊú∫Âô®Â≠¶‰π†Âä†ÈÄüÊï∞ÂÄºÊ®°ÊãüÊºîÁ§∫ÂÆåÊàê!")


def create_rl_solver_system() -> Dict:
    """ÂàõÂª∫RLÊ±ÇËß£Âô®Á≥ªÁªü"""
    system = {
        'agent': RLAgent,
        'environment': SolverEnvironment,
        'optimizer': RLSolverOptimizer
    }
    
    print("üîÑ RLÊ±ÇËß£Âô®Á≥ªÁªüÂàõÂª∫ÂÆåÊàê")
    return system


def demo_rl_solver_optimization():
    """ÊºîÁ§∫RLÊ±ÇËß£Âô®‰ºòÂåñ"""
    print("ü§ñ Âº∫ÂåñÂ≠¶‰π†Ê±ÇËß£Âô®‰ºòÂåñÊºîÁ§∫")
    print("=" * 60)
    
    try:
        # ÂàõÂª∫RLÊ±ÇËß£Âô®Á≥ªÁªü
        rl_system = create_rl_solver_system()
        
        # ÈÖçÁΩÆÊ±ÇËß£Âô®ÁéØÂ¢É
        solver_config = {
            'max_steps': 50,
            'convergence_threshold': 1e-6
        }
        
        # ÂàõÂª∫ÁéØÂ¢É
        env = rl_system['environment'](solver_config)
        state_dim = len(env.reset())
        action_dim = len(env.action_bounds)
        
        print(f"üìä ÁéØÂ¢ÉÈÖçÁΩÆ:")
        print(f"   Áä∂ÊÄÅÁª¥Â∫¶: {state_dim}")
        print(f"   Âä®‰ΩúÁª¥Â∫¶: {action_dim}")
        print(f"   ÊúÄÂ§ßÊ≠•Êï∞: {env.max_steps}")
        
        # ÂàõÂª∫RL‰ºòÂåñÂô®
        rl_optimizer = rl_system['optimizer'](state_dim, action_dim, solver_config)
        
        # ËÆ≠ÁªÉRLÊô∫ËÉΩ‰Ωì
        print("\nüîß ËÆ≠ÁªÉRLÊô∫ËÉΩ‰Ωì...")
        training_history = rl_optimizer.train(episodes=500)
        
        print(f"   ËÆ≠ÁªÉÂÆåÊàêÔºåÊúÄÁªàÂπ≥ÂùáÂ•ñÂä±: {training_history['final_avg_reward']:.4f}")
        
        # ÊµãËØï‰ºòÂåñÂêéÁöÑÁ≠ñÁï•
        print("\nüîß ÊµãËØï‰ºòÂåñÂêéÁöÑÊ±ÇËß£Á≠ñÁï•...")
        
        # Ê®°ÊãüÈóÆÈ¢òÁä∂ÊÄÅ
        test_state = np.array([0.0, 0.5, 0.1, 0.8, 0.3])
        
        # Ëé∑ÂèñÊúÄ‰ºòÁ≠ñÁï•
        optimal_strategy = rl_optimizer.optimize_solver_strategy(test_state)
        
        # ËØÑ‰º∞Á≠ñÁï•ÊÄßËÉΩ
        performance = rl_optimizer.evaluate_strategy(optimal_strategy, test_state)
        
        print(f"   Á≠ñÁï•ÊÄßËÉΩËØÑ‰º∞:")
        print(f"     ÊÄªÂ•ñÂä±: {performance['total_reward']:.4f}")
        print(f"     Ê≠•Êï∞: {performance['step_count']}")
        print(f"     ÊïàÁéá: {performance['efficiency']:.4f}")
        print(f"     Êî∂Êïõ: {performance['convergence']}")
        
        print("\n‚úÖ RLÊ±ÇËß£Âô®‰ºòÂåñÊºîÁ§∫ÂÆåÊàê!")
        return True
        
    except Exception as e:
        print(f"‚ùå RLÊ±ÇËß£Âô®‰ºòÂåñÊºîÁ§∫Â§±Ë¥•: {e}")
        return False


if __name__ == "__main__":
    demo_advanced_ml()
    demo_rl_solver_optimization()
