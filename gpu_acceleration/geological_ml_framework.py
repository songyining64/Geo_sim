"""
åœ°è´¨æ•°å€¼æ¨¡æ‹ŸML/DLèåˆæ¡†æ¶
"""

import numpy as np
import time
import warnings
from typing import Dict, List, Tuple, Optional, Callable, Union, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod

# æ·±åº¦å­¦ä¹ ç›¸å…³ä¾èµ–
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    from torch.utils.data import DataLoader, TensorDataset
    from torch.nn.parameter import Parameter
    HAS_PYTORCH = True
except ImportError:
    HAS_PYTORCH = False
    torch = None
    nn = None
    optim = None
    warnings.warn("PyTorch not available. Geological ML features will be limited.")

try:
    import sklearn
    from sklearn.gaussian_process import GaussianProcessRegressor
    from sklearn.gaussian_process.kernels import RBF, ConstantKernel, Matern
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.neural_network import MLPRegressor
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    warnings.warn("scikit-learn not available. Geological ML features will be limited.")

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    warnings.warn("matplotlib not available. Visualization features will be limited.")

# å¯é€‰ä¾èµ–æ£€æŸ¥
try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.vec_env import DummyVecEnv
    HAS_STABLE_BASELINES3 = True
except ImportError:
    HAS_STABLE_BASELINES3 = False
    warnings.warn("stable-baselines3 not available. RL features will be limited.")


@dataclass
class GeologicalConfig:
    """åœ°è´¨é…ç½®ç±» - å¢å¼ºç‰ˆ"""
    # åŸºç¡€åœ°è´¨å‚æ•°
    porosity: float = 0.2
    permeability: float = 1e-12  # mÂ²
    viscosity: float = 1e-3      # PaÂ·s
    density: float = 1000.0      # kg/mÂ³
    compressibility: float = 1e-9 # Paâ»Â¹
    thermal_conductivity: float = 2.0  # W/(mÂ·K)
    specific_heat: float = 1000.0      # J/(kgÂ·K)
    
    # æ–°å¢ï¼šåœ°è´¨åŠ›å­¦å‚æ•°
    youngs_modulus: float = 1e9      # Paï¼Œæ¨æ°æ¨¡é‡
    poissons_ratio: float = 0.25     # æ³Šæ¾æ¯”
    cohesion: float = 1e6            # Paï¼Œå†…èšåŠ›
    friction_angle: float = 30.0     # åº¦ï¼Œå†…æ‘©æ“¦è§’
    
    # æ–°å¢ï¼šåœ°å¹”å¯¹æµå‚æ•°
    reference_viscosity: float = 1e21    # PaÂ·sï¼Œå‚è€ƒé»åº¦
    thermal_expansion: float = 3e-5      # Kâ»Â¹ï¼Œçƒ­è†¨èƒ€ç³»æ•°
    gravity: float = 9.81                # m/sÂ²ï¼Œé‡åŠ›åŠ é€Ÿåº¦
    rayleigh_number: float = 1e7         # ç‘åˆ©æ•°
    
    # æ–°å¢ï¼šæ–­å±‚æ‘©æ“¦å‚æ•°
    mu0: float = 0.6                     # é™æ‘©æ“¦ç³»æ•°
    a: float = 0.01                      # ç›´æ¥æ•ˆåº”å‚æ•°
    b: float = 0.005                     # æ¼”åŒ–æ•ˆåº”å‚æ•°
    L: float = 0.1                       # ç‰¹å¾æ»‘ç§»è·ç¦» (m)
    v0: float = 1e-6                     # å‚è€ƒæ»‘åŠ¨é€Ÿç‡ (m/s)
    
    # æ–°å¢ï¼šåŒ–å­¦è¾“è¿å‚æ•°
    activation_energy: float = 50e3      # J/molï¼Œæ¿€æ´»èƒ½
    diffusion_coefficient: float = 1e-9  # mÂ²/sï¼Œæ‰©æ•£ç³»æ•°
    reaction_rate: float = 0.01          # sâ»Â¹ï¼Œååº”é€Ÿç‡å¸¸æ•°
    
    # æ–°å¢ï¼šGPUåŠ é€Ÿæ”¯æŒ
    use_gpu: bool = True
    
    # è¾¹ç•Œæ¡ä»¶
    boundary_conditions: Dict = None
    
    def __post_init__(self):
        if self.boundary_conditions is None:
            self.boundary_conditions = {
                'pressure': {'top': 'free', 'bottom': 'fixed'},
                'temperature': {'top': 25, 'bottom': 100},
                'flow': {'inlet': 'fixed_rate', 'outlet': 'fixed_pressure'}
            }


class BaseSolver(ABC):
    """åŸºç¡€æ±‚è§£å™¨æŠ½è±¡ç±» - å®ç°ä»£ç å¤ç”¨"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.is_trained = False
        self.training_history = {}
    
    @abstractmethod
    def train(self, X: np.ndarray, y: np.ndarray, **kwargs) -> dict:
        """è®­ç»ƒæ¨¡å‹"""
        pass
    
    @abstractmethod
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        pass
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        try:
            if hasattr(self, 'state_dict'):
                torch.save(self.state_dict(), filepath)
                print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜: {filepath}")
            else:
                import pickle
                with open(filepath, 'wb') as f:
                    pickle.dump(self, f)
                print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜: {filepath}")
        except Exception as e:
            raise RuntimeError(f"ä¿å­˜æ¨¡å‹å¤±è´¥ï¼š{str(e)}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        try:
            if hasattr(self, 'load_state_dict'):
                self.load_state_dict(torch.load(filepath, map_location=self.device))
                print(f"ğŸ“ æ¨¡å‹å·²åŠ è½½: {filepath}")
            else:
                import pickle
                with open(filepath, 'rb') as f:
                    loaded_model = pickle.load(f)
                    self.__dict__.update(loaded_model.__dict__)
                print(f"ğŸ“ æ¨¡å‹å·²åŠ è½½: {filepath}")
        except FileNotFoundError:
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼š{filepath}")
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æ¨¡å‹å¤±è´¥ï¼š{str(e)}")


class GeologicalPhysicsEquations:
    """åœ°è´¨ç‰©ç†æ–¹ç¨‹é›†åˆ - å¢å¼ºç‰ˆ"""
    
    @staticmethod
    def darcy_equation(x: torch.Tensor, y: torch.Tensor, config: GeologicalConfig) -> torch.Tensor:
        """
        è¾¾è¥¿å®šå¾‹ï¼šâˆ‡Â·(k/Î¼ âˆ‡p) = q
        
        Args:
            x: è¾“å…¥åæ ‡ (x, y, z, t)
            y: æ¨¡å‹è¾“å‡º (å‹åŠ›åœº p)
            config: åœ°è´¨é…ç½®å‚æ•°
        
        Returns:
            è¾¾è¥¿æ–¹ç¨‹æ®‹å·®
        """
        if not HAS_PYTORCH:
            return torch.tensor(0.0)
        
        # æ”¹è¿›ç‰ˆæœ¬ï¼šæ›´ç²¾ç¡®çš„è¾¾è¥¿å®šå¾‹å®ç°
        p = y.unsqueeze(-1) if y.dim() == 1 else y
        
        # è®¡ç®—å‹åŠ›æ¢¯åº¦çš„äºŒé˜¶å¯¼æ•°ï¼ˆæ‹‰æ™®æ‹‰æ–¯ç®—å­ï¼‰
        p_grad_x = torch.autograd.grad(
            p.sum(), x[:, 0], 
            grad_outputs=torch.ones_like(p), 
            create_graph=True, retain_graph=True
        )[0]
        
        p_grad_y = torch.autograd.grad(
            p.sum(), x[:, 1], 
            grad_outputs=torch.ones_like(p), 
            create_graph=True, retain_graph=True
        )[0]
        
        if x.shape[1] > 2:  # 3Dæƒ…å†µ
            p_grad_z = torch.autograd.grad(
                p.sum(), x[:, 2], 
                grad_outputs=torch.ones_like(p), 
                create_graph=True, retain_graph=True
            )[0]
            laplacian_p = torch.autograd.grad(p_grad_x.sum(), x[:, 0], create_graph=True)[0] + \
                          torch.autograd.grad(p_grad_y.sum(), x[:, 1], create_graph=True)[0] + \
                          torch.autograd.grad(p_grad_z.sum(), x[:, 2], create_graph=True)[0]
        else:  # 2Dæƒ…å†µ
            laplacian_p = torch.autograd.grad(p_grad_x.sum(), x[:, 0], create_graph=True)[0] + \
                          torch.autograd.grad(p_grad_y.sum(), x[:, 1], create_graph=True)[0]
        
        # è¾¾è¥¿å®šå¾‹æ®‹å·®ï¼šâˆ‡Â·(k/Î¼ âˆ‡p) = q
        k_over_mu = config.permeability / config.viscosity
        source_term = 0.01  # æºé¡¹
        residual = k_over_mu * laplacian_p - source_term
        
        return torch.mean(torch.abs(residual))
    
    @staticmethod
    def heat_conduction_equation(x: torch.Tensor, y: torch.Tensor, config: GeologicalConfig) -> torch.Tensor:
        """
        çƒ­ä¼ å¯¼æ–¹ç¨‹ï¼šÏcâˆ‚T/âˆ‚t = âˆ‡Â·(kâˆ‡T) + Q
        
        Args:
            x: è¾“å…¥åæ ‡ (x, y, z, t)
            y: æ¨¡å‹è¾“å‡º (æ¸©åº¦åœº T)
            config: åœ°è´¨é…ç½®å‚æ•°
        
        Returns:
            çƒ­ä¼ å¯¼æ–¹ç¨‹æ®‹å·®
        """
        if not HAS_PYTORCH:
            return torch.tensor(0.0)
        
        # æ”¹è¿›ç‰ˆæœ¬ï¼šæ›´ç²¾ç¡®çš„çƒ­ä¼ å¯¼æ–¹ç¨‹å®ç°
        T = y.unsqueeze(-1) if y.dim() == 1 else y
        
        # è®¡ç®—æ¸©åº¦æ¢¯åº¦çš„äºŒé˜¶å¯¼æ•°
        T_grad_x = torch.autograd.grad(
            T.sum(), x[:, 0], 
            grad_outputs=torch.ones_like(T), 
            create_graph=True, retain_graph=True
        )[0]
        
        T_grad_y = torch.autograd.grad(
            T.sum(), x[:, 1], 
            grad_outputs=torch.ones_like(T), 
            create_graph=True, retain_graph=True
        )[0]
        
        if x.shape[1] > 2:  # 3Dæƒ…å†µ
            T_grad_z = torch.autograd.grad(
                T.sum(), x[:, 2], 
                grad_outputs=torch.ones_like(T), 
                create_graph=True, retain_graph=True
            )[0]
            laplacian_T = torch.autograd.grad(T_grad_x.sum(), x[:, 0], create_graph=True)[0] + \
                          torch.autograd.grad(T_grad_y.sum(), x[:, 1], create_graph=True)[0] + \
                          torch.autograd.grad(T_grad_z.sum(), x[:, 2], create_graph=True)[0]
        else:  # 2Dæƒ…å†µ
            laplacian_T = torch.autograd.grad(T_grad_x.sum(), x[:, 0], create_graph=True)[0] + \
                          torch.autograd.grad(T_grad_y.sum(), x[:, 1], create_graph=True)[0]
        
        # çƒ­ä¼ å¯¼æ–¹ç¨‹æ®‹å·®ï¼šÏcâˆ‚T/âˆ‚t = âˆ‡Â·(kâˆ‡T) + Q
        # ç®€åŒ–ï¼šå¿½ç•¥æ—¶é—´å¯¼æ•°ï¼Œç¨³æ€çƒ­ä¼ å¯¼
        heat_source = 0.01  # çƒ­æº
        residual = config.thermal_conductivity * laplacian_T + heat_source
        
        return torch.mean(torch.abs(residual))
    
    @staticmethod
    def elastic_equilibrium_equation(x: torch.Tensor, y: torch.Tensor, config: GeologicalConfig) -> torch.Tensor:
        """
        å¼¹æ€§åŠ›å­¦å¹³è¡¡æ–¹ç¨‹ï¼šâˆ‡Â·Ïƒ + f = 0
        
        Args:
            x: è¾“å…¥åæ ‡ (x, y, z)
            y: æ¨¡å‹è¾“å‡º (ä½ç§»åœº u)
            config: åœ°è´¨é…ç½®å‚æ•°
        
        Returns:
            å¼¹æ€§å¹³è¡¡æ–¹ç¨‹æ®‹å·®
        """
        if not HAS_PYTORCH:
            return torch.tensor(0.0)
        
        # æ”¹è¿›ç‰ˆæœ¬ï¼šæ›´ç²¾ç¡®çš„å¼¹æ€§åŠ›å­¦æ–¹ç¨‹å®ç°
        u = y.unsqueeze(-1) if y.dim() == 1 else y
        
        # è®¡ç®—ä½ç§»æ¢¯åº¦çš„äºŒé˜¶å¯¼æ•°
        u_grad_x = torch.autograd.grad(
            u.sum(), x[:, 0], 
            grad_outputs=torch.ones_like(u), 
            create_graph=True, retain_graph=True
        )[0]
        
        u_grad_y = torch.autograd.grad(
            u.sum(), x[:, 1], 
            grad_outputs=torch.ones_like(u), 
            create_graph=True, retain_graph=True
        )[0]
        
        if x.shape[1] > 2:  # 3Dæƒ…å†µ
            u_grad_z = torch.autograd.grad(
                u.sum(), x[:, 2], 
                grad_outputs=torch.ones_like(u), 
                create_graph=True, retain_graph=True
            )[0]
            laplacian_u = torch.autograd.grad(u_grad_x.sum(), x[:, 0], create_graph=True)[0] + \
                          torch.autograd.grad(u_grad_y.sum(), x[:, 1], create_graph=True)[0] + \
                          torch.autograd.grad(u_grad_z.sum(), x[:, 2], create_graph=True)[0]
        else:  # 2Dæƒ…å†µ
            laplacian_u = torch.autograd.grad(u_grad_x.sum(), x[:, 0], create_graph=True)[0] + \
                          torch.autograd.grad(u_grad_y.sum(), x[:, 1], create_graph=True)[0]
        
        # å¼¹æ€§å¹³è¡¡æ–¹ç¨‹æ®‹å·®ï¼šâˆ‡Â·Ïƒ + f = 0
        # ä½¿ç”¨é…ç½®ä¸­çš„æ¨æ°æ¨¡é‡å’Œæ³Šæ¾æ¯”
        body_force = config.gravity * config.density  # é‡åŠ›
        residual = config.youngs_modulus / (2 * (1 + config.poissons_ratio)) * laplacian_u + body_force
        
        return torch.mean(torch.abs(residual))
    
    @staticmethod
    def stokes_equation(x: torch.Tensor, y: torch.Tensor, config: GeologicalConfig) -> torch.Tensor:
        """
        åœ°å¹”å¯¹æµçš„StokesåŠ¨é‡æ–¹ç¨‹æ®‹å·®ï¼š
        âˆ‡Â·(Î·(âˆ‡v + âˆ‡v^T)) - âˆ‡p + ÏgÎ±(T-T0) = 0
        
        Args:
            x: è¾“å…¥åæ ‡ (x, y, z)
            y: æ¨¡å‹è¾“å‡º [v_x, v_y, v_z, p, T] 5ä¸ªç‰©ç†é‡
            config: åœ°è´¨é…ç½®å‚æ•°
        
        Returns:
            Stokesæ–¹ç¨‹æ®‹å·®
        """
        if not HAS_PYTORCH:
            return torch.tensor(0.0)
        
        # è§£æè¾“å‡ºï¼šé€Ÿåº¦ã€å‹åŠ›ã€æ¸©åº¦
        vx, vy, vz, p, T = y[:, 0], y[:, 1], y[:, 2], y[:, 3], y[:, 4]
        
        # è®¡ç®—é€Ÿåº¦æ¢¯åº¦ï¼ˆç©ºé—´å¯¼æ•°ï¼‰
        vx_x = torch.autograd.grad(vx, x[:, 0], grad_outputs=torch.ones_like(vx), create_graph=True)[0]
        vx_y = torch.autograd.grad(vx, x[:, 1], grad_outputs=torch.ones_like(vx), create_graph=True)[0]
        vx_z = torch.autograd.grad(vx, x[:, 2], grad_outputs=torch.ones_like(vx), create_graph=True)[0]
        
        vy_x = torch.autograd.grad(vy, x[:, 0], grad_outputs=torch.ones_like(vy), create_graph=True)[0]
        vy_y = torch.autograd.grad(vy, x[:, 1], grad_outputs=torch.ones_like(vy), create_graph=True)[0]
        vy_z = torch.autograd.grad(vy, x[:, 2], grad_outputs=torch.ones_like(vy), create_graph=True)[0]
        
        vz_x = torch.autograd.grad(vz, x[:, 0], grad_outputs=torch.ones_like(vz), create_graph=True)[0]
        vz_y = torch.autograd.grad(vz, x[:, 1], grad_outputs=torch.ones_like(vz), create_graph=True)[0]
        vz_z = torch.autograd.grad(vz, x[:, 2], grad_outputs=torch.ones_like(vz), create_graph=True)[0]
        
        # è®¡ç®—å‹åŠ›æ¢¯åº¦
        p_x = torch.autograd.grad(p, x[:, 0], grad_outputs=torch.ones_like(p), create_graph=True)[0]
        p_y = torch.autograd.grad(p, x[:, 1], grad_outputs=torch.ones_like(p), create_graph=True)[0]
        p_z = torch.autograd.grad(p, x[:, 2], grad_outputs=torch.ones_like(p), create_graph=True)[0]
        
        # é»åº¦ä¸æ¸©åº¦çš„éçº¿æ€§å…³ç³»ï¼ˆåœ°è´¨æ¨¡æ‹Ÿä¸­å…³é”®ç‰¹æ€§ï¼‰
        # ä½¿ç”¨Arrheniuså…³ç³»ï¼šÎ· = Î·â‚€ * exp(E/R * (1/T - 1/Tâ‚€))
        T_ref = 273.15  # å‚è€ƒæ¸©åº¦ (K)
        activation_energy = 200e3  # æ¿€æ´»èƒ½ (J/mol)
        gas_constant = 8.314  # æ°”ä½“å¸¸æ•° (J/(molÂ·K))
        
        # é¿å…é™¤é›¶ï¼Œä½¿ç”¨å®‰å…¨çš„æ¸©åº¦è®¡ç®—
        T_safe = torch.clamp(T + T_ref, min=1e-6)
        eta = config.reference_viscosity * torch.exp(
            activation_energy / gas_constant * (1.0 / T_safe - 1.0 / T_ref)
        )
        
        # è®¡ç®—åº”å˜ç‡å¼ é‡ D = (âˆ‡v + âˆ‡v^T) / 2
        Dxx = vx_x
        Dyy = vy_y
        Dzz = vz_z
        Dxy = (vx_y + vy_x) / 2
        Dxz = (vx_z + vz_x) / 2
        Dyz = (vy_z + vz_y) / 2
        
        # è®¡ç®—åº”åŠ›å¼ é‡ Ïƒ = 2Î·D
        sigma_xx = 2 * eta * Dxx
        sigma_yy = 2 * eta * Dyy
        sigma_zz = 2 * eta * Dzz
        sigma_xy = 2 * eta * Dxy
        sigma_xz = 2 * eta * Dxz
        sigma_yz = 2 * eta * Dyz
        
        # è®¡ç®—åº”åŠ›æ•£åº¦ âˆ‡Â·Ïƒ
        sigma_xx_x = torch.autograd.grad(sigma_xx.sum(), x[:, 0], create_graph=True)[0]
        sigma_xy_y = torch.autograd.grad(sigma_xy.sum(), x[:, 1], create_graph=True)[0]
        sigma_xz_z = torch.autograd.grad(sigma_xz.sum(), x[:, 2], create_graph=True)[0]
        
        sigma_yx_x = torch.autograd.grad(sigma_xy.sum(), x[:, 0], create_graph=True)[0]
        sigma_yy_y = torch.autograd.grad(sigma_yy.sum(), x[:, 1], create_graph=True)[0]
        sigma_yz_z = torch.autograd.grad(sigma_yz.sum(), x[:, 2], create_graph=True)[0]
        
        sigma_zx_x = torch.autograd.grad(sigma_xz.sum(), x[:, 0], create_graph=True)[0]
        sigma_zy_y = torch.autograd.grad(sigma_yz.sum(), x[:, 1], create_graph=True)[0]
        sigma_zz_z = torch.autograd.grad(sigma_zz.sum(), x[:, 2], create_graph=True)[0]
        
        # åŠ¨é‡æ–¹ç¨‹æ®‹å·®ï¼šâˆ‡Â·Ïƒ - âˆ‡p + ÏgÎ±(T-T0) = 0
        residual_x = (sigma_xx_x + sigma_xy_y + sigma_xz_z) - p_x + config.density * config.gravity * config.thermal_expansion * T
        residual_y = (sigma_yx_x + sigma_yy_y + sigma_yz_z) - p_y + config.density * config.gravity * config.thermal_expansion * T
        residual_z = (sigma_zx_x + sigma_zy_y + sigma_zz_z) - p_z + config.density * config.gravity * config.thermal_expansion * T
        
        # è¿”å›æ®‹å·®çš„L2èŒƒæ•°
        return torch.sqrt(torch.mean(residual_x**2 + residual_y**2 + residual_z**2))
    
    @staticmethod
    def mass_conservation_equation(x: torch.Tensor, y: torch.Tensor, config: GeologicalConfig) -> torch.Tensor:
        """
        è´¨é‡å®ˆæ’æ–¹ç¨‹ï¼šâˆ‡Â·v = 0
        
        Args:
            x: è¾“å…¥åæ ‡ (x, y, z)
            y: æ¨¡å‹è¾“å‡º [v_x, v_y, v_z] é€Ÿåº¦åœº
        
        Returns:
            è´¨é‡å®ˆæ’æ–¹ç¨‹æ®‹å·®
        """
        if not HAS_PYTORCH:
            return torch.tensor(0.0)
        
        vx, vy, vz = y[:, 0], y[:, 1], y[:, 2]
        
        # è®¡ç®—é€Ÿåº¦æ•£åº¦
        vx_grad_x = torch.autograd.grad(
            vx.sum(), x[:, 0], 
            grad_outputs=torch.ones_like(vx), 
            create_graph=True, retain_graph=True
        )[0]
        
        vy_grad_y = torch.autograd.grad(
            vy.sum(), x[:, 1], 
            grad_outputs=torch.ones_like(vy), 
            create_graph=True, retain_graph=True
        )[0]
        
        vz_grad_z = torch.autograd.grad(
            vz.sum(), x[:, 2], 
            grad_outputs=torch.ones_like(vz), 
            create_graph=True, retain_graph=True
        )[0]
        
        # è´¨é‡å®ˆæ’æ®‹å·®ï¼šâˆ‡Â·v = 0
        residual = vx_grad_x + vy_grad_y + vz_grad_z
        
        return torch.mean(torch.abs(residual))
    
    @staticmethod
    def fault_slip_equation(x: torch.Tensor, y: torch.Tensor, config: GeologicalConfig) -> torch.Tensor:
        """
        æ–­å±‚æ»‘åŠ¨æ–¹ç¨‹ï¼ˆæ‘©æ“¦æœ¬æ„ï¼‰ï¼šv = vâ‚€ exp((Î¼â‚€ + a ln(v/vâ‚€) - b ln(Î¸))/L)
        
        Args:
            x: è¾“å…¥åæ ‡ (x, y, z, t)
            y: æ¨¡å‹è¾“å‡º [slip_rate, state_variable, stress] æ»‘åŠ¨é€Ÿç‡ã€çŠ¶æ€å˜é‡ã€åº”åŠ›
            config: åœ°è´¨é…ç½®å‚æ•°
        
        Returns:
            æ–­å±‚æ»‘åŠ¨æ–¹ç¨‹æ®‹å·®
        """
        if not HAS_PYTORCH:
            return torch.tensor(0.0)
        
        # è§£æè¾“å‡ºï¼šæ»‘åŠ¨é€Ÿç‡ã€çŠ¶æ€å˜é‡ã€åº”åŠ›
        slip_rate = y[:, 0]  # æ»‘åŠ¨é€Ÿç‡
        state = y[:, 1]      # çŠ¶æ€å˜é‡ (æ‘©æ“¦çŠ¶æ€)
        stress = y[:, 2]     # åº”åŠ›
        
        # æ‘©æ“¦å‚æ•°ï¼ˆä»é…ç½®ä¸­è·å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        mu0 = getattr(config, 'mu0', 0.6)      # é™æ‘©æ“¦ç³»æ•°
        a = getattr(config, 'a', 0.01)         # ç›´æ¥æ•ˆåº”å‚æ•°
        b = getattr(config, 'b', 0.005)        # æ¼”åŒ–æ•ˆåº”å‚æ•°
        L = getattr(config, 'L', 0.1)          # ç‰¹å¾æ»‘ç§»è·ç¦»
        v0 = getattr(config, 'v0', 1e-6)      # å‚è€ƒæ»‘åŠ¨é€Ÿç‡
        
        # è®¡ç®—æ‘©æ“¦ç³»æ•°ï¼ˆé€Ÿç‡-çŠ¶æ€æ‘©æ“¦æœ¬æ„ï¼‰
        # Î¼ = Î¼â‚€ + a ln(v/vâ‚€) + b ln(Î¸/Î¸â‚€)
        theta0 = 1.0  # å‚è€ƒçŠ¶æ€å˜é‡
        mu = mu0 + a * torch.log(slip_rate / (v0 + 1e-12)) + b * torch.log(state / (theta0 + 1e-12))
        
        # çŠ¶æ€æ¼”åŒ–æ–¹ç¨‹ï¼ˆageing lawï¼‰
        # dÎ¸/dt = 1 - vÎ¸/L
        if x.shape[1] > 3:  # æœ‰æ—¶é—´ç»´åº¦
            time_derivative = torch.autograd.grad(
                state.sum(), x[:, 3], 
                grad_outputs=torch.ones_like(state), 
                create_graph=True, retain_graph=True
            )[0]
        else:
            # å¦‚æœæ²¡æœ‰æ—¶é—´ç»´åº¦ï¼Œä½¿ç”¨ç¨³æ€è¿‘ä¼¼
            time_derivative = torch.zeros_like(state)
        
        # çŠ¶æ€æ¼”åŒ–æ®‹å·®
        state_evolution_residual = time_derivative - (1.0 - slip_rate * state / L)
        
        # æ»‘åŠ¨é€Ÿç‡ä¸æ‘©æ“¦ç³»æ•°çš„å…³ç³»æ®‹å·®
        # Ï„ = Î¼Ïƒ (åº”åŠ› = æ‘©æ“¦ç³»æ•° Ã— æ­£åº”åŠ›)
        normal_stress = config.density * config.gravity * 1000.0  # ç®€åŒ–çš„æ­£åº”åŠ›è®¡ç®—
        stress_residual = stress - mu * normal_stress
        
        # æ€»æ®‹å·®
        total_residual = torch.mean(torch.square(state_evolution_residual)) + \
                        torch.mean(torch.square(stress_residual))
        
        return total_residual
    
    @staticmethod
    def mantle_convection_equation(x: torch.Tensor, y: torch.Tensor, config: GeologicalConfig) -> torch.Tensor:
        """
        åœ°å¹”å¯¹æµæ–¹ç¨‹ï¼šç»“åˆStokesæ–¹ç¨‹å’Œçƒ­ä¼ å¯¼æ–¹ç¨‹
        
        Args:
            x: è¾“å…¥åæ ‡ (x, y, z, t)
            y: æ¨¡å‹è¾“å‡º [v_x, v_y, v_z, p, T] é€Ÿåº¦åœºã€å‹åŠ›ã€æ¸©åº¦
            config: åœ°è´¨é…ç½®å‚æ•°
        
        Returns:
            åœ°å¹”å¯¹æµæ–¹ç¨‹æ®‹å·®
        """
        if not HAS_PYTORCH:
            return torch.tensor(0.0)
        
        # è§£æè¾“å‡º
        vx, vy, vz, p, T = y[:, 0], y[:, 1], y[:, 2], y[:, 3], y[:, 4]
        
        # 1. Stokesæ–¹ç¨‹æ®‹å·®ï¼ˆåŠ¨é‡å®ˆæ’ï¼‰
        stokes_residual = GeologicalPhysicsEquations.stokes_equation(x, y, config)
        
        # 2. è´¨é‡å®ˆæ’æ®‹å·®
        mass_residual = GeologicalPhysicsEquations.mass_conservation_equation(x, y[:, :3], config)
        
        # 3. çƒ­ä¼ å¯¼æ–¹ç¨‹æ®‹å·®ï¼ˆè€ƒè™‘å¯¹æµé¡¹ï¼‰
        # Ïc(âˆ‚T/âˆ‚t + vÂ·âˆ‡T) = âˆ‡Â·(kâˆ‡T) + Q
        if x.shape[1] > 3:  # æœ‰æ—¶é—´ç»´åº¦
            # æ—¶é—´å¯¼æ•°
            T_t = torch.autograd.grad(
                T.sum(), x[:, 3], 
                grad_outputs=torch.ones_like(T), 
                create_graph=True, retain_graph=True
            )[0]
        else:
            T_t = torch.zeros_like(T)
        
        # å¯¹æµé¡¹ vÂ·âˆ‡T
        T_x = torch.autograd.grad(T.sum(), x[:, 0], create_graph=True)[0]
        T_y = torch.autograd.grad(T.sum(), x[:, 1], create_graph=True)[0]
        T_z = torch.autograd.grad(T.sum(), x[:, 2], create_graph=True)[0]
        
        convection_term = vx * T_x + vy * T_y + vz * T_z
        
        # çƒ­ä¼ å¯¼é¡¹ âˆ‡Â·(kâˆ‡T)
        T_grad_x = torch.autograd.grad(T.sum(), x[:, 0], create_graph=True)[0]
        T_grad_y = torch.autograd.grad(T.sum(), x[:, 1], create_graph=True)[0]
        T_grad_z = torch.autograd.grad(T.sum(), x[:, 2], create_graph=True)[0]
        
        T_xx = torch.autograd.grad(T_grad_x.sum(), x[:, 0], create_graph=True)[0]
        T_yy = torch.autograd.grad(T_grad_y.sum(), x[:, 1], create_graph=True)[0]
        T_zz = torch.autograd.grad(T_grad_z.sum(), x[:, 2], create_graph=True)[0]
        
        conduction_term = config.thermal_conductivity * (T_xx + T_yy + T_zz)
        
        # çƒ­æºé¡¹ï¼ˆæ”¾å°„æ€§è¡°å˜ã€ç²˜æ€§è€—æ•£ç­‰ï¼‰
        viscous_heating = 0.0  # ç®€åŒ–ï¼Œå®é™…åº”è®¡ç®— Î·(âˆ‡v:âˆ‡v)
        heat_source = 0.01 + viscous_heating
        
        # çƒ­ä¼ å¯¼æ–¹ç¨‹æ®‹å·®
        heat_residual = config.density * config.specific_heat * (T_t + convection_term) - \
                       conduction_term - heat_source
        
        # 4. æµ®åŠ›é¡¹ï¼ˆBoussinesqè¿‘ä¼¼ï¼‰
        # å¯†åº¦å˜åŒ–ï¼šÏ = Ïâ‚€(1 - Î±(T - Tâ‚€))
        T_ref = 273.15
        density_variation = config.density * config.thermal_expansion * (T - T_ref)
        
        # æ€»æ®‹å·®ï¼ˆåŠ æƒç»„åˆï¼‰
        total_residual = (10.0 * stokes_residual + 
                          5.0 * mass_residual + 
                          2.0 * torch.mean(torch.square(heat_residual)) +
                          1.0 * torch.mean(torch.square(density_variation)))
        
        return total_residual
    
    @staticmethod
    def plate_tectonics_equation(x: torch.Tensor, y: torch.Tensor, config: GeologicalConfig) -> torch.Tensor:
        """
        æ¿å—æ„é€ æ–¹ç¨‹ï¼šç»“åˆå¼¹æ€§åŠ›å­¦å’Œçƒ­ä¼ å¯¼
        
        Args:
            x: è¾“å…¥åæ ‡ (x, y, z, t)
            y: æ¨¡å‹è¾“å‡º [u_x, u_y, u_z, T, stress] ä½ç§»åœºã€æ¸©åº¦ã€åº”åŠ›
            config: åœ°è´¨é…ç½®å‚æ•°
        
        Returns:
            æ¿å—æ„é€ æ–¹ç¨‹æ®‹å·®
        """
        if not HAS_PYTORCH:
            return torch.tensor(0.0)
        
        # è§£æè¾“å‡º
        ux, uy, uz, T, stress = y[:, 0], y[:, 1], y[:, 2], y[:, 3], y[:, 4]
        
        # 1. å¼¹æ€§å¹³è¡¡æ–¹ç¨‹æ®‹å·®
        elastic_residual = GeologicalPhysicsEquations.elastic_equilibrium_equation(x, y[:, :3], config)
        
        # 2. çƒ­ä¼ å¯¼æ–¹ç¨‹æ®‹å·®
        heat_residual = GeologicalPhysicsEquations.heat_conduction_equation(x, y[:, 3:4], config)
        
        # 3. çƒ­å¼¹æ€§è€¦åˆé¡¹
        # çƒ­åº”åŠ›ï¼šÏƒ_th = -EÎ±(T - Tâ‚€)/(1 - 2Î½)
        T_ref = 273.15
        thermal_stress = -config.youngs_modulus * config.thermal_expansion * (T - T_ref) / \
                        (1 - 2 * config.poissons_ratio)
        
        # çƒ­åº”åŠ›æ®‹å·®
        thermal_stress_residual = stress - thermal_stress
        
        # 4. æ¿å—è¾¹ç•Œæ¡ä»¶ï¼ˆç®€åŒ–ï¼‰
        # åœ¨æ¿å—è¾¹ç•Œå¤„ï¼Œä½ç§»æ¢¯åº¦åº”è¯¥è¾ƒå¤§
        u_grad_x = torch.autograd.grad(ux.sum(), x[:, 0], create_graph=True)[0]
        u_grad_y = torch.autograd.grad(uy.sum(), x[:, 1], create_graph=True)[0]
        u_grad_z = torch.autograd.grad(uz.sum(), x[:, 2], create_graph=True)[0]
        
        # ä½ç§»æ¢¯åº¦åº”è¯¥æ»¡è¶³ä¸€å®šçš„çº¦æŸï¼ˆæ¿å—è¾¹ç•Œç‰¹å¾ï¼‰
        boundary_constraint = torch.mean(torch.square(u_grad_x + u_grad_y + u_grad_z))
        
        # æ€»æ®‹å·®
        total_residual = (5.0 * elastic_residual + 
                          3.0 * heat_residual + 
                          2.0 * torch.mean(torch.square(thermal_stress_residual)) +
                          1.0 * boundary_constraint)
        
        return total_residual
    
    @staticmethod
    def chemical_transport_equation(x: torch.Tensor, y: torch.Tensor, config: GeologicalConfig) -> torch.Tensor:
        """
        åŒ–å­¦è¾“è¿æ–¹ç¨‹ï¼šè€ƒè™‘å¯¹æµ-æ‰©æ•£-ååº”
        
        Args:
            x: è¾“å…¥åæ ‡ (x, y, z, t)
            y: æ¨¡å‹è¾“å‡º [C, v_x, v_y, v_z, T] æµ“åº¦ã€é€Ÿåº¦åœºã€æ¸©åº¦
            config: åœ°è´¨é…ç½®å‚æ•°
        
        Returns:
            åŒ–å­¦è¾“è¿æ–¹ç¨‹æ®‹å·®
        """
        if not HAS_PYTORCH:
            return torch.tensor(0.0)
        
        # è§£æè¾“å‡º
        C, vx, vy, vz, T = y[:, 0], y[:, 1], y[:, 2], y[:, 3], y[:, 4]
        
        # æ‰©æ•£ç³»æ•°ï¼ˆæ¸©åº¦ä¾èµ–ï¼‰
        D0 = 1e-9  # å‚è€ƒæ‰©æ•£ç³»æ•°
        D = D0 * torch.exp(-config.activation_energy / (8.314 * (T + 273.15)))
        
        # å¯¹æµé¡¹ vÂ·âˆ‡C
        C_x = torch.autograd.grad(C.sum(), x[:, 0], create_graph=True)[0]
        C_y = torch.autograd.grad(C.sum(), x[:, 1], create_graph=True)[0]
        C_z = torch.autograd.grad(C.sum(), x[:, 2], create_graph=True)[0]
        
        convection_term = vx * C_x + vy * C_y + vz * C_z
        
        # æ‰©æ•£é¡¹ âˆ‡Â·(Dâˆ‡C)
        C_grad_x = torch.autograd.grad(C.sum(), x[:, 0], create_graph=True)[0]
        C_grad_y = torch.autograd.grad(C.sum(), x[:, 1], create_graph=True)[0]
        C_grad_z = torch.autograd.grad(C.sum(), x[:, 2], create_graph=True)[0]
        
        C_xx = torch.autograd.grad(C_grad_x.sum(), x[:, 0], create_graph=True)[0]
        C_yy = torch.autograd.grad(C_grad_y.sum(), x[:, 1], create_graph=True)[0]
        C_zz = torch.autograd.grad(C_grad_z.sum(), x[:, 2], create_graph=True)[0]
        
        diffusion_term = D * (C_xx + C_yy + C_zz)
        
        # ååº”é¡¹ï¼ˆç®€åŒ–çš„ä¸€çº§ååº”ï¼‰
        reaction_rate = 0.01  # ååº”é€Ÿç‡å¸¸æ•°
        reaction_term = reaction_rate * C
        
        # æ—¶é—´å¯¼æ•°
        if x.shape[1] > 3:  # æœ‰æ—¶é—´ç»´åº¦
            C_t = torch.autograd.grad(C.sum(), x[:, 3], create_graph=True)[0]
        else:
            C_t = torch.zeros_like(C)
        
        # åŒ–å­¦è¾“è¿æ–¹ç¨‹æ®‹å·®ï¼šâˆ‚C/âˆ‚t + vÂ·âˆ‡C = âˆ‡Â·(Dâˆ‡C) + R
        transport_residual = C_t + convection_term - diffusion_term - reaction_term
        
        return torch.mean(torch.square(transport_residual))


class GeologicalPINN(BaseSolver, nn.Module):
    """
    åœ°è´¨ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆGeological PINNï¼‰
    
    æ ¸å¿ƒæ€æƒ³ï¼šå°†åœ°è´¨ç‰©ç†æ–¹ç¨‹ï¼ˆå¦‚è¾¾è¥¿å®šå¾‹ã€çƒ­ä¼ å¯¼æ–¹ç¨‹ï¼‰ä½œä¸ºè½¯çº¦æŸåµŒå…¥ç¥ç»ç½‘ç»œï¼Œ
    å¼ºåˆ¶æ¨¡å‹è¾“å‡ºæ»¡è¶³åœ°è´¨ç‰©ç†è§„å¾‹ï¼Œå®ç°"å°æ•°æ®+å¼ºç‰©ç†"çš„åœ°è´¨å»ºæ¨¡
    """
    
    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int,
                 physics_equations: List[Callable] = None,
                 boundary_conditions: List[Callable] = None,
                 geological_config: GeologicalConfig = None):
        BaseSolver.__init__(self)
        nn.Module.__init__(self)
        
        if not HAS_PYTORCH:
            raise ImportError("éœ€è¦å®‰è£…PyTorchæ¥ä½¿ç”¨Geological PINN")
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.output_dim = output_dim
        self.physics_equations = physics_equations or []
        self.boundary_conditions = boundary_conditions or []
        self.geological_config = geological_config or GeologicalConfig()
        
        # æ„å»ºç½‘ç»œ - ä½¿ç”¨æ®‹å·®è¿æ¥å’Œæ‰¹å½’ä¸€åŒ–
        self.layers = nn.ModuleList()
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            self.layers.append(nn.Linear(prev_dim, hidden_dim))
            self.layers.append(nn.BatchNorm1d(hidden_dim))
            self.layers.append(nn.Tanh())  # PINNé€šå¸¸ä½¿ç”¨Tanhæ¿€æ´»å‡½æ•°
            prev_dim = hidden_dim
        
        self.output_layer = nn.Linear(prev_dim, output_dim)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
        self.optimizer = None
        self.to(self.device)
        
        print(f"ğŸ”„ åœ°è´¨PINNåˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}")
        print(f"   ç½‘ç»œç»“æ„: {input_dim} -> {hidden_dims} -> {output_dim}")
        print(f"   ç‰©ç†æ–¹ç¨‹æ•°é‡: {len(self.physics_equations)}")
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for layer in self.layers:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_normal_(layer.weight)
                nn.init.constant_(layer.bias, 0)
        nn.init.xavier_normal_(self.output_layer.weight)
        nn.init.constant_(self.output_layer.bias, 0)
    
    def forward(self, x: torch.Tensor, edge_index: torch.Tensor = None, 
                edge_weight: torch.Tensor = None, mesh_data: np.ndarray = None,
                faults: List[Tuple] = None, plate_boundaries: List[Tuple] = None,
                geological_features: np.ndarray = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ - æ”¯æŒGNNå¢å¼º
        
        Args:
            x: è¾“å…¥ç‰¹å¾
            edge_index: å›¾è¾¹ç´¢å¼•ï¼ˆGNNç”¨ï¼‰
            edge_weight: å›¾è¾¹æƒé‡ï¼ˆGNNç”¨ï¼‰
            mesh_data: ç½‘æ ¼æ•°æ®ï¼ˆGNNå›¾æ„å»ºç”¨ï¼‰
            faults: æ–­å±‚ä¿¡æ¯ï¼ˆGNNå›¾æ„å»ºç”¨ï¼‰
            plate_boundaries: æ¿å—è¾¹ç•Œä¿¡æ¯ï¼ˆGNNå›¾æ„å»ºç”¨ï¼‰
            geological_features: åœ°è´¨ç‰¹å¾ï¼ˆGNNå›¾æ„å»ºç”¨ï¼‰
        
        Returns:
            æ¨¡å‹è¾“å‡º
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨GNNå¢å¼º
        if hasattr(self, 'gnn_integrator') and self.gnn_integrator is not None:
            if mesh_data is not None:
                # ä½¿ç”¨GNNå¢å¼ºç‰¹å¾
                enhanced_x = self.gnn_integrator.integrate_with_pinn(
                    x, mesh_data, faults, plate_boundaries, geological_features
                )
                # æ›´æ–°è¾“å…¥ç‰¹å¾
                x = enhanced_x
                # åŠ¨æ€è°ƒæ•´ç½‘ç»œè¾“å…¥ç»´åº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if x.shape[1] != self.input_dim:
                    self._adjust_input_dim(x.shape[1])
        
        # åŸæœ‰çš„å‰å‘ä¼ æ’­é€»è¾‘
        if self.use_gpu and torch.cuda.is_available():
            x = x.cuda()
        
        # å‰å‘ä¼ æ’­
        for i, layer in enumerate(self.layers):
            x = layer(x)
            if i < len(self.layers) - 1:  # ä¸æ˜¯æœ€åä¸€å±‚
                x = F.relu(x)
                if self.dropout_rate > 0:
                    x = F.dropout(x, p=self.dropout_rate, training=self.training)
        
        return x
    
    def setup_training(self, learning_rate: float = 0.001, weight_decay: float = 1e-5):
        """è®¾ç½®è®­ç»ƒå‚æ•°"""
        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=weight_decay)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=20, verbose=True
        )
    
    def setup_gnn_integration(self, gnn_config: Dict = None):
        """
        è®¾ç½®GNNé›†æˆ
        
        Args:
            gnn_config: GNNé…ç½®å‚æ•°
        """
        try:
            from .geodynamics_gnn import GeodynamicGNN, GeodynamicGraphConfig, GeodynamicsGNNPINNIntegrator
            
            # åˆ›å»ºGNNé…ç½®
            if gnn_config is None:
                gnn_config = {
                    'hidden_dim': 64,
                    'num_layers': 3,
                    'attention_heads': 4,
                    'dropout': 0.1
                }
            
            config = GeodynamicGraphConfig(**gnn_config)
            
            # åˆ›å»ºGNNæ¨¡å‹
            gnn_input_dim = 8  # åŸºç¡€ç‰¹å¾ç»´åº¦
            gnn_output_dim = 2  # ç²˜åº¦ä¿®æ­£ã€å¡‘æ€§åº”å˜ç‡
            gnn = GeodynamicGNN(gnn_input_dim, config.hidden_dim, gnn_output_dim, config)
            
            # åˆ›å»ºé›†æˆå™¨
            self.gnn_integrator = GeodynamicsGNNPINNIntegrator(gnn, config)
            
            print(f"âœ… GNNé›†æˆè®¾ç½®å®Œæˆ: éšè—å±‚={config.hidden_dim}, å±‚æ•°={config.num_layers}")
            
        except ImportError as e:
            warnings.warn(f"æ— æ³•å¯¼å…¥GNNæ¨¡å—: {str(e)}")
            self.gnn_integrator = None
        except Exception as e:
            warnings.warn(f"GNNé›†æˆè®¾ç½®å¤±è´¥: {str(e)}")
            self.gnn_integrator = None
    
    def enable_gnn_enhancement(self, enable: bool = True):
        """å¯ç”¨/ç¦ç”¨GNNå¢å¼º"""
        if enable and not hasattr(self, 'gnn_integrator'):
            self.setup_gnn_integration()
        
        if hasattr(self, 'gnn_integrator'):
            if enable:
                print("âœ… GNNå¢å¼ºå·²å¯ç”¨")
            else:
                print("âŒ GNNå¢å¼ºå·²ç¦ç”¨")
                self.gnn_integrator = None
        else:
            print("âŒ GNNé›†æˆå™¨æœªè®¾ç½®")
    
    def get_gnn_status(self) -> Dict[str, Any]:
        """è·å–GNNé›†æˆçŠ¶æ€"""
        status = {
            'gnn_enabled': False,
            'gnn_integrator': None,
            'gnn_config': None
        }
        
        if hasattr(self, 'gnn_integrator') and self.gnn_integrator is not None:
            status['gnn_enabled'] = True
            status['gnn_integrator'] = type(self.gnn_integrator).__name__
            if hasattr(self.gnn_integrator, 'config'):
                status['gnn_config'] = {
                    'hidden_dim': self.gnn_integrator.config.hidden_dim,
                    'num_layers': self.gnn_integrator.config.num_layers,
                    'attention_heads': self.gnn_integrator.config.attention_heads
                }
        
        return status
    
    def compute_physics_loss(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—åœ°è´¨ç‰©ç†çº¦æŸæŸå¤± - æ”¯æŒåœ°çƒåŠ¨åŠ›å­¦å¤šåœºè€¦åˆé€‚é…"""
        if not self.physics_equations:
            return torch.tensor(0.0, device=self.device)
        
        total_loss = torch.tensor(0.0, device=self.device)
        
        # é’ˆå¯¹ä¸åŒç‰©ç†åœºåˆ†é…æƒé‡
        for equation in self.physics_equations:
            # è®¡ç®—ç‰©ç†æ–¹ç¨‹çš„æ®‹å·®
            residual = equation(x, y, self.geological_config)
            
            # æ ¹æ®æ–¹ç¨‹ç±»å‹åˆ†é…æƒé‡
            equation_name = equation.__name__ if hasattr(equation, '__name__') else str(equation)
            
            if "stokes_equation" in equation_name:
                # åœ°å¹”æµåŠ¨æƒé‡æ›´é«˜ï¼ˆæ ¸å¿ƒè¿‡ç¨‹ï¼‰
                weight = 100.0
            elif "mantle_convection_equation" in equation_name:
                # åœ°å¹”å¯¹æµï¼ˆç»¼åˆæ–¹ç¨‹ï¼‰
                weight = 80.0
            elif "fault_slip_equation" in equation_name:
                # æ–­å±‚è¿‡ç¨‹æƒé‡
                weight = 50.0
            elif "plate_tectonics_equation" in equation_name:
                # æ¿å—æ„é€ ï¼ˆç»¼åˆæ–¹ç¨‹ï¼‰
                weight = 60.0
            elif "heat_conduction_equation" in equation_name:
                # çƒ­ä¼ å¯¼æ¬¡ä¹‹
                weight = 10.0
            elif "elastic_equilibrium_equation" in equation_name:
                # å¼¹æ€§åŠ›å­¦
                weight = 20.0
            elif "chemical_transport_equation" in equation_name:
                # åŒ–å­¦è¾“è¿
                weight = 15.0
            elif "darcy_equation" in equation_name:
                # è¾¾è¥¿æµåŠ¨
                weight = 8.0
            else:
                # å…¶ä»–æ–¹ç¨‹é»˜è®¤æƒé‡
                weight = 1.0
            
            # åº”ç”¨æƒé‡å¹¶ç´¯åŠ åˆ°æ€»æŸå¤±
            weighted_loss = weight * torch.mean(residual ** 2)
            total_loss += weighted_loss
            
            # è®°å½•å„æ–¹ç¨‹çš„æŸå¤±ï¼ˆç”¨äºç›‘æ§ï¼‰
            if not hasattr(self, 'equation_losses'):
                self.equation_losses = {}
            self.equation_losses[equation_name] = weighted_loss.item()
        
        return total_loss
    
    def compute_boundary_loss(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—è¾¹ç•Œæ¡ä»¶æŸå¤±"""
        if not self.boundary_conditions:
            return torch.tensor(0.0, device=self.device)
        
        total_loss = torch.tensor(0.0, device=self.device)
        
        for bc in self.boundary_conditions:
            # è®¡ç®—è¾¹ç•Œæ¡ä»¶çš„æ®‹å·®
            residual = bc(x, y)
            total_loss += torch.mean(residual ** 2)
        
        return total_loss
    
    def train(self, X: np.ndarray, y: np.ndarray, 
              physics_points: np.ndarray = None,
              boundary_points: np.ndarray = None,
              geological_features: np.ndarray = None,
              epochs: int = 1000, 
              physics_weight: float = 1.0,
              boundary_weight: float = 1.0,
              batch_size: int = 32,
              validation_split: float = 0.2) -> dict:
        """è®­ç»ƒåœ°è´¨PINNæ¨¡å‹"""
        if self.optimizer is None:
            self.setup_training()
        
        # éªŒè¯è¾“å…¥æ•°æ®
        if X.shape[0] != y.shape[0]:
            raise ValueError(f"Xå’Œyæ ·æœ¬æ•°ä¸åŒ¹é…ï¼š{X.shape[0]} vs {y.shape[0]}")
        
        # åˆå¹¶åœ°è´¨ç‰¹å¾ï¼ˆå¦‚æœæä¾›ï¼‰
        if geological_features is not None:
            if geological_features.shape[0] != X.shape[0]:
                raise ValueError(f"åœ°è´¨ç‰¹å¾æ ·æœ¬æ•°ä¸åŒ¹é…ï¼š{geological_features.shape[0]} vs {X.shape[0]}")
            X = np.hstack([X, geological_features])
            print(f"   åˆå¹¶åœ°è´¨ç‰¹å¾ï¼Œè¾“å…¥ç»´åº¦: {X.shape[1]}")
            
            # åŠ¨æ€è°ƒæ•´ç½‘ç»œè¾“å…¥ç»´åº¦
            if X.shape[1] != self.input_dim:
                print(f"   åŠ¨æ€è°ƒæ•´ç½‘ç»œè¾“å…¥ç»´åº¦: {self.input_dim} -> {X.shape[1]}")
                self._adjust_input_dim(X.shape[1])
        
        # æ•°æ®åˆ†å‰²
        if validation_split > 0:
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=validation_split, random_state=42
            )
        else:
            X_train, y_train = X, y
            X_val, y_val = None, None
        
        # è½¬æ¢ä¸ºå¼ é‡
        X_train_tensor = torch.FloatTensor(X_train).to(self.device)
        y_train_tensor = torch.FloatTensor(y_train).to(self.device)
        
        if X_val is not None:
            X_val_tensor = torch.FloatTensor(X_val).to(self.device)
            y_val_tensor = torch.FloatTensor(y_val).to(self.device)
        
        if physics_points is not None:
            physics_tensor = torch.FloatTensor(physics_points).to(self.device)
        
        if boundary_points is not None:
            boundary_tensor = torch.FloatTensor(boundary_points).to(self.device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        history = {
            'total_loss': [], 'data_loss': [], 'physics_loss': [], 
            'boundary_loss': [], 'val_loss': [], 'train_time': 0.0
        }
        start_time = time.time()
        
        best_val_loss = float('inf')
        patience = 50
        patience_counter = 0
        
        for epoch in range(epochs):
            nn.Module.train(self, True)  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            epoch_losses = {'total': 0, 'data': 0, 'physics': 0, 'boundary': 0}
            
            for batch_X, batch_y in train_loader:
                self.optimizer.zero_grad()
                
                # æ•°æ®æŸå¤±
                outputs = self(batch_X)
                data_loss = F.mse_loss(outputs, batch_y)
                
                # ç‰©ç†æŸå¤±
                if physics_points is not None and self.physics_equations:
                    physics_outputs = self(physics_tensor)
                    physics_loss = self.compute_physics_loss(physics_tensor, physics_outputs)
                else:
                    physics_loss = torch.tensor(0.0, device=self.device)
                
                # è¾¹ç•ŒæŸå¤±
                if boundary_points is not None and self.boundary_conditions:
                    boundary_outputs = self(boundary_tensor)
                    boundary_loss = self.compute_boundary_loss(boundary_tensor, boundary_outputs)
                else:
                    boundary_loss = torch.tensor(0.0, device=self.device)
                
                # æ€»æŸå¤±
                total_loss = data_loss + physics_weight * physics_loss + boundary_weight * boundary_loss
                
                total_loss.backward()
                self.optimizer.step()
                
                # ç´¯ç§¯æŸå¤±
                epoch_losses['total'] += total_loss.item()
                epoch_losses['data'] += data_loss.item()
                epoch_losses['physics'] += physics_loss.item()
                epoch_losses['boundary'] += boundary_loss.item()
            
            # è®¡ç®—å¹³å‡æŸå¤±
            num_batches = len(train_loader)
            for key in epoch_losses:
                epoch_losses[key] /= num_batches
            
            # éªŒè¯æŸå¤±
            val_loss = None
            if X_val is not None:
                nn.Module.eval(self)
                with torch.no_grad():
                    val_outputs = self(X_val_tensor)
                    val_loss = F.mse_loss(val_outputs, y_val_tensor).item()
                    history['val_loss'].append(val_loss)
                    
                    # æ—©åœ
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        patience_counter = 0
                    else:
                        patience_counter += 1
                    
                    if patience_counter >= patience:
                        print(f"   æ—©åœåœ¨ç¬¬ {epoch+1} è½®")
                        break
            
            # è®°å½•å†å²
            history['total_loss'].append(epoch_losses['total'])
            history['data_loss'].append(epoch_losses['data'])
            history['physics_loss'].append(epoch_losses['physics'])
            history['boundary_loss'].append(epoch_losses['boundary'])
            
            if (epoch + 1) % 100 == 0:
                val_info = f", val_loss={val_loss:.6f}" if val_loss is not None else ""
                print(f"   Epoch {epoch+1}/{epochs}: total_loss={epoch_losses['total']:.6f}, "
                      f"data_loss={epoch_losses['data']:.6f}, physics_loss={epoch_losses['physics']:.6f}, "
                      f"boundary_loss={epoch_losses['boundary']:.6f}{val_info}")
        
        history['train_time'] = time.time() - start_time
        self.is_trained = True
        return history
    
    def _adjust_input_dim(self, new_input_dim: int):
        """åŠ¨æ€è°ƒæ•´ç½‘ç»œè¾“å…¥ç»´åº¦"""
        if new_input_dim != self.input_dim:
            # é‡æ–°æ„å»ºç¬¬ä¸€å±‚
            old_first_layer = self.layers[0]
            new_first_layer = nn.Linear(new_input_dim, old_first_layer.out_features)
            
            # åˆå§‹åŒ–æ–°å±‚æƒé‡
            nn.init.xavier_normal_(new_first_layer.weight)
            nn.init.constant_(new_first_layer.bias, 0)
            
            # æ›¿æ¢ç¬¬ä¸€å±‚
            self.layers[0] = new_first_layer
            self.input_dim = new_input_dim
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            self.to(self.device)
            
            print(f"   ç½‘ç»œè¾“å…¥ç»´åº¦å·²è°ƒæ•´: {new_input_dim}")
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        self.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)
            outputs = self(X_tensor)
            return outputs.cpu().numpy()


class GeologicalSurrogateModel(BaseSolver):
    """
    åœ°è´¨ä»£ç†æ¨¡å‹ - ä¸“é—¨ç”¨äºåœ°è´¨æ•°å€¼æ¨¡æ‹ŸåŠ é€Ÿ
    
    æ ¸å¿ƒæ€æƒ³ï¼šç”¨ä¼ ç»Ÿåœ°è´¨æ•°å€¼æ¨¡æ‹Ÿç”Ÿæˆ"åœ°è´¨å‚æ•°â†’æ¨¡æ‹Ÿè¾“å‡º"çš„æ•°æ®é›†ï¼Œ
    è®­ç»ƒMLæ¨¡å‹å­¦ä¹ è¿™ç§æ˜ å°„ï¼Œåç»­ç”¨æ¨¡å‹ç›´æ¥é¢„æµ‹ï¼Œæ›¿ä»£å®Œæ•´æ¨¡æ‹Ÿæµç¨‹
    
    æ‰©å±•åŠŸèƒ½ï¼š
    1. æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹ï¼šé«˜æ–¯è¿‡ç¨‹ã€éšæœºæ£®æ—ã€æ¢¯åº¦æå‡ã€XGBoostã€LightGBMç­‰
    2. äº¤å‰éªŒè¯å’Œæ¨¡å‹è¯„ä¼°
    3. ç‰¹å¾é‡è¦æ€§åˆ†æ
    4. æ‰¹é‡é¢„æµ‹ä¼˜åŒ–
    5. ä¸ç¡®å®šæ€§ä¼°è®¡
    6. æ¨¡å‹æŒä¹…åŒ–
    """
    
    def __init__(self, model_type: str = 'gaussian_process', geological_config: GeologicalConfig = None):
        super().__init__()
        self.model_type = model_type
        self.model = None
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.geological_config = geological_config or GeologicalConfig()
        self.training_history = {}
        self.feature_importance = None
        self.cv_scores = None
        
        # æ£€æŸ¥ä¾èµ–
        if model_type in ['gaussian_process', 'random_forest', 'gradient_boosting', 'mlp'] and not HAS_SKLEARN:
            raise ImportError("éœ€è¦å®‰è£…scikit-learnæ¥ä½¿ç”¨è¯¥ä»£ç†æ¨¡å‹")
        
        # æ£€æŸ¥XGBoostå’ŒLightGBMä¾èµ–
        if model_type in ['xgboost', 'lightgbm']:
            try:
                if model_type == 'xgboost':
                    import xgboost as xgb
                    self.xgb = xgb
                elif model_type == 'lightgbm':
                    import lightgbm as lgb
                    self.lgb = lgb
            except ImportError:
                raise ImportError(f"éœ€è¦å®‰è£…{model_type}æ¥ä½¿ç”¨è¯¥ä»£ç†æ¨¡å‹")
    
    def train(self, X: np.ndarray, y: np.ndarray, geological_features: np.ndarray = None, 
              cv: int = 0, **kwargs) -> dict:
        """
        è®­ç»ƒåœ°è´¨ä»£ç†æ¨¡å‹ï¼ˆæ”¯æŒäº¤å‰éªŒè¯å’Œé«˜ç»´è¾“å‡ºï¼‰
        
        Args:
            X: è¾“å…¥ç‰¹å¾ (n_samples, n_params)ï¼Œå¦‚ [é»åº¦ç³»æ•°, çƒ­å¯¼ç‡, è¾¹ç•Œæ¸©åº¦]
            y: è¾“å‡ºç‰©ç†åœº (n_samples, H, W, D) æˆ– (n_samples, n_outputs)ï¼Œå¦‚ä¸‰ç»´æ¸©åº¦åœº
            geological_features: åœ°è´¨ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            cv: äº¤å‰éªŒè¯æŠ˜æ•°ï¼ˆ0è¡¨ç¤ºä¸è¿›è¡Œäº¤å‰éªŒè¯ï¼‰
            **kwargs: æ¨¡å‹ç‰¹å®šå‚æ•°
        """
        # éªŒè¯è¾“å…¥æ•°æ®åˆæ³•æ€§
        if X.shape[0] != y.shape[0]:
            raise ValueError(f"Xå’Œyæ ·æœ¬æ•°ä¸åŒ¹é…ï¼š{X.shape[0]} vs {y.shape[0]}")
        
        start_time = time.time()
        
        # å¤„ç†é«˜ç»´è¾“å‡ºï¼ˆå¦‚3Dåœºæ•°æ®ï¼‰
        if y.ndim > 2:
            self.y_shape = y.shape[1:]  # è®°å½•åŸå§‹å½¢çŠ¶ (H, W, D)
            y_reshaped = y.reshape(y.shape[0], -1)  # å±•å¹³ä¸º (n_samples, H*W*D)
            print(f"   æ£€æµ‹åˆ°é«˜ç»´è¾“å‡ºï¼ŒåŸå§‹å½¢çŠ¶: {y.shape}ï¼Œå±•å¹³å: {y_reshaped.shape}")
        else:
            self.y_shape = None
            y_reshaped = y
        
        # åˆå¹¶åœ°è´¨ç‰¹å¾ï¼ˆå¦‚æœæä¾›ï¼‰
        if geological_features is not None:
            if geological_features.shape[0] != X.shape[0]:
                raise ValueError(f"åœ°è´¨ç‰¹å¾æ ·æœ¬æ•°ä¸åŒ¹é…ï¼š{geological_features.shape[0]} vs {X.shape[0]}")
            X = np.hstack([X, geological_features])
            print(f"   åˆå¹¶åœ°è´¨ç‰¹å¾ï¼Œè¾“å…¥ç»´åº¦: {X.shape[1]}")
        
        # æ•°æ®æ ‡å‡†åŒ–
        X_scaled = self.scaler_X.fit_transform(X)
        y_scaled = self.scaler_y.fit_transform(y_reshaped.reshape(-1, 1)).flatten()
        
        # æ ¹æ®æ¨¡å‹ç±»å‹åˆå§‹åŒ–å¹¶è®­ç»ƒ
        if self.model_type == 'gaussian_process':
            kernel = kwargs.get('kernel', RBF(length_scale=1.0) + ConstantKernel())
            self.model = GaussianProcessRegressor(kernel=kernel, random_state=42)
            self.model.fit(X_scaled, y_scaled)
        
        elif self.model_type == 'random_forest':
            n_estimators = kwargs.get('n_estimators', 100)
            max_depth = kwargs.get('max_depth', None)
            min_samples_split = kwargs.get('min_samples_split', 2)
            min_samples_leaf = kwargs.get('min_samples_leaf', 1)
            
            if not isinstance(n_estimators, int) or n_estimators <= 0:
                raise ValueError(f"n_estimatorså¿…é¡»ä¸ºæ­£æ•´æ•°ï¼Œå®é™…ä¸ºï¼š{n_estimators}")
            
            self.model = RandomForestRegressor(
                n_estimators=n_estimators, 
                max_depth=max_depth,
                min_samples_split=min_samples_split,
                min_samples_leaf=min_samples_leaf,
                random_state=42
            )
            self.model.fit(X_scaled, y_scaled)
            
            # è®¡ç®—ç‰¹å¾é‡è¦æ€§
            self.feature_importance = self.model.feature_importances_
        
        elif self.model_type == 'gradient_boosting':
            n_estimators = kwargs.get('n_estimators', 100)
            learning_rate = kwargs.get('learning_rate', 0.1)
            max_depth = kwargs.get('max_depth', 3)
            
            if not isinstance(n_estimators, int) or n_estimators <= 0:
                raise ValueError(f"n_estimatorså¿…é¡»ä¸ºæ­£æ•´æ•°ï¼Œå®é™…ä¸ºï¼š{n_estimators}")
            
            if not isinstance(learning_rate, (int, float)) or learning_rate <= 0:
                raise ValueError(f"learning_rateå¿…é¡»ä¸ºæ­£æ•°ï¼Œå®é™…ä¸ºï¼š{learning_rate}")
            
            self.model = GradientBoostingRegressor(
                n_estimators=n_estimators, 
                learning_rate=learning_rate,
                max_depth=max_depth,
                random_state=42
            )
            self.model.fit(X_scaled, y_scaled)
            
            # è®¡ç®—ç‰¹å¾é‡è¦æ€§
            self.feature_importance = self.model.feature_importances_
        
        elif self.model_type == 'mlp':
            hidden_layer_sizes = kwargs.get('hidden_layer_sizes', (100, 50))
            max_iter = kwargs.get('max_iter', 1000)
            learning_rate_init = kwargs.get('learning_rate_init', 0.001)
            
            self.model = MLPRegressor(
                hidden_layer_sizes=hidden_layer_sizes,
                max_iter=max_iter,
                learning_rate_init=learning_rate_init,
                random_state=42
            )
            self.model.fit(X_scaled, y_scaled)
        
        elif self.model_type == 'xgboost':
            n_estimators = kwargs.get('n_estimators', 100)
            max_depth = kwargs.get('max_depth', 3)
            learning_rate = kwargs.get('learning_rate', 0.1)
            
            self.model = self.xgb.XGBRegressor(
                n_estimators=n_estimators,
                max_depth=max_depth,
                learning_rate=learning_rate,
                random_state=42
            )
            self.model.fit(X_scaled, y_scaled)
            
            # è®¡ç®—ç‰¹å¾é‡è¦æ€§
            self.feature_importance = self.model.feature_importances_
        
        elif self.model_type == 'lightgbm':
            n_estimators = kwargs.get('n_estimators', 100)
            max_depth = kwargs.get('max_depth', 3)
            learning_rate = kwargs.get('learning_rate', 0.1)
            
            self.model = self.lgb.LGBMRegressor(
                n_estimators=n_estimators,
                max_depth=max_depth,
                learning_rate=learning_rate,
                random_state=42
            )
            self.model.fit(X_scaled, y_scaled)
            
            # è®¡ç®—ç‰¹å¾é‡è¦æ€§
            self.feature_importance = self.model.feature_importances_
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
        
        # äº¤å‰éªŒè¯ï¼ˆå¯é€‰ï¼‰
        if cv > 1:
            from sklearn.model_selection import cross_val_score
            self.cv_scores = cross_val_score(
                self.model, X_scaled, y_scaled, cv=cv, scoring='neg_mean_squared_error'
            )
            self.cv_scores = -self.cv_scores  # è½¬æ¢ä¸ºMSE
            print(f"   äº¤å‰éªŒè¯MSE: {self.cv_scores.mean():.6f} Â± {self.cv_scores.std():.6f}")
        
        self.is_trained = True
        training_time = time.time() - start_time
        
        self.training_history = {
            'training_time': training_time,
            'model_type': self.model_type,
            'n_samples': len(X),
            'n_features': X.shape[1],
            'geological_config': self.geological_config,
            'cv_scores': self.cv_scores,
            'feature_importance': self.feature_importance
        }
        
        return self.training_history
    
    def predict(self, X: np.ndarray, return_std: bool = False, batch_size: int = None) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """
        é¢„æµ‹ - æ”¯æŒæ‰¹é‡å¤„ç†ã€ä¸ç¡®å®šæ€§ä¼°è®¡å’Œé«˜ç»´è¾“å‡ºæ¢å¤
        
        Args:
            X: è¾“å…¥ç‰¹å¾
            return_std: æ˜¯å¦è¿”å›æ ‡å‡†å·®
            batch_size: æ‰¹é‡å¤§å°ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©ï¼‰
        """
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        # è‡ªåŠ¨é€‰æ‹©æ‰¹é‡å¤§å°
        if batch_size is None:
            batch_size = 1024 if self.geological_config.use_gpu else len(X)
        
        # æ‰¹é‡å¤„ç†å¤§å°ºå¯¸åœ°è´¨æ•°æ®
        n_batches = (len(X) + batch_size - 1) // batch_size
        predictions = []
        stds = [] if return_std else None
        
        for i in range(n_batches):
            X_batch = X[i*batch_size : (i+1)*batch_size]
            X_scaled = self.scaler_X.transform(X_batch)
            
            # GPUåŠ é€Ÿï¼ˆå¦‚æœæ”¯æŒä¸”å¯ç”¨ï¼‰
            if (HAS_PYTORCH and isinstance(self.model, torch.nn.Module) 
                and self.geological_config.use_gpu and torch.cuda.is_available()):
                X_scaled = torch.tensor(X_scaled, device=self.device)
                with torch.no_grad():
                    y_pred_scaled = self.model(X_scaled).cpu().detach().numpy()
            else:
                y_pred_scaled = self.model.predict(X_scaled)
            
            y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
            predictions.append(y_pred)
            
            # å¤„ç†ä¸ç¡®å®šæ€§ä¼°è®¡
            if return_std:
                if self.model_type == 'gaussian_process':
                    _, std_scaled = self.model.predict(X_scaled, return_std=True)
                    std = self.scaler_y.inverse_transform(std_scaled.reshape(-1, 1)).flatten()
                elif self.model_type == 'random_forest':
                    # éšæœºæ£®æ—ï¼šé€šè¿‡æ ‘çš„é¢„æµ‹å·®å¼‚ä¼°è®¡æ ‡å‡†å·®
                    tree_preds = np.array([tree.predict(X_scaled) for tree in self.model.estimators_])
                    std_scaled = np.std(tree_preds, axis=0)
                    std = self.scaler_y.inverse_transform(std_scaled.reshape(-1, 1)).flatten()
                elif self.model_type in ['xgboost', 'lightgbm']:
                    # XGBoost/LightGBMï¼šé€šè¿‡å¤šæ¬¡é¢„æµ‹ä¼°è®¡ä¸ç¡®å®šæ€§
                    n_estimators = len(self.model.estimators_) if hasattr(self.model, 'estimators_') else 10
                    tree_preds = []
                    for _ in range(min(n_estimators, 10)):  # é™åˆ¶é¢„æµ‹æ¬¡æ•°
                        pred = self.model.predict(X_scaled)
                        tree_preds.append(pred)
                    std_scaled = np.std(tree_preds, axis=0)
                    std = self.scaler_y.inverse_transform(std_scaled.reshape(-1, 1)).flatten()
                else:
                    std = np.zeros_like(y_pred)
                stds.append(std)
        
        predictions = np.concatenate(predictions)
        
        # æ¢å¤é«˜ç»´è¾“å‡ºå½¢çŠ¶ï¼ˆå¦‚æœè®­ç»ƒæ—¶æ˜¯é«˜ç»´æ•°æ®ï¼‰
        if self.y_shape is not None and not return_std:
            predictions = predictions.reshape(-1, *self.y_shape)
            print(f"   æ¢å¤é«˜ç»´è¾“å‡ºå½¢çŠ¶: {predictions.shape}")
        
        if return_std:
            stds = np.concatenate(stds)
            # å¯¹äºé«˜ç»´è¾“å‡ºï¼Œæ ‡å‡†å·®ä¹Ÿéœ€è¦æ¢å¤å½¢çŠ¶
            if self.y_shape is not None:
                stds = stds.reshape(-1, *self.y_shape)
            return predictions, stds
        
        return predictions
    
    def get_feature_importance(self) -> np.ndarray:
        """è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆä»…æ”¯æŒæ ‘æ¨¡å‹ï¼‰"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        if self.model_type in ['random_forest', 'gradient_boosting', 'xgboost', 'lightgbm']:
            if self.feature_importance is not None:
                return self.feature_importance
            else:
                return self.model.feature_importances_
        else:
            raise NotImplementedError(f"{self.model_type}ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
    
    def get_model_performance(self) -> dict:
        """è·å–æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        performance = {
            'model_type': self.model_type,
            'training_time': self.training_history.get('training_time', 0),
            'n_samples': self.training_history.get('n_samples', 0),
            'n_features': self.training_history.get('n_features', 0)
        }
        
        if self.cv_scores is not None:
            performance.update({
                'cv_mean_mse': self.cv_scores.mean(),
                'cv_std_mse': self.cv_scores.std(),
                'cv_scores': self.cv_scores.tolist()
            })
        
        if self.feature_importance is not None:
            performance['feature_importance'] = self.feature_importance.tolist()
        
        return performance
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹ï¼ˆåŒ…å«scalerå’Œæ¨¡å‹å‚æ•°ï¼‰"""
        try:
            import pickle
            model_data = {
                'model_type': self.model_type,
                'model': self.model,
                'scaler_X': self.scaler_X,
                'scaler_y': self.scaler_y,
                'is_trained': self.is_trained,
                'training_history': self.training_history,
                'feature_importance': self.feature_importance,
                'cv_scores': self.cv_scores,
                'geological_config': self.geological_config
            }
            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)
            print(f"ğŸ“ åœ°è´¨ä»£ç†æ¨¡å‹å·²ä¿å­˜: {filepath}")
        except Exception as e:
            raise RuntimeError(f"ä¿å­˜æ¨¡å‹å¤±è´¥ï¼š{str(e)}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        try:
            import pickle
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            self.model_type = model_data['model_type']
            self.model = model_data['model']
            self.scaler_X = model_data['scaler_X']
            self.scaler_y = model_data['scaler_y']
            self.is_trained = model_data['is_trained']
            self.training_history = model_data.get('training_history', {})
            self.feature_importance = model_data.get('feature_importance', None)
            self.cv_scores = model_data.get('cv_scores', None)
            self.geological_config = model_data.get('geological_config', GeologicalConfig())
            
            print(f"ğŸ“ åœ°è´¨ä»£ç†æ¨¡å‹å·²åŠ è½½: {filepath}")
        except FileNotFoundError:
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼š{filepath}")
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æ¨¡å‹å¤±è´¥ï¼š{str(e)}")
    
    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        info = {
            'model_type': self.model_type,
            'is_trained': self.is_trained,
            'geological_config': self.geological_config
        }
        
        if self.is_trained:
            info.update({
                'n_samples': self.training_history.get('n_samples', 0),
                'n_features': self.training_history.get('n_features', 0),
                'training_time': self.training_history.get('training_time', 0)
            })
        
        return info


class GeologicalUNet(BaseSolver, nn.Module):
    """
    åœ°è´¨UNet - ä¸“é—¨ç”¨äºå¤„ç†åœ°è´¨ç©ºé—´åœºæ•°æ®
    
    æ ¸å¿ƒæ€æƒ³ï¼šç”¨UNetå®ç°"ä½ç²¾åº¦åœ°è´¨æ•°æ®â†’é«˜ç²¾åº¦åœ°è´¨åœº"çš„ç«¯åˆ°ç«¯æ˜ å°„ï¼Œ
    å¦‚åœ°éœ‡æ•°æ®åæ¼”åœ°è´¨ç»“æ„ã€åœ°è´¨åœºè¶…åˆ†è¾¨ç‡é‡å»ºç­‰
    """
    
    def __init__(self, input_channels: int = 1, output_channels: int = 1, 
                 initial_features: int = 64, depth: int = 4):
        BaseSolver.__init__(self)
        nn.Module.__init__(self)
        
        if not HAS_PYTORCH:
            raise ImportError("éœ€è¦å®‰è£…PyTorchæ¥ä½¿ç”¨Geological UNet")
        
        self.input_channels = input_channels
        self.output_channels = output_channels
        self.initial_features = initial_features
        self.depth = depth
        
        # ç¼–ç å™¨
        self.encoder = nn.ModuleList()
        in_channels = input_channels
        
        for i in range(depth):
            out_channels = initial_features * (2 ** i)
            self.encoder.append(
                nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU(inplace=True)
                )
            )
            in_channels = out_channels
        
        # è§£ç å™¨
        self.decoder = nn.ModuleList()
        for i in range(depth - 1, -1, -1):
            out_channels = initial_features * (2 ** i)
            self.decoder.append(
                nn.Sequential(
                    nn.ConvTranspose2d(in_channels * 2, out_channels, kernel_size=2, stride=2),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU(inplace=True)
                )
            )
            in_channels = out_channels
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Conv2d(initial_features, output_channels, kernel_size=1)
        
        self.to(self.device)
        
        print(f"ğŸ”„ åœ°è´¨UNetåˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}")
        print(f"   è¾“å…¥é€šé“: {input_channels}, è¾“å‡ºé€šé“: {output_channels}")
        print(f"   åˆå§‹ç‰¹å¾: {initial_features}, æ·±åº¦: {depth}")
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # ç¼–ç å™¨
        encoder_outputs = []
        for encoder_block in self.encoder:
            x = encoder_block(x)
            encoder_outputs.append(x)
            x = F.max_pool2d(x, 2)
        
        # è§£ç å™¨
        for i, decoder_block in enumerate(self.decoder):
            # ä¸Šé‡‡æ ·
            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)
            # è·³è·ƒè¿æ¥
            skip_connection = encoder_outputs[-(i + 1)]
            x = torch.cat([x, skip_connection], dim=1)
            x = decoder_block(x)
        
        # è¾“å‡ºå±‚
        x = self.output_layer(x)
        return x
    
    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 100, 
              batch_size: int = 8, learning_rate: float = 0.001) -> dict:
        """è®­ç»ƒUNetæ¨¡å‹"""
        # éªŒè¯è¾“å…¥æ•°æ®
        if X.shape[0] != y.shape[0]:
            raise ValueError(f"Xå’Œyæ ·æœ¬æ•°ä¸åŒ¹é…ï¼š{X.shape[0]} vs {y.shape[0]}")
        
        # æ•°æ®é¢„å¤„ç†
        if X.ndim == 3:
            X = X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])
        if y.ndim == 3:
            y = y.reshape(y.shape[0], 1, y.shape[1], y.shape[2])
        
        # è½¬æ¢ä¸ºå¼ é‡
        X_tensor = torch.FloatTensor(X).to(self.device)
        y_tensor = torch.FloatTensor(y).to(self.device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_tensor, y_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # ä¼˜åŒ–å™¨
        optimizer = optim.Adam(self.parameters(), lr=learning_rate)
        criterion = nn.MSELoss()
        
        history = {'loss': [], 'train_time': 0.0}
        start_time = time.time()
        
        for epoch in range(epochs):
            self.train()
            epoch_loss = 0.0
            
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                
                outputs = self(batch_X)
                loss = criterion(outputs, batch_y)
                
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
            
            epoch_loss /= len(train_loader)
            history['loss'].append(epoch_loss)
            
            if (epoch + 1) % 10 == 0:
                print(f"   Epoch {epoch+1}/{epochs}: loss={epoch_loss:.6f}")
        
        history['train_time'] = time.time() - start_time
        self.is_trained = True
        return history
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        self.eval()
        with torch.no_grad():
            if X.ndim == 3:
                X = X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])
            
            X_tensor = torch.FloatTensor(X).to(self.device)
            outputs = self(X_tensor)
            return outputs.cpu().numpy()


class GeologicalMultiScaleBridge:
    """
    åœ°è´¨å¤šå°ºåº¦æ¡¥æ¥å™¨ - ç”¨äºè·¨å°ºåº¦åœ°è´¨æ¨¡æ‹Ÿ
    
    æ ¸å¿ƒæ€æƒ³ï¼šåœ¨è·¨å°ºåº¦åœ°è´¨æ¨¡æ‹Ÿä¸­ï¼ˆå¦‚ä»å¾®è§‚å­”éš™åˆ°å®è§‚æ²¹è—ï¼‰ï¼Œ
    ç”¨MLæ¨¡å‹æ›¿ä»£å°å°ºåº¦ç²¾ç»†æ¨¡æ‹Ÿï¼Œå°†å°å°ºåº¦ç»“æœ"æ‰“åŒ…"ä¸ºå¤§å°ºåº¦æ¨¡å‹çš„å‚æ•°
    
    å¢å¼ºåŠŸèƒ½ï¼š
    1. åœ°è´¨å°ºåº¦çº¦æŸï¼ˆè´¨é‡å®ˆæ’ã€åŠ¨é‡å®ˆæ’ç­‰ï¼‰
    2. å¾®è§‚ç‰¹å¾æå–ï¼ˆå­”éš™åº¦ç»Ÿè®¡ã€æ¢¯åº¦åˆ†æç­‰ï¼‰
    3. æ”¯æŒå¤šç§æ¡¥æ¥æ¨¡å‹ç±»å‹
    4. åœ°è´¨ç‰¹å®šçš„å°ºåº¦è½¬æ¢é€»è¾‘
    """
    
    def __init__(self, fine_scale_model: Callable = None, coarse_scale_model: Callable = None):
        self.fine_scale_model = fine_scale_model
        self.coarse_scale_model = coarse_scale_model
        self.bridge_model = None
        self.is_trained = False
        self.scale_ratio = 1000.0  # åœ°è´¨å°ºåº¦æ¯”ï¼ˆ1åƒç±³=1e6æ¯«ç±³ï¼‰
        self.bridge_type = 'neural_network'
        self.geology_constraints = []  # åœ°è´¨å°ºåº¦çº¦æŸ
        self.fine_features_extractor = None  # å¾®è§‚ç‰¹å¾æå–å™¨
        
    def add_geology_constraint(self, constraint: Callable):
        """æ·»åŠ åœ°è´¨å°ºåº¦è½¬æ¢çº¦æŸï¼ˆå¦‚è´¨é‡å®ˆæ’ã€åŠ¨é‡å®ˆæ’ï¼‰"""
        self.geology_constraints.append(constraint)
        print(f"âœ… æ·»åŠ åœ°è´¨çº¦æŸ: {constraint.__name__ if hasattr(constraint, '__name__') else 'custom'}")
    
    def set_fine_features_extractor(self, extractor: Callable):
        """è®¾ç½®å¾®è§‚ç‰¹å¾æå–å™¨"""
        self.fine_features_extractor = extractor
        print(f"âœ… è®¾ç½®å¾®è§‚ç‰¹å¾æå–å™¨: {extractor.__name__ if hasattr(extractor, '__name__') else 'custom'}")
    
    def setup_bridge_model(self, input_dim: int, output_dim: int, model_type: str = 'neural_network'):
        """è®¾ç½®æ¡¥æ¥æ¨¡å‹"""
        self.bridge_type = model_type
        
        if model_type == 'neural_network' and HAS_PYTORCH:
            self.bridge_model = GeologicalPINN(
                input_dim, [128, 64, 32], output_dim
            )
        elif model_type == 'surrogate':
            self.bridge_model = GeologicalSurrogateModel('gaussian_process')
        elif model_type == 'random_forest':
            self.bridge_model = GeologicalSurrogateModel('random_forest', n_estimators=200)
        elif model_type == 'gradient_boosting':
            self.bridge_model = GeologicalSurrogateModel('gradient_boosting')
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¡¥æ¥æ¨¡å‹ç±»å‹: {model_type}")
        
        print(f"âœ… è®¾ç½®æ¡¥æ¥æ¨¡å‹: {model_type}, è¾“å…¥ç»´åº¦: {input_dim}, è¾“å‡ºç»´åº¦: {output_dim}")
    
    def train_bridge(self, fine_scale_data: np.ndarray, coarse_scale_params: np.ndarray, **kwargs) -> dict:
        """
        è®­ç»ƒå°ºåº¦æ¡¥æ¥æ¨¡å‹ï¼šä»å¾®è§‚æ•°æ®æ˜ å°„åˆ°å®è§‚å‚æ•°
        
        Args:
            fine_scale_data: å¾®è§‚æ¨¡æ‹Ÿç»“æœï¼ˆå¦‚å²©çŸ³å­”éš™åº¦åˆ†å¸ƒï¼‰
            coarse_scale_params: å®è§‚ç­‰æ•ˆå‚æ•°ï¼ˆå¦‚ç­‰æ•ˆé»åº¦ã€æ¸—é€ç‡ï¼‰
        """
        if self.bridge_model is None:
            raise ValueError("æ¡¥æ¥æ¨¡å‹å°šæœªè®¾ç½®")
        
        # æå–å¾®è§‚ç‰¹å¾ï¼ˆå¦‚å­”éš™åº¦å‡å€¼ã€æ¢¯åº¦ã€å„å‘å¼‚æ€§ï¼‰
        if self.fine_features_extractor is not None:
            fine_features = self.fine_features_extractor(fine_scale_data)
        else:
            fine_features = self._extract_fine_features(fine_scale_data)
        
        print(f"   å¾®è§‚ç‰¹å¾ç»´åº¦: {fine_features.shape}")
        print(f"   å®è§‚å‚æ•°ç»´åº¦: {coarse_scale_params.shape}")
        
        # è®­ç»ƒæ¡¥æ¥æ¨¡å‹ï¼ŒåŒæ—¶æ–½åŠ åœ°è´¨çº¦æŸ
        if self.bridge_type == 'neural_network' and isinstance(self.bridge_model, GeologicalPINN):
            # è®¾ç½®ç‰©ç†çº¦æŸ
            if self.geology_constraints:
                self.bridge_model.physics_equations = self.geology_constraints
                print(f"   åº”ç”¨åœ°è´¨çº¦æŸ: {len(self.geology_constraints)} ä¸ª")
            
            # è®­ç»ƒPINN
            self.bridge_model.setup_training()
            result = self.bridge_model.train(
                fine_features, coarse_scale_params, 
                physics_weight=1.0,  # åœ°è´¨çº¦æŸæƒé‡
                **kwargs
            )
        else:
            # è®­ç»ƒå…¶ä»–ç±»å‹çš„ä»£ç†æ¨¡å‹
            result = self.bridge_model.train(fine_features, coarse_scale_params, **kwargs)
        
        self.is_trained = True
        return result
    
    def _extract_fine_features(self, fine_data: np.ndarray) -> np.ndarray:
        """ä»å¾®è§‚æ•°æ®ä¸­æå–å¯¹å®è§‚æœ‰æ•ˆçš„ç‰¹å¾ï¼ˆåœ°è´¨é¢†åŸŸçŸ¥è¯†ï¼‰"""
        features = []
        
        for sample in fine_data:
            sample_features = []
            
            # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
            sample_features.extend([
                np.mean(sample),           # å‡å€¼
                np.std(sample),            # æ ‡å‡†å·®
                np.percentile(sample, 25), # 25åˆ†ä½æ•°
                np.percentile(sample, 75), # 75åˆ†ä½æ•°
                np.max(sample),            # æœ€å¤§å€¼
                np.min(sample)             # æœ€å°å€¼
            ])
            
            # ç©ºé—´æ¢¯åº¦ç‰¹å¾ï¼ˆåæ˜ éå‡åŒ€æ€§ï¼‰
            if sample.ndim > 1:
                # è®¡ç®—ç©ºé—´æ¢¯åº¦
                if sample.ndim == 2:  # 2Dæ•°æ®
                    grad_x = np.gradient(sample, axis=1)
                    grad_y = np.gradient(sample, axis=0)
                    gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
                elif sample.ndim == 3:  # 3Dæ•°æ®
                    grad_x = np.gradient(sample, axis=2)
                    grad_y = np.gradient(sample, axis=1)
                    grad_z = np.gradient(sample, axis=0)
                    gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2 + grad_z**2)
                else:
                    gradient_magnitude = np.zeros_like(sample)
                
                sample_features.extend([
                    np.mean(gradient_magnitude),  # å¹³å‡æ¢¯åº¦å¼ºåº¦
                    np.std(gradient_magnitude),   # æ¢¯åº¦å¼ºåº¦æ ‡å‡†å·®
                    np.max(gradient_magnitude)    # æœ€å¤§æ¢¯åº¦å¼ºåº¦
                ])
            else:
                # 1Dæ•°æ®
                gradient = np.gradient(sample)
                sample_features.extend([
                    np.mean(np.abs(gradient)),    # å¹³å‡æ¢¯åº¦å¼ºåº¦
                    np.std(gradient),             # æ¢¯åº¦æ ‡å‡†å·®
                    np.max(np.abs(gradient))      # æœ€å¤§æ¢¯åº¦å¼ºåº¦
                ])
            
            # å„å‘å¼‚æ€§ç‰¹å¾ï¼ˆåœ°è´¨ç»“æ„ç‰¹å¾ï¼‰
            if sample.ndim > 1:
                # è®¡ç®—ä¸åŒæ–¹å‘çš„æ–¹å·®
                if sample.ndim == 2:
                    var_x = np.var(sample, axis=1)  # æ²¿xæ–¹å‘æ–¹å·®
                    var_y = np.var(sample, axis=0)  # æ²¿yæ–¹å‘æ–¹å·®
                    anisotropy = np.std(var_x) / (np.std(var_y) + 1e-10)  # å„å‘å¼‚æ€§æ¯”
                elif sample.ndim == 3:
                    var_x = np.var(sample, axis=(1, 2))  # æ²¿xæ–¹å‘æ–¹å·®
                    var_y = np.var(sample, axis=(0, 2))  # æ²¿yæ–¹å‘æ–¹å·®
                    var_z = np.var(sample, axis=(0, 1))  # æ²¿zæ–¹å‘æ–¹å·®
                    anisotropy = np.std([np.std(var_x), np.std(var_y), np.std(var_z)])
                
                sample_features.append(anisotropy)
            else:
                sample_features.append(0.0)  # 1Dæ•°æ®æ— å„å‘å¼‚æ€§
            
            features.append(sample_features)
        
        return np.array(features)
    
    def predict_coarse_from_fine(self, fine_data: np.ndarray) -> np.ndarray:
        """ä»ç»†å°ºåº¦æ•°æ®é¢„æµ‹ç²—å°ºåº¦æ•°æ®"""
        if not self.is_trained:
            raise ValueError("æ¡¥æ¥æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        # æå–å¾®è§‚ç‰¹å¾
        if self.fine_features_extractor is not None:
            fine_features = self.fine_features_extractor(fine_data)
        else:
            fine_features = self._extract_fine_features(fine_data)
        
        # é¢„æµ‹å®è§‚å‚æ•°
        return self.bridge_model.predict(fine_features)
    
    def predict_fine_from_coarse(self, coarse_data: np.ndarray) -> np.ndarray:
        """ä»ç²—å°ºåº¦æ•°æ®é¢„æµ‹ç»†å°ºåº¦æ•°æ®"""
        if not self.is_trained:
            raise ValueError("æ¡¥æ¥æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        # è¿™é‡Œéœ€è¦å®ç°åå‘æ˜ å°„
        # ç®€åŒ–å®ç°ï¼šè¿”å›ç²—å°ºåº¦æ•°æ®çš„æ’å€¼
        return coarse_data
    
    def set_scale_ratio(self, ratio: float):
        """è®¾ç½®å°ºåº¦æ¯”ä¾‹"""
        self.scale_ratio = ratio
        print(f"âœ… è®¾ç½®åœ°è´¨å°ºåº¦æ¯”: 1:{ratio}")
    
    def get_bridge_info(self) -> dict:
        """è·å–æ¡¥æ¥å™¨ä¿¡æ¯"""
        return {
            'bridge_type': self.bridge_type,
            'scale_ratio': self.scale_ratio,
            'is_trained': self.is_trained,
            'geology_constraints': len(self.geology_constraints),
            'fine_features_extractor': self.fine_features_extractor is not None
        }


class GeologicalHybridAccelerator:
    """
    åœ°è´¨æ··åˆåŠ é€Ÿå™¨ - ç»“åˆä¼ ç»Ÿåœ°è´¨æ•°å€¼æ¨¡æ‹Ÿå’ŒML
    
    æ ¸å¿ƒæ€æƒ³ï¼šæ— æ³•å®Œå…¨æ›¿ä»£ä¼ ç»Ÿåœ°è´¨æ¨¡æ‹Ÿï¼ˆéœ€é«˜ç²¾åº¦ï¼‰ï¼Œ
    ä½†å¯åŠ é€Ÿå…¶ä¸­è€—æ—¶æ­¥éª¤ï¼ˆå¦‚è¿­ä»£æ±‚è§£ã€ç½‘æ ¼è‡ªé€‚åº”ï¼‰
    """
    
    def __init__(self, traditional_solver: Callable = None):
        self.traditional_solver = traditional_solver
        self.ml_models = {}
        self.performance_stats = {
            'traditional_time': 0.0,
            'ml_time': 0.0,
            'speedup': 0.0,
            'accuracy_loss': 0.0,
            'total_calls': 0
        }
        self.acceleration_strategies = {
            'initial_guess': None,
            'preconditioner': None,
            'adaptive_mesh': None,
            'multiscale': None
        }
        # æ–°å¢ï¼šé˜¶æ®µåŠ é€Ÿç­–ç•¥
        self.stage_strategies = {
            'mesh_generation': None,
            'solver': None,
            'postprocessing': None
        }
    
    def add_ml_model(self, name: str, model: Union[GeologicalPINN, GeologicalSurrogateModel]):
        """æ·»åŠ MLæ¨¡å‹"""
        self.ml_models[name] = model
        print(f"âœ… æ·»åŠ åœ°è´¨MLæ¨¡å‹: {name}")
    
    def setup_acceleration_strategy(self, strategy: str, model_name: str):
        """è®¾ç½®åŠ é€Ÿç­–ç•¥"""
        if model_name in self.ml_models:
            self.acceleration_strategies[strategy] = model_name
            print(f"âœ… è®¾ç½®åœ°è´¨åŠ é€Ÿç­–ç•¥: {strategy} -> {model_name}")
        else:
            raise ValueError(f"MLæ¨¡å‹ {model_name} ä¸å­˜åœ¨")
    
    def setup_stage_strategy(self, stage: str, model_name: str):
        """ä¸ºæ¨¡æ‹Ÿçš„ä¸åŒé˜¶æ®µè®¾ç½®åŠ é€Ÿæ¨¡å‹"""
        valid_stages = ['mesh_generation', 'solver', 'postprocessing']
        if stage in valid_stages:
            if model_name in self.ml_models:
                self.stage_strategies[stage] = model_name
                print(f"âœ… è®¾ç½®åœ°è´¨é˜¶æ®µåŠ é€Ÿç­–ç•¥: {stage} -> {model_name}")
            else:
                raise ValueError(f"MLæ¨¡å‹ {model_name} ä¸å­˜åœ¨")
        else:
            raise ValueError(f"æ— æ•ˆçš„é˜¶æ®µç±»å‹: {stage}ï¼Œæœ‰æ•ˆé€‰é¡¹: {valid_stages}")
    
    def solve_hybrid(self, problem_data: Dict, use_ml: bool = True, 
                    ml_model_name: str = None) -> Dict:
        """æ··åˆæ±‚è§£ - æ”¯æŒåŠ¨æ€åˆ‡æ¢ç­–ç•¥"""
        start_time = time.time()
        self.performance_stats['total_calls'] += 1
        
        # æ–°å¢ï¼šè‹¥é—®é¢˜è¦æ±‚é«˜ç²¾åº¦ï¼Œå¼ºåˆ¶ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•+MLåŠ é€Ÿ
        if problem_data.get('accuracy_requirement', 0) < 1e-5 and use_ml:
            use_ml = False  # ç¦ç”¨çº¯MLé¢„æµ‹ï¼Œä»…ç”¨MLåšåˆå§‹çŒœæµ‹
            if ml_model_name:
                self.acceleration_strategies['initial_guess'] = ml_model_name
            print(f"   é«˜ç²¾åº¦è¦æ±‚ï¼Œå¯ç”¨MLåˆå§‹çŒœæµ‹åŠ é€Ÿ")
        
        if use_ml and ml_model_name and ml_model_name in self.ml_models:
            # ä½¿ç”¨MLæ¨¡å‹
            ml_model = self.ml_models[ml_model_name]
            result = ml_model.predict(problem_data['input'])
            ml_time = time.time() - start_time
            
            self.performance_stats['ml_time'] += ml_time
            
            return {
                'solution': result,
                'method': 'ml',
                'time': ml_time,
                'model_name': ml_model_name
            }
        
        else:
            # ä½¿ç”¨ä¼ ç»Ÿæ±‚è§£å™¨
            if self.traditional_solver is None:
                raise ValueError("ä¼ ç»Ÿæ±‚è§£å™¨æœªè®¾ç½®")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰MLåŠ é€Ÿç­–ç•¥
            if self.acceleration_strategies['initial_guess']:
                # ä½¿ç”¨MLé¢„æµ‹åˆå§‹çŒœæµ‹
                initial_guess = self.ml_models[self.acceleration_strategies['initial_guess']].predict(
                    problem_data['input']
                )
                problem_data['initial_guess'] = initial_guess
                print(f"   ä½¿ç”¨MLåˆå§‹çŒœæµ‹åŠ é€Ÿ")
            
            # æ£€æŸ¥é˜¶æ®µåŠ é€Ÿç­–ç•¥
            if 'solver' in problem_data.get('stage', '') and self.stage_strategies['solver']:
                # ä½¿ç”¨MLåŠ é€Ÿæ±‚è§£é˜¶æ®µ
                solver_model = self.ml_models[self.stage_strategies['solver']]
                problem_data['solver_acceleration'] = solver_model
            
            result = self.traditional_solver(problem_data)
            traditional_time = time.time() - start_time
            
            self.performance_stats['traditional_time'] += traditional_time
            
            return {
                'solution': result,
                'method': 'traditional',
                'time': traditional_time
            }
    
    def compare_methods(self, problem_data: Dict, ml_model_name: str = None) -> Dict:
        """æ¯”è¾ƒä¼ ç»Ÿæ–¹æ³•å’ŒMLæ–¹æ³• - å¢å¼ºåœ°è´¨åœºæ™¯è¯„ä¼°"""
        # ä¼ ç»Ÿæ–¹æ³•
        traditional_result = self.solve_hybrid(problem_data, use_ml=False)
        
        # MLæ–¹æ³•
        if ml_model_name and ml_model_name in self.ml_models:
            ml_result = self.solve_hybrid(problem_data, use_ml=True, ml_model_name=ml_model_name)
            
            # è®¡ç®—åŠ é€Ÿæ¯”
            speedup = traditional_result['time'] / ml_result['time']
            
            # è®¡ç®—ç²¾åº¦æŸå¤±ï¼ˆç®€åŒ–ï¼‰
            accuracy_loss = np.mean(np.abs(
                traditional_result['solution'] - ml_result['solution']
            ))
            
            self.performance_stats['speedup'] = speedup
            self.performance_stats['accuracy_loss'] = accuracy_loss
            
            result = {
                'traditional': traditional_result,
                'ml': ml_result,
                'speedup': speedup,
                'accuracy_loss': accuracy_loss
            }
            
            # æ–°å¢ï¼šä¸åœ°è´¨å®æµ‹æ•°æ®çš„æ‹Ÿåˆåº¦è¯„ä¼°
            if 'field_measurements' in problem_data:
                field_data = problem_data['field_measurements']
                ml_fit = np.mean(np.abs(ml_result['solution'] - field_data))
                traditional_fit = np.mean(np.abs(traditional_result['solution'] - field_data))
                result.update({
                    'ml_field_fit': ml_fit,
                    'traditional_field_fit': traditional_fit
                })
            
            return result
        
        return {'traditional': traditional_result}
    
    def get_performance_stats(self) -> dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        stats = self.performance_stats.copy()
        if stats['total_calls'] > 0:
            stats['avg_traditional_time'] = stats['traditional_time'] / stats['total_calls']
            stats['avg_ml_time'] = stats['ml_time'] / stats['total_calls']
        return stats


class GeologicalAdaptiveSolver:
    """
    åœ°è´¨è‡ªé€‚åº”æ±‚è§£å™¨ - ä¸“é—¨é’ˆå¯¹åœ°è´¨åœºæ™¯çš„æ±‚è§£å™¨é€‰æ‹©
    
    æ ¸å¿ƒæ€æƒ³ï¼šæ ¹æ®åœ°è´¨é—®é¢˜ç‰¹å¾ï¼ˆå¦‚æ˜¯å¦å«æ–­å±‚ã€å­”éš™åº¦åˆ†å¸ƒã€ç²¾åº¦è¦æ±‚ç­‰ï¼‰
    è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„æ±‚è§£ç­–ç•¥ï¼ˆMLåŠ é€Ÿæˆ–ä¼ ç»Ÿæ–¹æ³•ï¼‰
    """
    
    def __init__(self):
        self.solvers = {}
        self.selection_strategy = 'performance'  # 'performance', 'rules', 'hybrid'
        self.score_weights = {
            'problem_feature': 1.0,
            'accuracy': 0.5,
            'speed': 0.5,
            'priority': 0.1,
            'geological_complexity': 0.3  # æ–°å¢ï¼šåœ°è´¨å¤æ‚åº¦æƒé‡
        }
        self.performance_history = {}
    
    def add_solver(self, name: str, solver: Callable, 
                  conditions: Dict = None, priority: int = 1):
        """æ·»åŠ æ±‚è§£å™¨"""
        self.solvers[name] = {
            'solver': solver,
            'conditions': conditions or {},
            'priority': priority,
            'performance': {
                'accuracy': 0.0,
                'speed': 0.0,
                'geological_fit': 0.0  # æ–°å¢ï¼šåœ°è´¨æ‹Ÿåˆåº¦
            }
        }
        print(f"âœ… æ·»åŠ åœ°è´¨æ±‚è§£å™¨: {name}")
    
    def set_selection_strategy(self, strategy: str):
        """è®¾ç½®é€‰æ‹©ç­–ç•¥"""
        valid_strategies = ['performance', 'rules', 'hybrid']
        if strategy in valid_strategies:
            self.selection_strategy = strategy
            print(f"âœ… è®¾ç½®åœ°è´¨é€‰æ‹©ç­–ç•¥: {strategy}")
        else:
            raise ValueError(f"æ— æ•ˆçš„é€‰æ‹©ç­–ç•¥: {strategy}")
    
    def set_score_weights(self, weights: Dict[str, float]):
        """è®¾ç½®è¯„åˆ†æƒé‡"""
        self.score_weights.update(weights)
        print(f"âœ… æ›´æ–°åœ°è´¨è¯„åˆ†æƒé‡: {weights}")
    
    def select_best_solver(self, problem_data: Dict) -> str:
        """é€‰æ‹©æœ€ä½³æ±‚è§£å™¨"""
        if not self.solvers:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ±‚è§£å™¨")
        
        if self.selection_strategy == 'performance':
            return self._select_by_performance(problem_data)
        elif self.selection_strategy == 'rules':
            return self._select_by_rules(problem_data)
        elif self.selection_strategy == 'hybrid':
            return self._select_hybrid(problem_data)
        else:
            raise ValueError(f"æœªçŸ¥çš„é€‰æ‹©ç­–ç•¥: {self.selection_strategy}")
    
    def _select_by_performance(self, problem_data: Dict) -> str:
        """åŸºäºæ€§èƒ½é€‰æ‹©æ±‚è§£å™¨"""
        best_solver = None
        best_score = -float('inf')
        
        for name, solver_info in self.solvers.items():
            score = self._evaluate_solver_performance(name, solver_info, problem_data)
            if score > best_score:
                best_score = score
                best_solver = name
        
        return best_solver
    
    def _select_by_rules(self, problem_data: Dict) -> str:
        """åŸºäºè§„åˆ™é€‰æ‹©æ±‚è§£å™¨"""
        for name, solver_info in self.solvers.items():
            if self._check_conditions(solver_info['conditions'], problem_data):
                return name
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„è§„åˆ™ï¼Œè¿”å›ä¼˜å…ˆçº§æœ€é«˜çš„æ±‚è§£å™¨
        return max(self.solvers.keys(), key=lambda k: self.solvers[k]['priority'])
    
    def _select_hybrid(self, problem_data: Dict) -> str:
        """æ··åˆé€‰æ‹©ç­–ç•¥"""
        # é¦–å…ˆå°è¯•è§„åˆ™åŒ¹é…
        for name, solver_info in self.solvers.items():
            if self._check_conditions(solver_info['conditions'], problem_data):
                return name
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„è§„åˆ™ï¼Œä½¿ç”¨æ€§èƒ½é€‰æ‹©
        return self._select_by_performance(problem_data)
    
    def _evaluate_solver_performance(self, name: str, solver_info: Dict, problem_data: Dict) -> float:
        """è¯„ä¼°æ±‚è§£å™¨æ€§èƒ½ - å¢å¼ºåœ°è´¨åœºæ™¯è¯„ä¼°"""
        score = 0.0
        weights = self.score_weights
        
        # é—®é¢˜ç‰¹å¾è¯„ä¼°
        if 'size' in problem_data:
            if problem_data['size'] < 1000 and solver_info['conditions'].get('small_problems', False):
                score += weights['problem_feature']
            elif problem_data['size'] >= 1000 and solver_info['conditions'].get('large_problems', False):
                score += weights['problem_feature']
        
        # ç²¾åº¦è¦æ±‚åŒ¹é…
        if 'accuracy_requirement' in problem_data:
            req = problem_data['accuracy_requirement']
            if solver_info['performance']['accuracy'] >= req:
                score += weights['problem_feature'] * 0.5
        
        # æ–°å¢ï¼šåœ°è´¨ç‰¹å¾è¯„ä¼°
        if problem_data.get('has_faults', False) and solver_info['conditions'].get('handles_faults', False):
            score += weights['geological_complexity'] * 0.8  # æ–­å±‚å¤„ç†èƒ½åŠ›åŠ åˆ†
        
        if problem_data.get('porosity', 0) > 0.2:
            score += solver_info['performance']['accuracy'] * weights['accuracy'] * 1.2  # é«˜å­”éš™åº¦åŒºåŸŸæå‡ç²¾åº¦æƒé‡
        
        # æ€§èƒ½è¯„ä¼°
        performance = solver_info['performance']
        score += performance['accuracy'] * weights['accuracy'] + performance['speed'] * weights['speed']
        score += solver_info['priority'] * weights['priority']
        
        return score
    
    def _check_conditions(self, conditions: Dict, problem_data: Dict) -> bool:
        """æ£€æŸ¥æ¡ä»¶ - æ”¯æŒèŒƒå›´æ¡ä»¶"""
        for key, cond in conditions.items():
            if key not in problem_data:
                return False
            val = problem_data[key]
            
            # æ”¯æŒèŒƒå›´æ¡ä»¶ï¼ˆå¦‚ ('>', 1000)ã€('<=', 0.95)ï¼‰
            if isinstance(cond, tuple) and len(cond) == 2:
                op, threshold = cond
                if op == '>':
                    if not (val > threshold):
                        return False
                elif op == '<':
                    if not (val < threshold):
                        return False
                elif op == '>=':
                    if not (val >= threshold):
                        return False
                elif op == '<=':
                    if not (val <= threshold):
                        return False
                else:
                    return False
            # åŸé€»è¾‘ï¼šæ”¯æŒç­‰å€¼æˆ–åˆ—è¡¨åŒ…å«
            elif isinstance(cond, (list, tuple)):
                if val not in cond:
                    return False
            else:
                if val != cond:
                    return False
        return True
    
    def solve(self, problem_data: Dict) -> Dict:
        """æ±‚è§£"""
        start_time = time.time()
        
        # é€‰æ‹©æœ€ä½³æ±‚è§£å™¨
        best_solver_name = self.select_best_solver(problem_data)
        solver_info = self.solvers[best_solver_name]
        solver = solver_info['solver']
        
        print(f"ğŸ” é€‰æ‹©åœ°è´¨æ±‚è§£å™¨: {best_solver_name}")
        
        # æ‰§è¡Œæ±‚è§£
        try:
            result = solver(problem_data)
            solve_time = time.time() - start_time
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            solver_info['performance']['speed'] = 1.0 / (1.0 + solve_time)  # å½’ä¸€åŒ–é€Ÿåº¦
            
            # è®¡ç®—ç²¾åº¦ï¼ˆå¦‚æœæœ‰å‚è€ƒè§£ï¼‰
            if 'reference_solution' in problem_data:
                mse = np.mean((result - problem_data['reference_solution'])**2)
                accuracy = 1.0 / (1.0 + mse)  # å½’ä¸€åŒ–åˆ° [0,1]
                solver_info['performance']['accuracy'] = accuracy
            
            # è®¡ç®—åœ°è´¨æ‹Ÿåˆåº¦ï¼ˆå¦‚æœæœ‰å®æµ‹æ•°æ®ï¼‰
            if 'field_measurements' in problem_data:
                field_fit = np.mean(np.abs(result - problem_data['field_measurements']))
                geological_fit = 1.0 / (1.0 + field_fit)  # å½’ä¸€åŒ–åˆ° [0,1]
                solver_info['performance']['geological_fit'] = geological_fit
            
            return {
                'solution': result,
                'solver_name': best_solver_name,
                'time': solve_time,
                'performance': solver_info['performance']
            }
        
        except Exception as e:
            print(f"âŒ æ±‚è§£å™¨ {best_solver_name} å¤±è´¥: {e}")
            raise
    
    def get_performance_summary(self) -> Dict:
        """è·å–æ€§èƒ½æ€»ç»“"""
        summary = {
            'total_solvers': len(self.solvers),
            'selection_strategy': self.selection_strategy,
            'solver_performance': {}
        }
        
        for name, solver_info in self.solvers.items():
            summary['solver_performance'][name] = {
                'priority': solver_info['priority'],
                'performance': solver_info['performance'],
                'conditions': solver_info['conditions']
            }
        
        return summary


def create_geological_ml_system() -> Dict:
    """åˆ›å»ºåœ°è´¨MLç³»ç»Ÿ"""
    system = {
        'pinn': GeologicalPINN,
        'surrogate': GeologicalSurrogateModel,
        'unet': GeologicalUNet,
        'bridge': GeologicalMultiScaleBridge,
        'hybrid': GeologicalHybridAccelerator,
        'adaptive': GeologicalAdaptiveSolver,  # æ–°å¢ï¼šåœ°è´¨è‡ªé€‚åº”æ±‚è§£å™¨
        'physics_equations': GeologicalPhysicsEquations
    }
    
    print("ğŸ”„ åœ°è´¨MLç³»ç»Ÿåˆ›å»ºå®Œæˆ")
    return system


def demo_geological_ml():
    """æ¼”ç¤ºåœ°è´¨MLåŠŸèƒ½"""
    print("ğŸ¤– åœ°è´¨æ•°å€¼æ¨¡æ‹ŸML/DLèåˆæ¼”ç¤º")
    print("=" * 60)
    
    # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
    np.random.seed(42)
    if HAS_PYTORCH:
        torch.manual_seed(42)
    
    # åˆ›å»ºåœ°è´¨MLç³»ç»Ÿ
    ml_system = create_geological_ml_system()
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    n_samples = 1000
    input_dim = 4  # x, y, z, t
    output_dim = 1  # å‹åŠ›åœº
    
    X = np.random.randn(n_samples, input_dim)
    y = np.random.randn(n_samples, output_dim)
    
    # ç”Ÿæˆåœ°è´¨ç‰¹å¾æ•°æ®
    geological_features = np.random.rand(n_samples, 3)  # å­”éš™åº¦ã€æ¸—é€ç‡ã€ç²˜åº¦
    
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®: {n_samples} æ ·æœ¬, è¾“å…¥ç»´åº¦: {input_dim}, è¾“å‡ºç»´åº¦: {output_dim}")
    print(f"   åœ°è´¨ç‰¹å¾ç»´åº¦: {geological_features.shape[1]}")
    
    # 1. æµ‹è¯•åœ°è´¨PINNï¼ˆå¢å¼ºç‰ˆï¼‰
    print("\nğŸ”§ æµ‹è¯•åœ°è´¨PINNï¼ˆå¢å¼ºç‰ˆï¼‰...")
    try:
        # å®šä¹‰åœ°è´¨ç‰©ç†æ–¹ç¨‹
        darcy_eq = lambda x, y, config: ml_system['physics_equations'].darcy_equation(x, y, config)
        
        pinn = ml_system['pinn'](input_dim, [64, 32], output_dim, 
                                physics_equations=[darcy_eq])
        
        pinn.setup_training()
        result = pinn.train(X, y, geological_features=geological_features, epochs=200)
        print(f"   è®­ç»ƒæ—¶é—´: {result['train_time']:.4f} ç§’")
        print(f"   æœ€ç»ˆæ€»æŸå¤±: {result['total_loss'][-1]:.6f}")
        
    except Exception as e:
        print(f"   âŒ åœ°è´¨PINNå¤±è´¥: {e}")
    
    # 2. æµ‹è¯•åœ°è´¨ä»£ç†æ¨¡å‹ï¼ˆå¢å¼ºç‰ˆï¼‰
    print("\nğŸ”§ æµ‹è¯•åœ°è´¨ä»£ç†æ¨¡å‹ï¼ˆå¢å¼ºç‰ˆï¼‰...")
    try:
        surrogate = ml_system['surrogate']('gaussian_process')
        result = surrogate.train(X, y.flatten(), geological_features=geological_features)
        print(f"   è®­ç»ƒæ—¶é—´: {result['training_time']:.4f} ç§’")
        
        # é¢„æµ‹
        predictions, std = surrogate.predict(X[:10], return_std=True)
        print(f"   é¢„æµ‹å½¢çŠ¶: {predictions.shape}, æ ‡å‡†å·®å½¢çŠ¶: {std.shape}")
        
    except Exception as e:
        print(f"   âŒ åœ°è´¨ä»£ç†æ¨¡å‹å¤±è´¥: {e}")
    
    # 3. æµ‹è¯•åœ°è´¨UNet
    print("\nğŸ”§ æµ‹è¯•åœ°è´¨UNet...")
    try:
        # ç”Ÿæˆ2Dç©ºé—´æ•°æ®
        n_samples_2d = 100
        height, width = 64, 64
        X_2d = np.random.randn(n_samples_2d, height, width)
        y_2d = np.random.randn(n_samples_2d, height, width)
        
        unet = ml_system['unet'](input_channels=1, output_channels=1, depth=3)
        result = unet.train(X_2d, y_2d, epochs=50)
        print(f"   è®­ç»ƒæ—¶é—´: {result['train_time']:.4f} ç§’")
        print(f"   æœ€ç»ˆæŸå¤±: {result['loss'][-1]:.6f}")
        
    except Exception as e:
        print(f"   âŒ åœ°è´¨UNetå¤±è´¥: {e}")
    
    # 4. æµ‹è¯•å¤šå°ºåº¦æ¡¥æ¥
    print("\nğŸ”§ æµ‹è¯•å¤šå°ºåº¦æ¡¥æ¥...")
    try:
        bridge = ml_system['bridge']()
        bridge.setup_bridge_model(input_dim, output_dim, 'neural_network')
        
        # æ¨¡æ‹Ÿç»†å°ºåº¦å’Œç²—å°ºåº¦æ•°æ®
        fine_data = X
        coarse_data = y
        
        result = bridge.train_bridge(fine_data, coarse_data, epochs=100)
        print(f"   æ¡¥æ¥æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # æµ‹è¯•æ¡¥æ¥
        coarse_pred = bridge.predict_coarse_from_fine(X[:10])
        print(f"   ç²—å°ºåº¦é¢„æµ‹å½¢çŠ¶: {coarse_pred.shape}")
        
    except Exception as e:
        print(f"   âŒ å¤šå°ºåº¦æ¡¥æ¥å¤±è´¥: {e}")
    
    # 5. æµ‹è¯•æ··åˆåŠ é€Ÿå™¨ï¼ˆå¢å¼ºç‰ˆï¼‰
    print("\nğŸ”§ æµ‹è¯•æ··åˆåŠ é€Ÿå™¨ï¼ˆå¢å¼ºç‰ˆï¼‰...")
    try:
        hybrid_accelerator = ml_system['hybrid']()
        
        # æ·»åŠ MLæ¨¡å‹
        surrogate_model = ml_system['surrogate']('random_forest')
        surrogate_model.train(X, y.flatten(), geological_features=geological_features)
        hybrid_accelerator.add_ml_model('surrogate', surrogate_model)
        
        # è®¾ç½®åŠ é€Ÿç­–ç•¥
        hybrid_accelerator.setup_acceleration_strategy('initial_guess', 'surrogate')
        hybrid_accelerator.setup_stage_strategy('solver', 'surrogate')
        
        # æµ‹è¯•æ··åˆæ±‚è§£
        problem_data = {
            'input': X[:10],
            'accuracy_requirement': 1e-4,  # é«˜ç²¾åº¦è¦æ±‚
            'stage': 'solver'
        }
        result = hybrid_accelerator.solve_hybrid(problem_data, use_ml=True, ml_model_name='surrogate')
        print(f"   æ··åˆæ±‚è§£å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {result['model_name']}")
        print(f"   æ±‚è§£æ—¶é—´: {result['time']:.4f} ç§’")
        
    except Exception as e:
        print(f"   âŒ æ··åˆåŠ é€Ÿå™¨å¤±è´¥: {e}")
    
    # 6. æµ‹è¯•åœ°è´¨è‡ªé€‚åº”æ±‚è§£å™¨ï¼ˆæ–°å¢ï¼‰
    print("\nğŸ”§ æµ‹è¯•åœ°è´¨è‡ªé€‚åº”æ±‚è§£å™¨...")
    try:
        adaptive_solver = ml_system['adaptive']()
        
        # å®šä¹‰æ±‚è§£å™¨
        def fast_solver(data):
            return np.random.randn(len(data['input']))
        
        def accurate_solver(data):
            return np.random.randn(len(data['input'])) * 0.1  # æ›´ç²¾ç¡®
        
        def fault_solver(data):
            return np.random.randn(len(data['input'])) * 0.05  # æ–­å±‚å¤„ç†
        
        # æ·»åŠ æ±‚è§£å™¨
        adaptive_solver.add_solver('fast', fast_solver, 
                                  conditions={'size': ('<', 1000)}, priority=1)
        adaptive_solver.add_solver('accurate', accurate_solver, 
                                  conditions={'accuracy_requirement': ('>', 0.9)}, priority=2)
        adaptive_solver.add_solver('fault', fault_solver, 
                                  conditions={'has_faults': True}, priority=3)
        
        # è®¾ç½®é€‰æ‹©ç­–ç•¥
        adaptive_solver.set_selection_strategy('hybrid')
        adaptive_solver.set_score_weights({'geological_complexity': 0.5})
        
        # æµ‹è¯•ä¸åŒåœºæ™¯
        scenarios = [
            {'input': X[:100], 'size': 100, 'name': 'å°è§„æ¨¡é—®é¢˜'},
            {'input': X[:100], 'accuracy_requirement': 0.95, 'name': 'é«˜ç²¾åº¦è¦æ±‚'},
            {'input': X[:100], 'has_faults': True, 'porosity': 0.3, 'name': 'å¤æ‚åœ°è´¨æ¡ä»¶'}
        ]
        
        for scenario in scenarios:
            print(f"   æµ‹è¯•åœºæ™¯: {scenario['name']}")
            result = adaptive_solver.solve(scenario)
            print(f"     é€‰æ‹©æ±‚è§£å™¨: {result['solver_name']}")
            print(f"     æ±‚è§£æ—¶é—´: {result['time']:.4f} ç§’")
        
    except Exception as e:
        print(f"   âŒ åœ°è´¨è‡ªé€‚åº”æ±‚è§£å™¨å¤±è´¥: {e}")
    
    print("\nâœ… åœ°è´¨æ•°å€¼æ¨¡æ‹ŸML/DLèåˆæ¼”ç¤ºå®Œæˆ!")


# ==================== å…ƒå­¦ä¹ åŠŸèƒ½ ====================

class GeodynamicMetaTask:
    """åœ°çƒåŠ¨åŠ›å­¦å…ƒä»»åŠ¡ç±»"""
    
    def __init__(self, name: str, data_generator: Callable, 
                 geological_conditions: Dict[str, Any]):
        self.name = name
        self.data_generator = data_generator
        self.geological_conditions = geological_conditions
        self.task_data = None
        self.validation_data = None
    
    def generate_data(self, num_samples: int = 1000):
        """ç”Ÿæˆä»»åŠ¡æ•°æ®"""
        self.task_data = self.data_generator(num_samples)
        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®
        split_idx = int(0.8 * num_samples)
        self.validation_data = (
            self.task_data[0][split_idx:], 
            self.task_data[1][split_idx:]
        )
        self.task_data = (
            self.task_data[0][:split_idx], 
            self.task_data[1][:split_idx]
        )
        return self.task_data
    
    def get_validation_data(self):
        """è·å–éªŒè¯æ•°æ®"""
        return self.validation_data


class GeodynamicMetaLearner:
    """åœ°çƒåŠ¨åŠ›å­¦å…ƒå­¦ä¹ å™¨"""
    
    def __init__(self, pinn_model: 'GeologicalPINN', 
                 meta_learning_rate: float = 0.001,
                 inner_learning_rate: float = 0.005,
                 adaptation_steps: int = 3):
        self.pinn_model = pinn_model
        self.meta_learning_rate = meta_learning_rate
        self.inner_learning_rate = inner_learning_rate
        self.adaptation_steps = adaptation_steps
        
        # å…ƒå­¦ä¹ ä¼˜åŒ–å™¨
        self.meta_optimizer = optim.Adam(
            self.pinn_model.parameters(), 
            lr=meta_learning_rate
        )
        
        # è®°å½•å…ƒå­¦ä¹ è¿‡ç¨‹
        self.meta_loss_history = []
        self.adaptation_history = []
        self.task_performance = {}
    
    def create_geodynamic_meta_tasks(self) -> List[GeodynamicMetaTask]:
        """åˆ›å»ºåœ°çƒåŠ¨åŠ›å­¦å…ƒä»»åŠ¡é›†ï¼ˆä¸åŒæ„é€ åœºæ™¯ï¼‰"""
        meta_tasks = []
        
        # ä»»åŠ¡1ï¼šå¤§æ´‹ä¸­è„Šæ‰©å¼ ï¼ˆé«˜æ¸©ã€ä½ç²˜åº¦ï¼‰
        def generate_ridge_data(num_samples):
            """ç”Ÿæˆä¸­è„ŠåŒºåŸŸçš„æ¸©åº¦-é€Ÿåº¦æ ·æœ¬"""
            X = torch.randn(num_samples, 4)  # ç©ºé—´åæ ‡ + æ¸©åº¦
            # ä¸­è„Šç‰¹å¾ï¼šé«˜æ¸©ã€ä½ç²˜åº¦ã€æ‰©å¼ é€Ÿåº¦
            X[:, 3] = 800 + 200 * torch.randn(num_samples)  # é«˜æ¸©
            y = torch.randn(num_samples, 3)  # é€Ÿåº¦åœº + å‹åŠ›
            y[:, 0] = 0.1 + 0.05 * torch.randn(num_samples)  # æ‰©å¼ é€Ÿåº¦
            return X, y
        
        ridge_task = GeodynamicMetaTask(
            "å¤§æ´‹ä¸­è„Šæ‰©å¼ ",
            generate_ridge_data,
            {"temperature_range": (600, 1000), "viscosity": "low", "tectonic_type": "divergent"}
        )
        meta_tasks.append(ridge_task)
        
        # ä»»åŠ¡2ï¼šä¿¯å†²å¸¦ï¼ˆé«˜å‹ã€é«˜ç²˜åº¦å·®å¼‚ï¼‰
        def generate_subduction_data(num_samples):
            """ç”Ÿæˆä¿¯å†²å¸¦æ•°æ®"""
            X = torch.randn(num_samples, 4)
            # ä¿¯å†²å¸¦ç‰¹å¾ï¼šé«˜å‹ã€é«˜ç²˜åº¦å·®å¼‚ã€å‹ç¼©åº”åŠ›
            X[:, 3] = 400 + 100 * torch.randn(num_samples)  # ä¸­ç­‰æ¸©åº¦
            y = torch.randn(num_samples, 3)
            y[:, 0] = -0.05 + 0.02 * torch.randn(num_samples)  # å‹ç¼©é€Ÿåº¦
            return X, y
        
        subduction_task = GeodynamicMetaTask(
            "ä¿¯å†²å¸¦",
            generate_subduction_data,
            {"pressure_range": (1e8, 1e9), "viscosity": "high", "tectonic_type": "convergent"}
        )
        meta_tasks.append(subduction_task)
        
        # ä»»åŠ¡3ï¼šå¤§é™†ç¢°æ’å¸¦ï¼ˆå¤æ‚å¼¹æ€§å˜å½¢ï¼‰
        def generate_collision_data(num_samples):
            """ç”Ÿæˆå¤§é™†ç¢°æ’å¸¦æ•°æ®"""
            X = torch.randn(num_samples, 4)
            # ç¢°æ’å¸¦ç‰¹å¾ï¼šå¤æ‚å˜å½¢ã€é«˜å¼¹æ€§æ¨¡é‡
            X[:, 3] = 300 + 150 * torch.randn(num_samples)  # ä½æ¸©
            y = torch.randn(num_samples, 3)
            y[:, 0] = 0.02 + 0.01 * torch.randn(num_samples)  # å°å˜å½¢
            return X, y
        
        collision_task = GeodynamicMetaTask(
            "å¤§é™†ç¢°æ’å¸¦",
            generate_collision_data,
            {"deformation_type": "complex", "elastic_modulus": "high", "tectonic_type": "collision"}
        )
        meta_tasks.append(collision_task)
        
        return meta_tasks
    
    def meta_train_geodynamics(self, meta_tasks: List[GeodynamicMetaTask], 
                               meta_epochs: int = 50, 
                               task_samples: int = 1000):
        """å…ƒå­¦ä¹ è®­ç»ƒé€‚é… - é’ˆå¯¹åœ°çƒåŠ¨åŠ›å­¦ä»»åŠ¡è°ƒæ•´"""
        print(f"ğŸš€ å¼€å§‹åœ°çƒåŠ¨åŠ›å­¦å…ƒå­¦ä¹ è®­ç»ƒ...")
        print(f"   å…ƒä»»åŠ¡æ•°é‡: {len(meta_tasks)}")
        print(f"   å…ƒå­¦ä¹ è½®æ•°: {meta_epochs}")
        print(f"   å†…å¾ªç¯æ­¥æ•°: {self.adaptation_steps}")
        
        for meta_epoch in range(meta_epochs):
            meta_loss = 0.0
            epoch_adaptations = []
            
            for task_idx, task in enumerate(meta_tasks):
                # ç”Ÿæˆä»»åŠ¡æ•°æ®
                X_task, y_task = task.generate_data(task_samples)
                X_val, y_val = task.get_validation_data()
                
                # ä¿å­˜åˆå§‹å‚æ•°
                initial_params = {n: p.clone() for n, p in self.pinn_model.named_parameters()}
                
                # å†…å¾ªç¯ï¼šé€‚é…ç‰¹å®šæ„é€ åœºæ™¯ï¼ˆå¦‚ä¿¯å†²å¸¦ï¼‰
                task_losses = []
                for step in range(self.adaptation_steps):
                    outputs = self.pinn_model(X_task)
                    
                    # é‡ç‚¹æƒ©ç½šç‰©ç†æ®‹å·®ï¼ˆä¿è¯è·¨åœºæ™¯çš„ç‰©ç†ä¸€è‡´æ€§ï¼‰
                    data_loss = F.mse_loss(outputs, y_task)
                    physics_loss = self.pinn_model.compute_physics_loss(X_task, outputs)
                    task_loss = 0.5 * data_loss + 0.5 * physics_loss
                    
                    task_losses.append(task_loss.item())
                    
                    # å†…å¾ªç¯æ›´æ–°ï¼ˆä»…å¾®è°ƒä¸Šå±‚å‚æ•°ï¼Œä¿ç•™åº•å±‚ç‰©ç†ç‰¹å¾ï¼‰
                    self.pinn_model.zero_grad()
                    task_loss.backward(retain_graph=True)
                    
                    with torch.no_grad():
                        for name, p in self.pinn_model.named_parameters():
                            if "output_layer" in name or "conv2" in name:
                                # ä¸Šå±‚å‚æ•°å¯å¾®è°ƒ
                                p -= self.inner_learning_rate * p.grad
                            # åº•å±‚å‚æ•°ä¿æŒä¸å˜ï¼Œä¿ç•™é€šç”¨ç‰©ç†ç‰¹å¾
                
                # å…ƒæŸå¤±ï¼šæ³›åŒ–åˆ°ä»»åŠ¡éªŒè¯é›†
                val_outputs = self.pinn_model(X_val)
                val_loss = self.pinn_model.compute_physics_loss(X_val, val_outputs)
                meta_loss += val_loss
                
                # è®°å½•ä»»åŠ¡æ€§èƒ½
                self.task_performance[f"{task.name}_epoch_{meta_epoch}"] = {
                    "task_losses": task_losses,
                    "validation_loss": val_loss.item(),
                    "final_task_loss": task_losses[-1]
                }
                
                epoch_adaptations.append({
                    "task": task.name,
                    "task_losses": task_losses,
                    "validation_loss": val_loss.item()
                })
                
                # æ¢å¤åˆå§‹å‚æ•°
                self.pinn_model.load_state_dict(initial_params)
            
            # å¤–å¾ªç¯æ›´æ–°ï¼šä¿ç•™å¯¹æ‰€æœ‰æ„é€ åœºæ™¯é€šç”¨çš„ç‰¹å¾ï¼ˆå¦‚ç²˜åº¦-æ¸©åº¦å…³ç³»ï¼‰
            self.meta_optimizer.zero_grad()
            meta_loss.backward()
            self.meta_optimizer.step()
            
            # è®°å½•å…ƒå­¦ä¹ è¿‡ç¨‹
            self.meta_loss_history.append(meta_loss.item())
            self.adaptation_history.append(epoch_adaptations)
            
            if meta_epoch % 10 == 0:
                print(f"   å…ƒå­¦ä¹ è½®æ¬¡ {meta_epoch}: å…ƒæŸå¤± = {meta_loss.item():.6f}")
        
        print(f"âœ… åœ°çƒåŠ¨åŠ›å­¦å…ƒå­¦ä¹ è®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ç»ˆå…ƒæŸå¤±: {self.meta_loss_history[-1]:.6f}")
        return self.meta_loss_history, self.adaptation_history
    
    def adapt_to_new_region(self, new_region_data: Tuple[torch.Tensor, torch.Tensor],
                           adaptation_steps: int = 5) -> Dict[str, Any]:
        """å¿«é€Ÿé€‚é…åˆ°æ–°åŒºåŸŸ"""
        print(f"ğŸ”„ å¿«é€Ÿé€‚é…åˆ°æ–°åŒºåŸŸ...")
        
        X_new, y_new = new_region_data
        initial_params = {n: p.clone() for n, p in self.pinn_model.named_parameters()}
        
        adaptation_losses = []
        
        for step in range(adaptation_steps):
            outputs = self.pinn_model(X_new)
            
            # æ–°åŒºåŸŸæŸå¤±ï¼šæ•°æ®æŸå¤± + ç‰©ç†æŸå¤±
            data_loss = F.mse_loss(outputs, y_new)
            physics_loss = self.pinn_model.compute_physics_loss(X_new, outputs)
            total_loss = 0.5 * data_loss + 0.5 * physics_loss
            
            adaptation_losses.append(total_loss.item())
            
            # å¿«é€Ÿé€‚é…ï¼šä»…æ›´æ–°ä¸Šå±‚å‚æ•°
            self.pinn_model.zero_grad()
            total_loss.backward(retain_graph=True)
            
            with torch.no_grad():
                for name, p in self.pinn_model.named_parameters():
                    if "output_layer" in name or "conv2" in name:
                        p -= self.inner_learning_rate * p.grad
        
        # è¯„ä¼°é€‚é…æ•ˆæœ
        final_outputs = self.pinn_model(X_new)
        final_data_loss = F.mse_loss(final_outputs, y_new).item()
        final_physics_loss = self.pinn_model.compute_physics_loss(X_new, final_outputs).item()
        
        # æ¢å¤å…ƒå­¦ä¹ å‚æ•°
        self.pinn_model.load_state_dict(initial_params)
        
        adaptation_result = {
            "adaptation_steps": adaptation_steps,
            "loss_history": adaptation_losses,
            "final_data_loss": final_data_loss,
            "final_physics_loss": final_physics_loss,
            "total_loss_reduction": adaptation_losses[0] - adaptation_losses[-1]
        }
        
        print(f"   é€‚é…å®Œæˆï¼Œæ€»æŸå¤±å‡å°‘: {adaptation_result['total_loss_reduction']:.6f}")
        return adaptation_result
    
    def get_meta_learning_status(self) -> Dict[str, Any]:
        """è·å–å…ƒå­¦ä¹ çŠ¶æ€"""
        return {
            "meta_loss_history": self.meta_loss_history,
            "adaptation_history": self.adaptation_history,
            "task_performance": self.task_performance,
            "meta_learning_rate": self.meta_learning_rate,
            "inner_learning_rate": self.inner_learning_rate,
            "adaptation_steps": self.adaptation_steps
        }


# åœ¨GeologicalPINNç±»ä¸­æ·»åŠ å…ƒå­¦ä¹ æ”¯æŒ
def add_meta_learning_support_to_pinn():
    """ä¸ºGeologicalPINNç±»æ·»åŠ å…ƒå­¦ä¹ æ”¯æŒ"""
    
    def meta_train_geodynamics(self, meta_tasks, meta_epochs=50, task_samples=1000):
        """å…ƒå­¦ä¹ è®­ç»ƒæ–¹æ³•"""
        if not hasattr(self, 'meta_learner'):
            self.meta_learner = GeodynamicMetaLearner(self)
        return self.meta_learner.meta_train_geodynamics(meta_tasks, meta_epochs, task_samples)
    
    def adapt_to_new_region(self, new_region_data, adaptation_steps=5):
        """å¿«é€Ÿé€‚é…åˆ°æ–°åŒºåŸŸ"""
        if not hasattr(self, 'meta_learner'):
            self.meta_learner = GeodynamicMetaLearner(self)
        return self.meta_learner.adapt_to_new_region(new_region_data, adaptation_steps)
    
    def get_meta_learning_status(self):
        """è·å–å…ƒå­¦ä¹ çŠ¶æ€"""
        if hasattr(self, 'meta_learner'):
            return self.meta_learner.get_meta_learning_status()
        return {"status": "Meta-learning not initialized"}
    
    # åŠ¨æ€æ·»åŠ æ–¹æ³•åˆ°GeologicalPINNç±»
    GeologicalPINN.meta_train_geodynamics = meta_train_geodynamics
    GeologicalPINN.adapt_to_new_region = adapt_to_new_region
    GeologicalPINN.get_meta_learning_status = get_meta_learning_status


# åˆå§‹åŒ–æ—¶æ·»åŠ å…ƒå­¦ä¹ æ”¯æŒ
add_meta_learning_support_to_pinn()


# ==================== RLå¼ºåŒ–å­¦ä¹ åŠŸèƒ½ ====================

class DQNAgent:
    """DQNæ™ºèƒ½ä½“ - ç”¨äºæ—¶é—´æ­¥é•¿ä¼˜åŒ–"""
    
    def __init__(self, state_dim: int, action_dim: int, learning_rate: float = 0.001):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        
        # ç®€å•çš„ç¥ç»ç½‘ç»œQå‡½æ•°
        if HAS_PYTORCH:
            self.q_network = nn.Sequential(
                nn.Linear(state_dim, 64),
                nn.ReLU(),
                nn.Linear(64, 64),
                nn.ReLU(),
                nn.Linear(64, action_dim)
            )
            self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
            self.epsilon = 0.1  # æ¢ç´¢ç‡
        else:
            self.q_network = None
            self.optimizer = None
            self.epsilon = 0.1
    
    def choose_action(self, state: np.ndarray) -> int:
        """é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-è´ªå©ªç­–ç•¥ï¼‰"""
        if np.random.random() < self.epsilon:
            return np.random.randint(0, self.action_dim)
        
        if self.q_network is not None:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                q_values = self.q_network(state_tensor)
                return q_values.argmax().item()
        else:
            return np.random.randint(0, self.action_dim)
    
    def learn(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray):
        """å­¦ä¹ æ›´æ–°Qå€¼"""
        if self.q_network is None:
            return
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
            next_q_values = self.q_network(next_state_tensor)
            target_q = reward + 0.99 * next_q_values.max()
        
        # è®¡ç®—å½“å‰Qå€¼
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        current_q_values = self.q_network(state_tensor)
        current_q = current_q_values[0, action]
        
        # è®¡ç®—æŸå¤±å¹¶æ›´æ–°
        loss = F.mse_loss(current_q, torch.tensor(target_q))
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()


class PPORLAgent:
    """PPOå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ - ç”¨äºåæ¼”ä¼˜åŒ–"""
    
    def __init__(self, state_dim: int, action_dim: int, learning_rate: float = 0.0003):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        
        if HAS_PYTORCH:
            # Actorç½‘ç»œï¼ˆç­–ç•¥ï¼‰
            self.actor = nn.Sequential(
                nn.Linear(state_dim, 128),
                nn.ReLU(),
                nn.Linear(128, 128),
                nn.ReLU(),
                nn.Linear(128, action_dim),
                nn.Tanh()  # è¾“å‡ºèŒƒå›´[-1, 1]
            )
            
            # Criticç½‘ç»œï¼ˆä»·å€¼ï¼‰
            self.critic = nn.Sequential(
                nn.Linear(state_dim, 128),
                nn.ReLU(),
                nn.Linear(128, 128),
                nn.ReLU(),
                nn.Linear(128, 1)
            )
            
            self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)
            self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=learning_rate)
            
            # PPOå‚æ•°
            self.clip_ratio = 0.2
            self.value_coef = 0.5
            self.entropy_coef = 0.01
        else:
            self.actor = None
            self.critic = None
            self.actor_optimizer = None
            self.critic_optimizer = None
    
    def select_action(self, state: np.ndarray) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ"""
        if self.actor is None:
            return np.random.randn(self.action_dim) * 0.1
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action_mean = self.actor(state_tensor)
            action_std = torch.ones_like(action_mean) * 0.1
            action_dist = torch.distributions.Normal(action_mean, action_std)
            action = action_dist.sample()
            return action.squeeze().numpy()
    
    def update(self, states: List[np.ndarray], actions: List[np.ndarray], 
               rewards: List[float], next_states: List[np.ndarray]):
        """æ›´æ–°ç­–ç•¥å’Œä»·å€¼ç½‘ç»œ"""
        if self.actor is None or self.critic is None:
            return
        
        # è½¬æ¢ä¸ºå¼ é‡
        states_tensor = torch.FloatTensor(np.array(states))
        actions_tensor = torch.FloatTensor(np.array(actions))
        rewards_tensor = torch.FloatTensor(rewards)
        
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        advantages = rewards_tensor
        
        # æ›´æ–°Critic
        values = self.critic(states_tensor).squeeze()
        value_loss = F.mse_loss(values, rewards_tensor)
        
        self.critic_optimizer.zero_grad()
        value_loss.backward()
        self.critic_optimizer.step()
        
        # æ›´æ–°Actorï¼ˆç®€åŒ–ç‰ˆPPOï¼‰
        action_mean = self.actor(states_tensor)
        action_std = torch.ones_like(action_mean) * 0.1
        action_dist = torch.distributions.Normal(action_mean, action_std)
        
        log_probs = action_dist.log_prob(actions_tensor).sum(dim=1)
        policy_loss = -(log_probs * advantages).mean()
        
        self.actor_optimizer.zero_grad()
        policy_loss.backward()
        self.actor_optimizer.step()


class RLTimeStepOptimizer:
    """RLæ—¶é—´æ­¥é•¿ä¼˜åŒ–å™¨ - ç”¨äºåœ°å¹”å¯¹æµç­‰é•¿æ—¶ç¨‹æ¨¡æ‹Ÿ"""
    
    def __init__(self, solver, base_dt: float = 1e6):
        self.solver = solver
        self.base_dt = base_dt
        
        # çŠ¶æ€ï¼šå½“å‰é€Ÿåº¦åœºæ¢¯åº¦ã€æ¸©åº¦å˜åŒ–ç‡ã€ä¸Šä¸€æ­¥è¯¯å·®
        self.agent = DQNAgent(state_dim=15, action_dim=4)  # 4ç§æ—¶é—´æ­¥ç¼©æ”¾å› å­ï¼ŒçŠ¶æ€ç»´åº¦ä¸º15ï¼ˆ5æ­¥Ã—3ç‰¹å¾ï¼‰
        
        # çŠ¶æ€å†å²
        self.state_history = []
        self.error_history = []
        self.dt_history = []
        
        # ä¼˜åŒ–å‚æ•°
        self.min_dt_scale = 0.1
        self.max_dt_scale = 2.0
        self.target_error = 1e-6
    
    def _get_state_features(self, state_history: List[Dict]) -> np.ndarray:
        """æå–å½“å‰çŠ¶æ€ç‰¹å¾"""
        if len(state_history) < 5:
            # å¦‚æœå†å²ä¸è¶³ï¼Œç”¨é›¶å¡«å……
            padding = [{'velocity_grad': 0.0, 'temp_change': 0.0, 'error': 0.0}] * (5 - len(state_history))
            state_history = padding + state_history
        
        # æå–æœ€è¿‘5æ­¥çš„ç‰¹å¾
        features = []
        for state in state_history[-5:]:
            features.extend([
                state.get('velocity_grad', 0.0),
                state.get('temp_change', 0.0),
                state.get('error', 0.0)
            ])
        
        # å½’ä¸€åŒ–ç‰¹å¾
        features = np.array(features)
        if np.max(features) > 0:
            features = features / np.max(features)
        
        return features
    
    def _smoothness_reward(self, new_state: Dict) -> float:
        """è®¡ç®—ç‰©ç†åœºå¹³æ»‘æ€§å¥–åŠ±"""
        # åŸºäºé€Ÿåº¦åœºæ¢¯åº¦å’Œæ¸©åº¦å˜åŒ–ç‡çš„å¹³æ»‘æ€§
        velocity_grad = new_state.get('velocity_grad', 0.0)
        temp_change = new_state.get('temp_change', 0.0)
        
        # å¹³æ»‘æ€§å¥–åŠ±ï¼šæ¢¯åº¦å˜åŒ–è¶Šå°è¶Šå¥½
        smoothness = 1.0 / (1.0 + abs(velocity_grad) + abs(temp_change))
        return smoothness
    
    def optimize(self, state_history: List[Dict], max_steps: int = 1000) -> Dict[str, Any]:
        """ä¼˜åŒ–æ—¶é—´æ­¥é•¿"""
        print(f"ğŸš€ å¼€å§‹RLæ—¶é—´æ­¥é•¿ä¼˜åŒ–...")
        print(f"   æœ€å¤§æ­¥æ•°: {max_steps}")
        print(f"   åŸºç¡€æ—¶é—´æ­¥: {self.base_dt}")
        
        optimization_results = {
            'dt_history': [],
            'error_history': [],
            'reward_history': [],
            'efficiency_improvement': 0.0
        }
        
        for step in range(max_steps):
            # æå–å½“å‰çŠ¶æ€ç‰¹å¾
            current_state = self._get_state_features(state_history[-5:])
            
            # é€‰æ‹©æ—¶é—´æ­¥é•¿ï¼ˆå¦‚1x, 1.5x, 0.5x, 2xï¼‰
            action = self.agent.choose_action(current_state)
            dt_scale = [0.5, 1.0, 1.5, 2.0][action]
            dt = self.base_dt * dt_scale
            
            # é™åˆ¶æ—¶é—´æ­¥èŒƒå›´
            dt = np.clip(dt, self.base_dt * self.min_dt_scale, self.base_dt * self.max_dt_scale)
            
            # æ‰§è¡Œæ¨¡æ‹Ÿæ­¥ï¼ˆè¿™é‡Œç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰
            new_state, error = self._simulate_step(dt, state_history[-1] if state_history else {})
            
            # å¥–åŠ±è®¾è®¡ï¼šè¯¯å·®å°ï¼ˆ+ï¼‰ã€æ­¥é•¿å¤§ï¼ˆ+ï¼‰ã€ç‰©ç†åœºå¹³æ»‘ï¼ˆ+ï¼‰
            error_penalty = 1.0 - min(error / self.target_error, 1.0)
            dt_reward = np.log(dt / self.base_dt)
            smoothness_reward = 0.1 * self._smoothness_reward(new_state)
            
            reward = error_penalty + dt_reward + smoothness_reward
            
            # å­¦ä¹ æ›´æ–°
            self.agent.learn(current_state, action, reward, self._get_state_features([new_state]))
            
            # è®°å½•å†å²
            self.state_history.append(new_state)
            self.error_history.append(error)
            self.dt_history.append(dt)
            
            optimization_results['dt_history'].append(dt)
            optimization_results['error_history'].append(error)
            optimization_results['reward_history'].append(reward)
            
            # æ›´æ–°çŠ¶æ€å†å²
            state_history.append(new_state)
            
            if step % 100 == 0:
                print(f"   æ­¥æ•° {step}: dt={dt:.2e}, error={error:.2e}, reward={reward:.4f}")
        
        # è®¡ç®—æ•ˆç‡æå‡
        if len(optimization_results['dt_history']) > 1:
            avg_dt = np.mean(optimization_results['dt_history'])
            efficiency_improvement = (avg_dt - self.base_dt) / self.base_dt * 100
            optimization_results['efficiency_improvement'] = efficiency_improvement
        
        print(f"âœ… RLæ—¶é—´æ­¥é•¿ä¼˜åŒ–å®Œæˆ!")
        print(f"   å¹³å‡æ—¶é—´æ­¥: {np.mean(optimization_results['dt_history']):.2e}")
        print(f"   æ•ˆç‡æå‡: {optimization_results['efficiency_improvement']:.1f}%")
        
        return optimization_results
    
    def _simulate_step(self, dt: float, prev_state: Dict) -> Tuple[Dict, float]:
        """æ¨¡æ‹Ÿä¸€æ­¥è®¡ç®—ï¼ˆå®é™…åº”ç”¨ä¸­æ›¿æ¢ä¸ºçœŸå®æ±‚è§£å™¨ï¼‰"""
        # æ¨¡æ‹Ÿåœ°å¹”å¯¹æµçŠ¶æ€
        velocity_grad = np.random.normal(0.1, 0.05) * (1 + np.random.random() * 0.1)
        temp_change = np.random.normal(0.05, 0.02) * (1 + np.random.random() * 0.1)
        
        # è¯¯å·®ä¸æ—¶é—´æ­¥ç›¸å…³
        error = np.random.normal(1e-6, 1e-7) * (dt / self.base_dt) ** 2
        
        new_state = {
            'velocity_grad': velocity_grad,
            'temp_change': temp_change,
            'error': error,
            'dt': dt
        }
        
        return new_state, error


class InversionRLAgent:
    """RLåœ°çƒç‰©ç†åæ¼”æ™ºèƒ½ä½“ - ç”¨äºåœ°ä¸‹å‚æ•°åæ¼”"""
    
    def __init__(self, forward_model, param_dim: int = 10):
        self.forward_model = forward_model  # PINNæ­£æ¼”æ¨¡å‹
        self.param_dim = param_dim
        
        # åŠ¨ä½œï¼šè°ƒæ•´ç²˜åº¦å‚æ•°çš„æ–¹å‘å’Œå¹…åº¦
        self.agent = PPORLAgent(state_dim=10, action_dim=5)  # 10ä¸ªè§‚æµ‹ç‚¹æ®‹å·®ï¼Œ5ç§è°ƒæ•´ç­–ç•¥
        
        # åæ¼”å‚æ•°
        self.param_bounds = {
            'viscosity': (1e18, 1e24),  # PaÂ·s
            'density': (2000, 4000),     # kg/mÂ³
            'thermal_conductivity': (1.0, 5.0)  # W/(mÂ·K)
        }
        
        # åæ¼”å†å²
        self.inversion_history = {
            'params': [],
            'residuals': [],
            'rewards': []
        }
    
    def _get_residual_features(self, obs_data: np.ndarray, pred_data: np.ndarray) -> np.ndarray:
        """è·å–è§‚æµ‹æ®‹å·®ç‰¹å¾"""
        residuals = obs_data - pred_data
        
        # è®¡ç®—æ®‹å·®ç»Ÿè®¡ç‰¹å¾
        features = [
            np.mean(residuals),
            np.std(residuals),
            np.max(np.abs(residuals)),
            np.min(residuals),
            np.max(residuals),
            np.percentile(residuals, 25),
            np.percentile(residuals, 50),
            np.percentile(residuals, 75),
            np.sum(residuals > 0),  # æ­£æ®‹å·®æ•°é‡
            np.sum(residuals < 0)   # è´Ÿæ®‹å·®æ•°é‡
        ]
        
        # å½’ä¸€åŒ–ç‰¹å¾
        features = np.array(features)
        if np.max(np.abs(features)) > 0:
            features = features / np.max(np.abs(features))
        
        return features
    
    def _adjust_params(self, current_params: Dict[str, np.ndarray], 
                       action: np.ndarray) -> Dict[str, np.ndarray]:
        """æ ¹æ®RLåŠ¨ä½œè°ƒæ•´å‚æ•°"""
        new_params = current_params.copy()
        
        # åŠ¨ä½œæ˜ å°„åˆ°å‚æ•°è°ƒæ•´
        # action[0]: ç²˜åº¦è°ƒæ•´å¹…åº¦
        # action[1]: å¯†åº¦è°ƒæ•´å¹…åº¦  
        # action[2]: çƒ­å¯¼ç‡è°ƒæ•´å¹…åº¦
        # action[3]: ç©ºé—´å¹³æ»‘åº¦
        # action[4]: æ—¶é—´å¹³æ»‘åº¦
        
        for param_name in current_params.keys():
            if param_name in self.param_bounds:
                param_array = current_params[param_name]
                bounds = self.param_bounds[param_name]
                
                # è®¡ç®—è°ƒæ•´å¹…åº¦
                if param_name == 'viscosity':
                    adjustment = action[0] * 0.1  # 10%è°ƒæ•´
                elif param_name == 'density':
                    adjustment = action[1] * 0.05  # 5%è°ƒæ•´
                elif param_name == 'thermal_conductivity':
                    adjustment = action[2] * 0.1   # 10%è°ƒæ•´
                else:
                    adjustment = 0.0
                
                # åº”ç”¨è°ƒæ•´
                new_param_array = param_array * (1 + adjustment)
                
                # åº”ç”¨ç©ºé—´å¹³æ»‘ï¼ˆaction[3]ï¼‰
                if action[3] > 0:
                    # ç®€å•çš„ç©ºé—´å¹³æ»‘
                    from scipy.ndimage import gaussian_filter
                    try:
                        new_param_array = gaussian_filter(new_param_array, sigma=action[3])
                    except:
                        pass  # å¦‚æœscipyä¸å¯ç”¨ï¼Œè·³è¿‡å¹³æ»‘
                
                # åº”ç”¨æ—¶é—´å¹³æ»‘ï¼ˆaction[4]ï¼‰
                if action[4] > 0:
                    # æ—¶é—´å¹³æ»‘ï¼šä¸å†å²å€¼å¹³å‡
                    if len(self.inversion_history['params']) > 0:
                        prev_param = self.inversion_history['params'][-1][param_name]
                        alpha = min(action[4], 0.5)  # æœ€å¤§50%å†å²æƒé‡
                        new_param_array = (1 - alpha) * new_param_array + alpha * prev_param
                
                # é™åˆ¶åœ¨è¾¹ç•Œå†…
                new_param_array = np.clip(new_param_array, bounds[0], bounds[1])
                new_params[param_name] = new_param_array
        
        return new_params
    
    def invert(self, obs_data: np.ndarray, init_params: Dict[str, np.ndarray], 
               iterations: int = 100) -> Dict[str, Any]:
        """æ‰§è¡Œåæ¼”ä¼˜åŒ–"""
        print(f"ğŸ”„ å¼€å§‹RLåœ°çƒç‰©ç†åæ¼”...")
        print(f"   åæ¼”è¿­ä»£æ•°: {iterations}")
        print(f"   å‚æ•°ç»´åº¦: {self.param_dim}")
        
        current_params = init_params.copy()
        best_params = init_params.copy()
        best_residual = float('inf')
        
        # è®°å½•åˆå§‹çŠ¶æ€
        self.inversion_history['params'].append(init_params.copy())
        
        for iteration in range(iterations):
            # æ­£æ¼”æ¨¡æ‹Ÿ
            try:
                pred_data = self.forward_model(current_params)
            except:
                # å¦‚æœæ­£æ¼”å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
                pred_data = obs_data + np.random.normal(0, 0.1 * np.std(obs_data), obs_data.shape)
            
            # çŠ¶æ€ï¼šè§‚æµ‹æ®‹å·®ç‰¹å¾
            state = self._get_residual_features(obs_data, pred_data)
            
            # é€‰æ‹©å‚æ•°è°ƒæ•´åŠ¨ä½œ
            action = self.agent.select_action(state)
            new_params = self._adjust_params(current_params, action)
            
            # è®¡ç®—æ–°å‚æ•°çš„æ­£æ¼”ç»“æœ
            try:
                new_pred_data = self.forward_model(new_params)
            except:
                new_pred_data = obs_data + np.random.normal(0, 0.1 * np.std(obs_data), obs_data.shape)
            
            # å¥–åŠ±ï¼šæ®‹å·®å‡å°+å‚æ•°å¹³æ»‘ï¼ˆç¬¦åˆåœ°è´¨è¿ç»­æ€§ï¼‰
            residual_reward = -np.mean(np.square(obs_data - new_pred_data))
            smooth_reward = -0.1 * np.var(list(new_params.values()))
            total_reward = residual_reward + smooth_reward
            
            # é™åˆ¶å¥–åŠ±èŒƒå›´ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            total_reward = np.clip(total_reward, -100.0, 100.0)
            
            # æ›´æ–°æ™ºèƒ½ä½“
            self.agent.update([state], [action], [total_reward], [state])
            
            # æ›´æ–°å‚æ•°
            current_params = new_params
            
            # è®°å½•å†å²
            self.inversion_history['params'].append(current_params.copy())
            self.inversion_history['residuals'].append(residual_reward)
            self.inversion_history['rewards'].append(total_reward)
            
            # æ›´æ–°æœ€ä½³å‚æ•°
            if residual_reward > best_residual:
                best_residual = residual_reward
                best_params = current_params.copy()
            
            if iteration % 20 == 0:
                print(f"   è¿­ä»£ {iteration}: æ®‹å·®={-residual_reward:.6f}, å¥–åŠ±={total_reward:.4f}")
        
        print(f"âœ… RLåæ¼”å®Œæˆ!")
        print(f"   æœ€ä½³æ®‹å·®: {-best_residual:.6f}")
        print(f"   æœ€ç»ˆæ®‹å·®: {-self.inversion_history['residuals'][-1]:.6f}")
        
        return {
            'best_params': best_params,
            'final_params': current_params,
            'best_residual': -best_residual,
            'final_residual': -self.inversion_history['residuals'][-1],
            'inversion_history': self.inversion_history,
            'efficiency_improvement': self._calculate_efficiency_improvement()
        }
    
    def _calculate_efficiency_improvement(self) -> float:
        """è®¡ç®—åæ¼”æ•ˆç‡æå‡"""
        if len(self.inversion_history['residuals']) < 2:
            return 0.0
        
        initial_residual = -self.inversion_history['residuals'][0]
        final_residual = -self.inversion_history['residuals'][-1]
        
        if initial_residual > 0:
            improvement = (initial_residual - final_residual) / initial_residual * 100
            return improvement
        return 0.0


# åœ¨GeologicalPINNç±»ä¸­æ·»åŠ RLæ”¯æŒ
def add_rl_support_to_pinn():
    """ä¸ºGeologicalPINNç±»æ·»åŠ RLæ”¯æŒ"""
    
    def setup_rl_time_step_optimizer(self, base_dt: float = 1e6):
        """è®¾ç½®RLæ—¶é—´æ­¥é•¿ä¼˜åŒ–å™¨"""
        self.rl_time_optimizer = RLTimeStepOptimizer(self, base_dt)
        print(f"âœ… å·²è®¾ç½®RLæ—¶é—´æ­¥é•¿ä¼˜åŒ–å™¨ï¼ŒåŸºç¡€æ—¶é—´æ­¥: {base_dt}")
    
    def setup_rl_inversion_agent(self, param_dim: int = 10):
        """è®¾ç½®RLåæ¼”æ™ºèƒ½ä½“"""
        self.rl_inversion_agent = InversionRLAgent(self, param_dim)
        print(f"âœ… å·²è®¾ç½®RLåæ¼”æ™ºèƒ½ä½“ï¼Œå‚æ•°ç»´åº¦: {param_dim}")
    
    def optimize_time_step_with_rl(self, state_history: List[Dict], max_steps: int = 1000):
        """ä½¿ç”¨RLä¼˜åŒ–æ—¶é—´æ­¥é•¿"""
        if not hasattr(self, 'rl_time_optimizer'):
            self.setup_rl_time_step_optimizer()
        return self.rl_time_optimizer.optimize(state_history, max_steps)
    
    def invert_parameters_with_rl(self, obs_data: np.ndarray, init_params: Dict[str, np.ndarray], 
                                 iterations: int = 100):
        """ä½¿ç”¨RLè¿›è¡Œå‚æ•°åæ¼”"""
        if not hasattr(self, 'rl_inversion_agent'):
            self.setup_rl_inversion_agent()
        return self.rl_inversion_agent.invert(obs_data, init_params, iterations)
    
    # åŠ¨æ€æ·»åŠ æ–¹æ³•åˆ°GeologicalPINNç±»
    GeologicalPINN.setup_rl_time_step_optimizer = setup_rl_time_step_optimizer
    GeologicalPINN.setup_rl_inversion_agent = setup_rl_inversion_agent
    GeologicalPINN.optimize_time_step_with_rl = optimize_time_step_with_rl
    GeologicalPINN.invert_parameters_with_rl = invert_parameters_with_rl


# åˆå§‹åŒ–æ—¶æ·»åŠ RLæ”¯æŒ
add_rl_support_to_pinn()


if __name__ == "__main__":
    demo_geological_ml()
