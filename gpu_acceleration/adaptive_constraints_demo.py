"""
è‡ªé€‚åº”ç‰©ç†çº¦æŸä¸å¼ºåŒ–å­¦ä¹ æ§åˆ¶å™¨æ¼”ç¤º
å±•ç¤ºå¦‚ä½•åœ¨ç°æœ‰çš„ geological_ml_framework.py ä¸­æ·»åŠ åŠ¨æ€æƒé‡è°ƒæ•´åŠŸèƒ½
"""

import numpy as np
import time
import warnings
from typing import Dict, List, Tuple, Optional, Callable, Union, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod

# æ·±åº¦å­¦ä¹ ç›¸å…³ä¾èµ–
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    from torch.utils.data import DataLoader, TensorDataset
    from torch.nn.parameter import Parameter
    HAS_PYTORCH = True
except ImportError:
    HAS_PYTORCH = False
    torch = None
    nn = None
    optim = None
    warnings.warn("PyTorch not available. Geological ML features will be limited.")

# å¯é€‰ä¾èµ–æ£€æŸ¥
try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.vec_env import DummyVecEnv
    HAS_STABLE_BASELINES3 = True
except ImportError:
    HAS_STABLE_BASELINES3 = False
    warnings.warn("stable-baselines3 not available. RL features will be limited.")


class AdaptivePhysicalConstraint:
    """è‡ªé€‚åº”ç‰©ç†çº¦æŸç±» - åŠ¨æ€è°ƒæ•´æƒé‡"""
    
    def __init__(self, name: str, equation: Callable, initial_weight: float = 1.0,
                 min_weight: float = 0.01, max_weight: float = 10.0,
                 adaptation_rate: float = 0.1):
        self.name = name
        self.equation = equation
        self.current_weight = initial_weight
        self.min_weight = min_weight
        self.max_weight = max_weight
        self.adaptation_rate = adaptation_rate
        
        # å†å²è®°å½•
        self.residual_history = []
        self.weight_history = []
        self.adaptation_history = []
        
        # è‡ªé€‚åº”å‚æ•°
        self.target_residual = 1e-6
        self.residual_window = 10  # ç”¨äºè®¡ç®—å¹³å‡æ®‹å·®çš„çª—å£å¤§å°
        
    def compute_residual(self, *args, **kwargs) -> float:
        """è®¡ç®—çº¦æŸæ®‹å·®"""
        try:
            residual = self.equation(*args, **kwargs)
            self.residual_history.append(residual)
            
            # ä¿æŒå†å²è®°å½•å¤§å°
            if len(self.residual_history) > 100:
                self.residual_history.pop(0)
                
            return float(residual)
        except Exception as e:
            warnings.warn(f"è®¡ç®—çº¦æŸ {self.name} æ®‹å·®å¤±è´¥: {str(e)}")
            return np.inf
    
    def adapt_weight(self, current_residual: float):
        """è‡ªé€‚åº”è°ƒæ•´æƒé‡"""
        if len(self.residual_history) < self.residual_window:
            return
        
        # è®¡ç®—æœ€è¿‘çª—å£å†…çš„å¹³å‡æ®‹å·®
        recent_residuals = self.residual_history[-self.residual_window:]
        avg_residual = np.mean(recent_residuals)
        
        # è®¡ç®—æ®‹å·®æ¯”ç‡
        residual_ratio = avg_residual / (self.target_residual + 1e-12)
        
        # è‡ªé€‚åº”è°ƒæ•´æƒé‡
        if residual_ratio > 1.0:  # æ®‹å·®è¿‡å¤§ï¼Œå¢åŠ æƒé‡
            weight_change = self.adaptation_rate * (residual_ratio - 1.0)
            new_weight = min(self.max_weight, self.current_weight * (1.0 + weight_change))
        else:  # æ®‹å·®è¾ƒå°ï¼Œé€‚å½“å‡å°‘æƒé‡
            weight_change = self.adaptation_rate * (1.0 - residual_ratio) * 0.5
            new_weight = max(self.min_weight, self.current_weight * (1.0 - weight_change))
        
        # è®°å½•è°ƒæ•´å†å²
        self.weight_history.append(self.current_weight)
        self.adaptation_history.append({
            'timestamp': time.time(),
            'old_weight': self.current_weight,
            'new_weight': new_weight,
            'residual_ratio': residual_ratio,
            'avg_residual': avg_residual
        })
        
        # æ›´æ–°æƒé‡
        self.current_weight = new_weight
        
        # ä¿æŒå†å²è®°å½•å¤§å°
        if len(self.weight_history) > 100:
            self.weight_history.pop(0)
        if len(self.adaptation_history) > 100:
            self.adaptation_history.pop(0)
    
    def get_adaptation_summary(self) -> Dict[str, Any]:
        """è·å–è‡ªé€‚åº”è°ƒæ•´æ‘˜è¦"""
        if not self.adaptation_history:
            return {}
        
        recent_adaptations = self.adaptation_history[-10:]  # æœ€è¿‘10æ¬¡è°ƒæ•´
        
        return {
            'name': self.name,
            'current_weight': self.current_weight,
            'initial_weight': self.weight_history[0] if self.weight_history else self.current_weight,
            'total_adaptations': len(self.adaptation_history),
            'recent_adaptations': recent_adaptations,
            'residual_trend': self._compute_residual_trend(),
            'weight_trend': self._compute_weight_trend()
        }
    
    def _compute_residual_trend(self) -> str:
        """è®¡ç®—æ®‹å·®è¶‹åŠ¿"""
        if len(self.residual_history) < 2:
            return "insufficient_data"
        
        recent = np.mean(self.residual_history[-10:])
        earlier = np.mean(self.residual_history[-20:-10]) if len(self.residual_history) >= 20 else self.residual_history[0]
        
        if recent < earlier * 0.8:
            return "decreasing"
        elif recent > earlier * 1.2:
            return "increasing"
        else:
            return "stable"
    
    def _compute_weight_trend(self) -> str:
        """è®¡ç®—æƒé‡è¶‹åŠ¿"""
        if len(self.weight_history) < 2:
            return "insufficient_data"
        
        recent = np.mean(self.weight_history[-10:])
        earlier = np.mean(self.weight_history[-20:-10]) if len(self.weight_history) >= 20 else self.weight_history[0]
        
        if recent > earlier * 1.1:
            return "increasing"
        elif recent < earlier * 0.9:
            return "decreasing"
        else:
            return "stable"


class RLConstraintController:
    """å¼ºåŒ–å­¦ä¹ çº¦æŸæ§åˆ¶å™¨ - è‡ªåŠ¨ä¼˜åŒ–ç‰©ç†çº¦æŸæƒé‡"""
    
    def __init__(self, constraints: List[AdaptivePhysicalConstraint],
                 state_dim: int = 10, action_dim: int = None,
                 learning_rate: float = 3e-4, gamma: float = 0.99):
        self.constraints = constraints
        self.state_dim = state_dim
        self.action_dim = action_dim or len(constraints)
        self.learning_rate = learning_rate
        self.gamma = gamma
        
        # å¼ºåŒ–å­¦ä¹ ç¯å¢ƒçŠ¶æ€
        self.current_state = None
        self.action_history = []
        self.reward_history = []
        self.state_history = []
        
        # åˆå§‹åŒ–RLä»£ç†
        self.agent = self._create_rl_agent()
        
        # æ§åˆ¶å‚æ•°
        self.control_frequency = 5  # æ¯5æ¬¡è¿­ä»£æ§åˆ¶ä¸€æ¬¡
        self.iteration_count = 0
        self.last_control_time = time.time()
        
    def _create_rl_agent(self):
        """åˆ›å»ºå¼ºåŒ–å­¦ä¹ ä»£ç†"""
        if not HAS_STABLE_BASELINES3:
            warnings.warn("stable-baselines3 not available, using simple controller")
            return None
        
        try:
            # åˆ›å»ºç®€å•çš„ç¯å¢ƒåŒ…è£…å™¨
            class ConstraintEnvironment:
                def __init__(self, controller):
                    self.controller = controller
                    self.action_space = self.controller.action_dim
                    self.observation_space = self.controller.state_dim
                
                def reset(self):
                    return np.zeros(self.controller.state_dim)
                
                def step(self, action):
                    # æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›å¥–åŠ±
                    reward = self.controller._execute_action(action)
                    next_state = self.controller._get_state()
                    done = False
                    info = {}
                    return next_state, reward, done, info
            
            env = DummyVecEnv([lambda: ConstraintEnvironment(self)])
            
            # åˆ›å»ºPPOä»£ç†
            agent = PPO(
                "MlpPolicy",
                env,
                learning_rate=self.learning_rate,
                gamma=self.gamma,
                verbose=0
            )
            
            return agent
            
        except Exception as e:
            warnings.warn(f"åˆ›å»ºRLä»£ç†å¤±è´¥: {str(e)}")
            return None
    
    def _get_state(self) -> np.ndarray:
        """è·å–å½“å‰çŠ¶æ€å‘é‡"""
        state = []
        
        # çº¦æŸæ®‹å·®
        for constraint in self.constraints:
            if constraint.residual_history:
                recent_residuals = constraint.residual_history[-5:]  # æœ€è¿‘5ä¸ªæ®‹å·®
                avg_residual = np.mean(recent_residuals)
                state.append(avg_residual)
            else:
                state.append(0.0)
        
        # æƒé‡ä¿¡æ¯
        for constraint in self.constraints:
            state.append(constraint.current_weight)
        
        # è‡ªé€‚åº”å†å²
        for constraint in self.constraints:
            if constraint.adaptation_history:
                recent_adaptations = constraint.adaptation_history[-3:]  # æœ€è¿‘3æ¬¡è°ƒæ•´
                avg_weight_change = np.mean([abs(a['new_weight'] - a['old_weight']) 
                                          for a in recent_adaptations])
                state.append(avg_weight_change)
            else:
                state.append(0.0)
        
        # å¡«å……åˆ°æŒ‡å®šç»´åº¦
        while len(state) < self.state_dim:
            state.append(0.0)
        
        return np.array(state[:self.state_dim])
    
    def _execute_action(self, action: np.ndarray) -> float:
        """æ‰§è¡ŒåŠ¨ä½œå¹¶è®¡ç®—å¥–åŠ±"""
        if len(action) != len(self.constraints):
            return 0.0
        
        # åº”ç”¨æƒé‡è°ƒæ•´
        old_weights = [c.current_weight for c in self.constraints]
        
        for i, constraint in enumerate(self.constraints):
            # å°†åŠ¨ä½œæ˜ å°„åˆ°æƒé‡è°ƒæ•´
            weight_adjustment = np.tanh(action[i]) * 0.2  # é™åˆ¶è°ƒæ•´å¹…åº¦
            new_weight = constraint.current_weight * (1.0 + weight_adjustment)
            
            # åº”ç”¨çº¦æŸ
            new_weight = np.clip(new_weight, constraint.min_weight, constraint.max_weight)
            constraint.current_weight = new_weight
        
        # è®¡ç®—å¥–åŠ±ï¼šåŸºäºæ®‹å·®æ”¹å–„
        total_residual_improvement = 0.0
        for constraint in self.constraints:
            if len(constraint.residual_history) >= 2:
                old_residual = constraint.residual_history[-2]
                new_residual = constraint.compute_residual()  # é‡æ–°è®¡ç®—
                improvement = old_residual - new_residual
                total_residual_improvement += improvement
        
        # å½’ä¸€åŒ–å¥–åŠ±
        reward = total_residual_improvement / (len(self.constraints) + 1e-6)
        
        return reward
    
    def control_constraints(self):
        """æ§åˆ¶çº¦æŸæƒé‡"""
        self.iteration_count += 1
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ§åˆ¶
        if self.iteration_count % self.control_frequency != 0:
            return
        
        current_time = time.time()
        if current_time - self.last_control_time < 1.0:  # è‡³å°‘é—´éš”1ç§’
            return
        
        try:
            # è·å–å½“å‰çŠ¶æ€
            current_state = self._get_state()
            self.state_history.append(current_state)
            
            if self.agent is not None:
                # ä½¿ç”¨RLä»£ç†é€‰æ‹©åŠ¨ä½œ
                action, _ = self.agent.predict(current_state, deterministic=True)
                reward = self._execute_action(action)
                
                # è®°å½•å†å²
                self.action_history.append(action)
                self.reward_history.append(reward)
                
                # è®­ç»ƒä»£ç†ï¼ˆå¦‚æœæ•°æ®è¶³å¤Ÿï¼‰
                if len(self.state_history) >= 10:
                    self._train_agent()
            else:
                # ä½¿ç”¨ç®€å•çš„å¯å‘å¼æ§åˆ¶
                self._heuristic_control()
            
            self.last_control_time = current_time
            
        except Exception as e:
            warnings.warn(f"çº¦æŸæ§åˆ¶å¤±è´¥: {str(e)}")
    
    def _heuristic_control(self):
        """å¯å‘å¼æ§åˆ¶ç­–ç•¥"""
        for constraint in self.constraints:
            if constraint.residual_history:
                recent_residuals = constraint.residual_history[-5:]
                avg_residual = np.mean(recent_residuals)
                
                # åŸºäºæ®‹å·®å¤§å°è°ƒæ•´æƒé‡
                if avg_residual > constraint.target_residual * 2:
                    # æ®‹å·®è¿‡å¤§ï¼Œå¢åŠ æƒé‡
                    constraint.current_weight = min(
                        constraint.max_weight,
                        constraint.current_weight * 1.2
                    )
                elif avg_residual < constraint.target_residual * 0.5:
                    # æ®‹å·®è¾ƒå°ï¼Œé€‚å½“å‡å°‘æƒé‡
                    constraint.current_weight = max(
                        constraint.min_weight,
                        constraint.current_weight * 0.9
                    )
    
    def _train_agent(self):
        """è®­ç»ƒRLä»£ç†"""
        if self.agent is None or len(self.state_history) < 10:
            return
        
        try:
            # åˆ›å»ºè®­ç»ƒæ•°æ®
            states = np.array(self.state_history[-10:])
            actions = np.array(self.action_history[-10:])
            rewards = np.array(self.reward_history[-10:])
            
            # è®­ç»ƒä»£ç†
            self.agent.learn(total_timesteps=100)
            
        except Exception as e:
            warnings.warn(f"è®­ç»ƒRLä»£ç†å¤±è´¥: {str(e)}")
    
    def get_control_summary(self) -> Dict[str, Any]:
        """è·å–æ§åˆ¶æ‘˜è¦"""
        return {
            'total_iterations': self.iteration_count,
            'control_frequency': self.control_frequency,
            'last_control_time': self.last_control_time,
            'total_actions': len(self.action_history),
            'total_rewards': len(self.reward_history),
            'avg_reward': np.mean(self.reward_history) if self.reward_history else 0.0,
            'rl_agent_active': self.agent is not None,
            'constraint_states': [c.get_adaptation_summary() for c in self.constraints]
        }


def demo_adaptive_constraints():
    """æ¼”ç¤ºè‡ªé€‚åº”ç‰©ç†çº¦æŸåŠŸèƒ½"""
    print("=== è‡ªé€‚åº”ç‰©ç†çº¦æŸæ¼”ç¤º ===")
    
    # 1. åˆ›å»ºç‰©ç†çº¦æŸæ–¹ç¨‹
    def darcy_equation(x, y_pred):
        """DarcyæµåŠ¨æ–¹ç¨‹çº¦æŸ"""
        # ç®€åŒ–çš„Darcyæ–¹ç¨‹æ®‹å·®è®¡ç®—
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è®¡ç®—çœŸå®çš„PDEæ®‹å·®
        return np.random.normal(0, 1e-6)  # æ¨¡æ‹Ÿæ®‹å·®
    
    def heat_equation(x, y_pred):
        """çƒ­ä¼ å¯¼æ–¹ç¨‹çº¦æŸ"""
        return np.random.normal(0, 1e-5)  # æ¨¡æ‹Ÿæ®‹å·®
    
    # 2. åˆ›å»ºè‡ªé€‚åº”çº¦æŸ
    darcy_constraint = AdaptivePhysicalConstraint(
        name="Darcyæ–¹ç¨‹",
        equation=darcy_equation,
        initial_weight=1.0,
        min_weight=0.01,
        max_weight=5.0,
        adaptation_rate=0.1
    )
    
    heat_constraint = AdaptivePhysicalConstraint(
        name="çƒ­ä¼ å¯¼æ–¹ç¨‹",
        equation=heat_equation,
        initial_weight=0.5,
        min_weight=0.01,
        max_weight=3.0,
        adaptation_rate=0.08
    )
    
    print(f"âœ… åˆ›å»ºç‰©ç†çº¦æŸ: {darcy_constraint.name}, {heat_constraint.name}")
    
    # 3. åˆ›å»ºå¼ºåŒ–å­¦ä¹ æ§åˆ¶å™¨
    controller = RLConstraintController(
        constraints=[darcy_constraint, heat_constraint],
        state_dim=6,  # 2ä¸ªçº¦æŸ Ã— 3ä¸ªçŠ¶æ€ç»´åº¦
        action_dim=2, # 2ä¸ªçº¦æŸçš„æƒé‡è°ƒæ•´
        learning_rate=1e-3,
        gamma=0.99
    )
    
    print(f"âœ… åˆ›å»ºRLæ§åˆ¶å™¨ï¼Œç®¡ç† {len(controller.constraints)} ä¸ªçº¦æŸ")
    
    # 4. æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    print("\nğŸ”„ å¼€å§‹æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹...")
    
    for step in range(20):
        # æ¨¡æ‹Ÿè®¡ç®—æ®‹å·®
        darcy_residual = darcy_constraint.compute_residual()
        heat_residual = heat_constraint.compute_residual()
        
        # è‡ªé€‚åº”è°ƒæ•´æƒé‡
        darcy_constraint.adapt_weight(darcy_residual)
        heat_constraint.adapt_weight(heat_residual)
        
        # æ§åˆ¶å™¨è‡ªåŠ¨è°ƒæ•´
        controller.control_constraints()
        
        # æ¯5æ­¥æ˜¾ç¤ºçŠ¶æ€
        if step % 5 == 0:
            print(f"\næ­¥éª¤ {step}:")
            print(f"  Darcyçº¦æŸ: æ®‹å·®={darcy_residual:.2e}, æƒé‡={darcy_constraint.current_weight:.4f}")
            print(f"  çƒ­ä¼ å¯¼çº¦æŸ: æ®‹å·®={heat_residual:.2e}, æƒé‡={heat_constraint.current_weight:.4f}")
            
            # è·å–çº¦æŸæ‘˜è¦
            darcy_summary = darcy_constraint.get_adaptation_summary()
            heat_summary = heat_constraint.get_adaptation_summary()
            
            print(f"  Darcyè¶‹åŠ¿: æ®‹å·®={darcy_summary['residual_trend']}, æƒé‡={darcy_summary['weight_trend']}")
            print(f"  çƒ­ä¼ å¯¼è¶‹åŠ¿: æ®‹å·®={heat_summary['residual_trend']}, æƒé‡={heat_summary['weight_trend']}")
    
    # 5. è·å–æœ€ç»ˆæ‘˜è¦
    print("\nğŸ“Š è®­ç»ƒå®Œæˆï¼Œè·å–æ‘˜è¦ä¿¡æ¯:")
    
    controller_summary = controller.get_control_summary()
    print(f"æ§åˆ¶å™¨æ‘˜è¦:")
    print(f"  æ€»è¿­ä»£æ¬¡æ•°: {controller_summary['total_iterations']}")
    print(f"  æ€»åŠ¨ä½œæ•°: {controller_summary['total_actions']}")
    print(f"  å¹³å‡å¥–åŠ±: {controller_summary['avg_reward']:.6f}")
    print(f"  RLä»£ç†çŠ¶æ€: {'æ¿€æ´»' if controller_summary['rl_agent_active'] else 'æœªæ¿€æ´»'}")
    
    # çº¦æŸæ‘˜è¦
    for constraint in [darcy_constraint, heat_constraint]:
        summary = constraint.get_adaptation_summary()
        print(f"\n{constraint.name} çº¦æŸæ‘˜è¦:")
        print(f"  å½“å‰æƒé‡: {summary['current_weight']:.4f}")
        print(f"  åˆå§‹æƒé‡: {summary['initial_weight']:.4f}")
        print(f"  æ€»è°ƒæ•´æ¬¡æ•°: {summary['total_adaptations']}")
        print(f"  æ®‹å·®è¶‹åŠ¿: {summary['residual_trend']}")
        print(f"  æƒé‡è¶‹åŠ¿: {summary['weight_trend']}")
    
    print("\nğŸ‰ è‡ªé€‚åº”ç‰©ç†çº¦æŸæ¼”ç¤ºå®Œæˆï¼")


def demo_integration_with_existing_pinn():
    """æ¼”ç¤ºå¦‚ä½•ä¸ç°æœ‰PINNé›†æˆ"""
    print("\n=== ä¸ç°æœ‰PINNé›†æˆæ¼”ç¤º ===")
    
    if not HAS_PYTORCH:
        print("âŒ PyTorchä¸å¯ç”¨ï¼Œè·³è¿‡PINNé›†æˆæ¼”ç¤º")
        return
    
    # 1. åˆ›å»ºç‰©ç†çº¦æŸ
    def darcy_equation(x, y_pred):
        """DarcyæµåŠ¨æ–¹ç¨‹çº¦æŸ"""
        return np.random.normal(0, 1e-6)
    
    darcy_constraint = AdaptivePhysicalConstraint(
        name="Darcyæ–¹ç¨‹",
        equation=darcy_equation,
        initial_weight=1.0
    )
    
    # 2. åˆ›å»ºçº¦æŸæ§åˆ¶å™¨
    controller = RLConstraintController(
        constraints=[darcy_constraint],
        state_dim=3,
        action_dim=1
    )
    
    print("âœ… åˆ›å»ºçº¦æŸç³»ç»Ÿ")
    
    # 3. æ¨¡æ‹ŸPINNè®­ç»ƒè¿‡ç¨‹
    print("ğŸ”„ æ¨¡æ‹ŸPINNè®­ç»ƒè¿‡ç¨‹...")
    
    for epoch in range(10):
        # æ¨¡æ‹Ÿè®¡ç®—æ®‹å·®
        residual = darcy_constraint.compute_residual()
        
        # è‡ªé€‚åº”è°ƒæ•´æƒé‡
        darcy_constraint.adapt_weight(residual)
        
        # æ§åˆ¶å™¨è°ƒæ•´
        controller.control_constraints()
        
        if epoch % 2 == 0:
            print(f"Epoch {epoch}: æ®‹å·®={residual:.2e}, æƒé‡={darcy_constraint.current_weight:.4f}")
    
    print("âœ… PINNé›†æˆæ¼”ç¤ºå®Œæˆ")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    demo_adaptive_constraints()
    demo_integration_with_existing_pinn()
