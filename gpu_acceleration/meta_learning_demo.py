"""
åœ°çƒåŠ¨åŠ›å­¦å…ƒå­¦ä¹ æ¼”ç¤º
å±•ç¤ºå¦‚ä½•ä½¿ç”¨å…ƒå­¦ä¹ å¿«é€Ÿé€‚é…ä¸åŒåœ°è´¨åœºæ™¯
"""

import numpy as np
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any

# å¯¼å…¥åœ°è´¨MLæ¡†æ¶
try:
    from geological_ml_framework import (
        GeologicalPINN, GeologicalConfig, GeologicalPhysicsEquations,
        GeodynamicMetaLearner, GeodynamicMetaTask
    )
    print("âœ… æˆåŠŸå¯¼å…¥åœ°è´¨MLæ¡†æ¶")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
    exit(1)


def demo_meta_learning_basic():
    """æ¼”ç¤ºåŸºæœ¬å…ƒå­¦ä¹ åŠŸèƒ½"""
    print("\nğŸš€ æ¼”ç¤ºåŸºæœ¬å…ƒå­¦ä¹ åŠŸèƒ½...")
    
    # 1. åˆ›å»ºåœ°è´¨é…ç½®
    config = GeologicalConfig(
        reference_viscosity=1e21,
        thermal_expansion=3e-5,
        gravity=9.81,
        mu0=0.6,
        a=0.01,
        b=0.005
    )
    
    # 2. åˆ›å»ºPINNæ¨¡å‹
    input_dim = 4  # ç©ºé—´åæ ‡ + æ¸©åº¦
    hidden_dims = [64, 128, 64]
    output_dim = 3  # é€Ÿåº¦åœº + å‹åŠ›
    
    pinn = GeologicalPINN(
        input_dim=input_dim,
        hidden_dims=hidden_dims,
        output_dim=output_dim,
        geological_config=config
    )
    
    # 3. åˆ›å»ºå…ƒå­¦ä¹ å™¨
    meta_learner = GeodynamicMetaLearner(
        pinn_model=pinn,
        meta_learning_rate=0.001,
        inner_learning_rate=0.005,
        adaptation_steps=3
    )
    
    # 4. åˆ›å»ºå…ƒä»»åŠ¡
    meta_tasks = meta_learner.create_geodynamic_meta_tasks()
    print(f"   åˆ›å»ºäº† {len(meta_tasks)} ä¸ªå…ƒä»»åŠ¡:")
    for task in meta_tasks:
        print(f"     - {task.name}: {task.geological_conditions}")
    
    return pinn, meta_learner, meta_tasks


def demo_meta_training():
    """æ¼”ç¤ºå…ƒå­¦ä¹ è®­ç»ƒè¿‡ç¨‹"""
    print("\nğŸ”§ æ¼”ç¤ºå…ƒå­¦ä¹ è®­ç»ƒè¿‡ç¨‹...")
    
    pinn, meta_learner, meta_tasks = demo_meta_learning_basic()
    
    # å¼€å§‹å…ƒå­¦ä¹ è®­ç»ƒ
    print("\n   å¼€å§‹å…ƒå­¦ä¹ è®­ç»ƒ...")
    meta_loss_history, adaptation_history = meta_learner.meta_train_geodynamics(
        meta_tasks=meta_tasks,
        meta_epochs=20,  # å‡å°‘è½®æ•°ä»¥å¿«é€Ÿæ¼”ç¤º
        task_samples=500  # å‡å°‘æ ·æœ¬æ•°ä»¥å¿«é€Ÿæ¼”ç¤º
    )
    
    # æ˜¾ç¤ºè®­ç»ƒç»“æœ
    print(f"\n   å…ƒå­¦ä¹ è®­ç»ƒå®Œæˆ!")
    print(f"   æœ€ç»ˆå…ƒæŸå¤±: {meta_loss_history[-1]:.6f}")
    print(f"   æŸå¤±å‡å°‘: {meta_loss_history[0] - meta_loss_history[-1]:.6f}")
    
    return pinn, meta_learner, meta_loss_history, adaptation_history


def demo_rapid_adaptation():
    """æ¼”ç¤ºå¿«é€Ÿé€‚é…åˆ°æ–°åŒºåŸŸ"""
    print("\nğŸ”„ æ¼”ç¤ºå¿«é€Ÿé€‚é…åˆ°æ–°åŒºåŸŸ...")
    
    pinn, meta_learner, meta_loss_history, adaptation_history = demo_meta_training()
    
    # æ¨¡æ‹Ÿæ–°åŒºåŸŸæ•°æ®ï¼ˆå–œé©¬æ‹‰é›…ç¢°æ’å¸¦ï¼‰
    def generate_himalaya_data(num_samples):
        """ç”Ÿæˆå–œé©¬æ‹‰é›…ç¢°æ’å¸¦æ•°æ®"""
        X = torch.randn(num_samples, 4)
        # å–œé©¬æ‹‰é›…ç‰¹å¾ï¼šé«˜æµ·æ‹”ã€ä½æ¸©ã€å¤æ‚å˜å½¢
        X[:, 3] = 200 + 100 * torch.randn(num_samples)  # ä½æ¸©
        y = torch.randn(num_samples, 3)
        y[:, 0] = 0.01 + 0.005 * torch.randn(num_samples)  # å°å˜å½¢
        return X, y
    
    new_region_data = generate_himalaya_data(300)
    
    # å¿«é€Ÿé€‚é…
    print("\n   å¿«é€Ÿé€‚é…åˆ°å–œé©¬æ‹‰é›…åŒºåŸŸ...")
    adaptation_result = meta_learner.adapt_to_new_region(
        new_region_data=new_region_data,
        adaptation_steps=5
    )
    
    print(f"\n   é€‚é…ç»“æœ:")
    print(f"     - é€‚é…æ­¥æ•°: {adaptation_result['adaptation_steps']}")
    print(f"     - æœ€ç»ˆæ•°æ®æŸå¤±: {adaptation_result['final_data_loss']:.6f}")
    print(f"     - æœ€ç»ˆç‰©ç†æŸå¤±: {adaptation_result['final_physics_loss']:.6f}")
    print(f"     - æ€»æŸå¤±å‡å°‘: {adaptation_result['total_loss_reduction']:.6f}")
    
    return adaptation_result


def demo_cross_tectonic_domain():
    """æ¼”ç¤ºè·¨æ„é€ åŸŸæ¨¡æ‹Ÿ"""
    print("\nğŸŒ æ¼”ç¤ºè·¨æ„é€ åŸŸæ¨¡æ‹Ÿ...")
    
    pinn, meta_learner, meta_loss_history, adaptation_history = demo_meta_training()
    
    # æ¨¡æ‹Ÿä»å®‰ç¬¬æ–¯å±±è„‰åˆ°å–œé©¬æ‹‰é›…çš„è¿ç§»
    print("\n   æ¨¡æ‹Ÿä»å®‰ç¬¬æ–¯å±±è„‰åˆ°å–œé©¬æ‹‰é›…çš„è¿ç§»...")
    
    # å®‰ç¬¬æ–¯å±±è„‰æ•°æ®ï¼ˆä¿¯å†²å¸¦ï¼‰
    def generate_andes_data(num_samples):
        """ç”Ÿæˆå®‰ç¬¬æ–¯å±±è„‰æ•°æ®"""
        X = torch.randn(num_samples, 4)
        X[:, 3] = 350 + 120 * torch.randn(num_samples)  # ä¸­ç­‰æ¸©åº¦
        y = torch.randn(num_samples, 3)
        y[:, 0] = -0.03 + 0.015 * torch.randn(num_samples)  # å‹ç¼©
        return X, y
    
    # å–œé©¬æ‹‰é›…æ•°æ®ï¼ˆç¢°æ’å¸¦ï¼‰
    def generate_himalaya_data(num_samples):
        """ç”Ÿæˆå–œé©¬æ‹‰é›…æ•°æ®"""
        X = torch.randn(num_samples, 4)
        X[:, 3] = 180 + 80 * torch.randn(num_samples)  # ä½æ¸©
        y = torch.randn(num_samples, 3)
        y[:, 0] = 0.008 + 0.004 * torch.randn(num_samples)  # å°å˜å½¢
        return X, y
    
    # æµ‹è¯•è¿ç§»æ•ˆæœ
    andes_data = generate_andes_data(200)
    himalaya_data = generate_himalaya_data(200)
    
    # åœ¨å®‰ç¬¬æ–¯æ•°æ®ä¸Šè®­ç»ƒ
    print("    åœ¨å®‰ç¬¬æ–¯å±±è„‰æ•°æ®ä¸Šè®­ç»ƒ...")
    andes_result = meta_learner.adapt_to_new_region(andes_data, adaptation_steps=3)
    
    # åœ¨å–œé©¬æ‹‰é›…æ•°æ®ä¸Šå¿«é€Ÿé€‚é…
    print("    åœ¨å–œé©¬æ‹‰é›…æ•°æ®ä¸Šå¿«é€Ÿé€‚é…...")
    himalaya_result = meta_learner.adapt_to_new_region(himalaya_data, adaptation_steps=2)
    
    print(f"\n   è·¨æ„é€ åŸŸè¿ç§»ç»“æœ:")
    print(f"     - å®‰ç¬¬æ–¯è®­ç»ƒæŸå¤±å‡å°‘: {andes_result['total_loss_reduction']:.6f}")
    print(f"     - å–œé©¬æ‹‰é›…é€‚é…æŸå¤±å‡å°‘: {himalaya_result['total_loss_reduction']:.6f}")
    print(f"     - è¿ç§»æ•ˆç‡: {himalaya_result['total_loss_reduction'] / andes_result['total_loss_reduction']:.2f}")
    
    return andes_result, himalaya_result


def demo_multi_scale_transfer():
    """æ¼”ç¤ºå¤šå°ºåº¦è¿ç§»"""
    print("\nğŸ“ æ¼”ç¤ºå¤šå°ºåº¦è¿ç§»...")
    
    pinn, meta_learner, meta_loss_history, adaptation_history = demo_meta_training()
    
    # æ¨¡æ‹Ÿä»åŒºåŸŸå°ºåº¦(100km)åˆ°å±€éƒ¨æ–­å±‚å°ºåº¦(10km)çš„è¿ç§»
    print("\n   æ¨¡æ‹Ÿä»åŒºåŸŸå°ºåº¦(100km)åˆ°å±€éƒ¨æ–­å±‚å°ºåº¦(10km)çš„è¿ç§»...")
    
    # åŒºåŸŸå°ºåº¦æ•°æ®ï¼ˆç²—ç½‘æ ¼ï¼‰
    def generate_regional_data(num_samples):
        """ç”ŸæˆåŒºåŸŸå°ºåº¦æ•°æ®"""
        X = torch.randn(num_samples, 4)
        X[:, :3] *= 100  # 100kmå°ºåº¦
        X[:, 3] = 400 + 150 * torch.randn(num_samples)
        y = torch.randn(num_samples, 3)
        y[:, 0] = 0.05 + 0.02 * torch.randn(num_samples)
        return X, y
    
    # å±€éƒ¨å°ºåº¦æ•°æ®ï¼ˆç»†ç½‘æ ¼ï¼‰
    def generate_local_data(num_samples):
        """ç”Ÿæˆå±€éƒ¨å°ºåº¦æ•°æ®"""
        X = torch.randn(num_samples, 4)
        X[:, :3] *= 10   # 10kmå°ºåº¦
        X[:, 3] = 350 + 100 * torch.randn(num_samples)
        y = torch.randn(num_samples, 3)
        y[:, 0] = 0.02 + 0.01 * torch.randn(num_samples)
        return X, y
    
    # æµ‹è¯•å¤šå°ºåº¦è¿ç§»
    regional_data = generate_regional_data(300)
    local_data = generate_local_data(400)
    
    # åœ¨åŒºåŸŸå°ºåº¦ä¸Šè®­ç»ƒ
    print("    åœ¨åŒºåŸŸå°ºåº¦ä¸Šè®­ç»ƒ...")
    regional_result = meta_learner.adapt_to_new_region(regional_data, adaptation_steps=4)
    
    # åœ¨å±€éƒ¨å°ºåº¦ä¸Šå¿«é€Ÿé€‚é…
    print("    åœ¨å±€éƒ¨å°ºåº¦ä¸Šå¿«é€Ÿé€‚é…...")
    local_result = meta_learner.adapt_to_new_region(local_data, adaptation_steps=2)
    
    print(f"\n   å¤šå°ºåº¦è¿ç§»ç»“æœ:")
    print(f"     - åŒºåŸŸå°ºåº¦è®­ç»ƒæŸå¤±å‡å°‘: {regional_result['total_loss_reduction']:.6f}")
    print(f"     - å±€éƒ¨å°ºåº¦é€‚é…æŸå¤±å‡å°‘: {local_result['total_loss_reduction']:.6f}")
    print(f"     - å°ºåº¦è¿ç§»æ•ˆç‡: {local_result['total_loss_reduction'] / regional_result['total_loss_reduction']:.2f}")
    
    return regional_result, local_result


def demo_meta_learning_monitoring():
    """æ¼”ç¤ºå…ƒå­¦ä¹ ç›‘æ§å’Œåˆ†æ"""
    print("\nğŸ“Š æ¼”ç¤ºå…ƒå­¦ä¹ ç›‘æ§å’Œåˆ†æ...")
    
    pinn, meta_learner, meta_loss_history, adaptation_history = demo_meta_training()
    
    # è·å–å…ƒå­¦ä¹ çŠ¶æ€
    status = meta_learner.get_meta_learning_status()
    
    print(f"\n   å…ƒå­¦ä¹ çŠ¶æ€:")
    print(f"     - å…ƒå­¦ä¹ ç‡: {status['meta_learning_rate']}")
    print(f"     - å†…å¾ªç¯å­¦ä¹ ç‡: {status['inner_learning_rate']}")
    print(f"     - å†…å¾ªç¯æ­¥æ•°: {status['adaptation_steps']}")
    print(f"     - å…ƒå­¦ä¹ è½®æ•°: {len(status['meta_loss_history'])}")
    
    # åˆ†æä»»åŠ¡æ€§èƒ½
    print(f"\n   ä»»åŠ¡æ€§èƒ½åˆ†æ:")
    for task_name, performance in status['task_performance'].items():
        if 'epoch_19' in task_name:  # æœ€åä¸€è½®
            print(f"     - {task_name}:")
            print(f"       éªŒè¯æŸå¤±: {performance['validation_loss']:.6f}")
            print(f"       æœ€ç»ˆä»»åŠ¡æŸå¤±: {performance['final_task_loss']:.6f}")
    
    # å¯è§†åŒ–å…ƒå­¦ä¹ è¿‡ç¨‹
    try:
        plt.figure(figsize=(12, 8))
        
        # å…ƒæŸå¤±å†å²
        plt.subplot(2, 2, 1)
        plt.plot(status['meta_loss_history'])
        plt.title('å…ƒæŸå¤±å†å²')
        plt.xlabel('å…ƒå­¦ä¹ è½®æ¬¡')
        plt.ylabel('å…ƒæŸå¤±')
        plt.grid(True)
        
        # ä»»åŠ¡æŸå¤±å¯¹æ¯”
        plt.subplot(2, 2, 2)
        task_names = list(set([name.split('_epoch_')[0] for name in status['task_performance'].keys()]))
        final_losses = []
        for task_name in task_names:
            task_perfs = [v for k, v in status['task_performance'].items() if k.startswith(task_name)]
            if task_perfs:
                final_losses.append(task_perfs[-1]['final_task_loss'])
        
        plt.bar(task_names, final_losses)
        plt.title('å„ä»»åŠ¡æœ€ç»ˆæŸå¤±')
        plt.ylabel('æŸå¤±å€¼')
        plt.xticks(rotation=45)
        
        # éªŒè¯æŸå¤±å¯¹æ¯”
        plt.subplot(2, 2, 3)
        val_losses = []
        for task_name in task_names:
            task_perfs = [v for k, v in status['task_performance'].items() if k.startswith(task_name)]
            if task_perfs:
                val_losses.append(task_perfs[-1]['validation_loss'])
        
        plt.bar(task_names, val_losses)
        plt.title('å„ä»»åŠ¡éªŒè¯æŸå¤±')
        plt.ylabel('æŸå¤±å€¼')
        plt.xticks(rotation=45)
        
        # æŸå¤±å‡å°‘è¶‹åŠ¿
        plt.subplot(2, 2, 4)
        loss_reductions = []
        for task_name in task_names:
            task_perfs = [v for k, v in status['task_performance'].items() if k.startswith(task_name)]
            if task_perfs:
                first_loss = task_perfs[0]['final_task_loss']
                last_loss = task_perfs[-1]['final_task_loss']
                loss_reductions.append(first_loss - last_loss)
        
        plt.bar(task_names, loss_reductions)
        plt.title('å„ä»»åŠ¡æŸå¤±å‡å°‘')
        plt.ylabel('æŸå¤±å‡å°‘å€¼')
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.savefig('meta_learning_analysis.png', dpi=300, bbox_inches='tight')
        print(f"\n   å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜ä¸º: meta_learning_analysis.png")
        
    except Exception as e:
        print(f"   å¯è§†åŒ–å¤±è´¥: {e}")
    
    return status


def demo_integration_with_existing_pinn():
    """æ¼”ç¤ºä¸ç°æœ‰PINNçš„é›†æˆ"""
    print("\nğŸ”— æ¼”ç¤ºä¸ç°æœ‰PINNçš„é›†æˆ...")
    
    # åˆ›å»ºPINNæ¨¡å‹
    config = GeologicalConfig()
    pinn = GeologicalPINN(
        input_dim=4,
        hidden_dims=[32, 64, 32],
        output_dim=3,
        geological_config=config
    )
    
    # ç›´æ¥ä½¿ç”¨PINNçš„å…ƒå­¦ä¹ æ–¹æ³•
    print("\n   ä½¿ç”¨PINNå†…ç½®çš„å…ƒå­¦ä¹ æ–¹æ³•...")
    
    # åˆ›å»ºå…ƒä»»åŠ¡
    meta_learner = GeodynamicMetaLearner(pinn)
    meta_tasks = meta_learner.create_geodynamic_meta_tasks()
    
    # é€šè¿‡PINNè¿›è¡Œå…ƒå­¦ä¹ 
    print("    å¼€å§‹å…ƒå­¦ä¹ è®­ç»ƒ...")
    meta_loss_history, adaptation_history = pinn.meta_train_geodynamics(
        meta_tasks=meta_tasks,
        meta_epochs=15,
        task_samples=400
    )
    
    print(f"    å…ƒå­¦ä¹ å®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {meta_loss_history[-1]:.6f}")
    
    # æµ‹è¯•å¿«é€Ÿé€‚é…
    print("    æµ‹è¯•å¿«é€Ÿé€‚é…åˆ°æ–°åŒºåŸŸ...")
    new_data = (torch.randn(100, 4), torch.randn(100, 3))
    adaptation_result = pinn.adapt_to_new_region(new_data, adaptation_steps=3)
    
    print(f"    é€‚é…å®Œæˆï¼ŒæŸå¤±å‡å°‘: {adaptation_result['total_loss_reduction']:.6f}")
    
    # è·å–å…ƒå­¦ä¹ çŠ¶æ€
    status = pinn.get_meta_learning_status()
    print(f"    å…ƒå­¦ä¹ çŠ¶æ€: {len(status['meta_loss_history'])} è½®å®Œæˆ")
    
    return pinn, meta_loss_history, adaptation_result


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸŒ åœ°çƒåŠ¨åŠ›å­¦å…ƒå­¦ä¹ åŠŸèƒ½æ¼”ç¤º")
    print("=" * 50)
    
    try:
        # 1. åŸºæœ¬å…ƒå­¦ä¹ åŠŸèƒ½
        demo_meta_learning_basic()
        
        # 2. å…ƒå­¦ä¹ è®­ç»ƒè¿‡ç¨‹
        demo_meta_training()
        
        # 3. å¿«é€Ÿé€‚é…åˆ°æ–°åŒºåŸŸ
        demo_rapid_adaptation()
        
        # 4. è·¨æ„é€ åŸŸæ¨¡æ‹Ÿ
        demo_cross_tectonic_domain()
        
        # 5. å¤šå°ºåº¦è¿ç§»
        demo_multi_scale_transfer()
        
        # 6. å…ƒå­¦ä¹ ç›‘æ§å’Œåˆ†æ
        demo_meta_learning_monitoring()
        
        # 7. ä¸ç°æœ‰PINNçš„é›†æˆ
        demo_integration_with_existing_pinn()
        
        print("\nâœ… æ‰€æœ‰å…ƒå­¦ä¹ æ¼”ç¤ºå®Œæˆ!")
        print("\nğŸ“‹ åŠŸèƒ½æ€»ç»“:")
        print("   - æ”¯æŒå¤šç§åœ°çƒåŠ¨åŠ›å­¦æ„é€ åœºæ™¯")
        print("   - å¿«é€Ÿé€‚é…åˆ°æ–°åŒºåŸŸï¼ˆä»…éœ€å°‘é‡æ•°æ®ï¼‰")
        print("   - è·¨æ„é€ åŸŸæ¨¡æ‹Ÿèƒ½åŠ›")
        print("   - å¤šå°ºåº¦è¿ç§»æ”¯æŒ")
        print("   - å®Œæ•´çš„ç›‘æ§å’Œåˆ†æå·¥å…·")
        print("   - ä¸ç°æœ‰PINNæ— ç¼é›†æˆ")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
