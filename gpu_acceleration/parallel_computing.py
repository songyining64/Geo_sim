"""
å¹¶è¡Œè®¡ç®—æ¨¡å—
"""

import numpy as np
import time
from typing import Dict, List, Tuple, Optional
import warnings

# MPIç›¸å…³ä¾èµ–
try:
    from mpi4py import MPI
    HAS_MPI = True
except ImportError:
    HAS_MPI = False
    MPI = None

try:
    import scipy.sparse as sp
    from scipy.sparse.linalg import spsolve
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False


class MPIManager:
    """MPIç®¡ç†å™¨"""
    
    def __init__(self):
        if not HAS_MPI:
            raise ImportError("éœ€è¦å®‰è£…mpi4pyæ¥ä½¿ç”¨å¹¶è¡Œè®¡ç®—")
        
        self.comm = MPI.COMM_WORLD
        self.rank = self.comm.Get_rank()
        self.size = self.comm.Get_size()
        self.is_root = self.rank == 0
        
        print(f"ğŸ”„ MPIè¿›ç¨‹ {self.rank}/{self.size} åˆå§‹åŒ–å®Œæˆ")
    
    def get_rank(self) -> int:
        """è·å–å½“å‰è¿›ç¨‹æ’å"""
        return self.rank
    
    def get_size(self) -> int:
        """è·å–æ€»è¿›ç¨‹æ•°"""
        return self.size
    
    def is_root_process(self) -> bool:
        """æ˜¯å¦ä¸ºæ ¹è¿›ç¨‹"""
        return self.is_root
    
    def barrier(self):
        """åŒæ­¥æ‰€æœ‰è¿›ç¨‹"""
        self.comm.Barrier()
    
    def broadcast(self, data, root: int = 0):
        """å¹¿æ’­æ•°æ®"""
        return self.comm.bcast(data, root=root)
    
    def gather(self, data, root: int = 0):
        """æ”¶é›†æ•°æ®"""
        return self.comm.gather(data, root=root)
    
    def scatter(self, data, root: int = 0):
        """åˆ†å‘æ•°æ®"""
        return self.comm.scatter(data, root=root)
    
    def reduce(self, data, op=MPI.SUM, root: int = 0):
        """å½’çº¦æ“ä½œ"""
        return self.comm.reduce(data, op=op, root=root)
    
    def allreduce(self, data, op=MPI.SUM):
        """å…¨å±€å½’çº¦æ“ä½œ"""
        return self.comm.allreduce(data, op=op)


class DomainDecomposition:
    """åŸŸåˆ†è§£"""
    
    def __init__(self, mpi_manager: MPIManager):
        self.mpi = mpi_manager
        self.local_domain = None
        self.global_domain = None
    
    def decompose_1d(self, n_points: int) -> Tuple[int, int, int]:
        """
        1DåŸŸåˆ†è§£
        
        Args:
            n_points: æ€»ç‚¹æ•°
            
        Returns:
            start_idx: èµ·å§‹ç´¢å¼•
            end_idx: ç»“æŸç´¢å¼•
            local_size: æœ¬åœ°å¤§å°
        """
        rank = self.mpi.get_rank()
        size = self.mpi.get_size()
        
        # è®¡ç®—æ¯ä¸ªè¿›ç¨‹çš„ç‚¹æ•°
        points_per_process = n_points // size
        remainder = n_points % size
        
        # åˆ†é…èµ·å§‹å’Œç»“æŸç´¢å¼•
        start_idx = rank * points_per_process + min(rank, remainder)
        end_idx = start_idx + points_per_process + (1 if rank < remainder else 0)
        local_size = end_idx - start_idx
        
        return start_idx, end_idx, local_size
    
    def decompose_2d(self, nx: int, ny: int) -> Tuple[int, int, int, int]:
        """
        2DåŸŸåˆ†è§£
        
        Args:
            nx: xæ–¹å‘ç‚¹æ•°
            ny: yæ–¹å‘ç‚¹æ•°
            
        Returns:
            x_start, x_end, y_start, y_end: æœ¬åœ°åŸŸè¾¹ç•Œ
        """
        rank = self.mpi.get_rank()
        size = self.mpi.get_size()
        
        # ç®€åŒ–çš„2Dåˆ†è§£ï¼ˆæŒ‰è¡Œåˆ†è§£ï¼‰
        rows_per_process = ny // size
        remainder = ny % size
        
        y_start = rank * rows_per_process + min(rank, remainder)
        y_end = y_start + rows_per_process + (1 if rank < remainder else 0)
        
        return 0, nx, y_start, y_end
    
    def create_local_mesh(self, global_mesh, decomposition_info):
        """åˆ›å»ºæœ¬åœ°ç½‘æ ¼"""
        if len(decomposition_info) == 3:  # 1D
            start_idx, end_idx, local_size = decomposition_info
            local_coords = global_mesh.coordinates[start_idx:end_idx]
        else:  # 2D
            x_start, x_end, y_start, y_end = decomposition_info
            # ç®€åŒ–çš„2Dç½‘æ ¼æå–
            local_coords = global_mesh.coordinates[y_start:y_end, x_start:x_end]
        
        return local_coords


class ParallelSolver:
    """å¹¶è¡Œæ±‚è§£å™¨"""
    
    def __init__(self, mpi_manager: MPIManager):
        self.mpi = mpi_manager
        self.domain_decomp = DomainDecomposition(mpi_manager)
        self.performance_stats = {
            'solve_time': 0.0,
            'communication_time': 0.0,
            'iterations': 0
        }
    
    def solve_parallel_linear_system(self, A, b: np.ndarray, 
                                   max_iterations: int = 1000,
                                   tolerance: float = 1e-6) -> np.ndarray:
        """
        å¹¶è¡Œæ±‚è§£çº¿æ€§ç³»ç»Ÿ
        
        Args:
            A: ç³»æ•°çŸ©é˜µ
            b: å³ç«¯å‘é‡
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            tolerance: æ”¶æ•›å®¹å·®
            
        Returns:
            x: è§£å‘é‡
        """
        if not HAS_SCIPY:
            raise ImportError("éœ€è¦å®‰è£…scipyæ¥ä½¿ç”¨å¹¶è¡Œæ±‚è§£å™¨")
        
        start_time = time.time()
        
        # åŸŸåˆ†è§£
        n_points = len(b)
        decomposition_info = self.domain_decomp.decompose_1d(n_points)
        start_idx, end_idx, local_size = decomposition_info
        
        # æå–æœ¬åœ°æ•°æ®
        local_b = b[start_idx:end_idx]
        
        # æå–æœ¬åœ°çŸ©é˜µï¼ˆç®€åŒ–å¤„ç†ï¼‰
        if hasattr(A, 'tocsr'):
            A_csr = A.tocsr()
            local_A = A_csr[start_idx:end_idx, :]
        else:
            local_A = A[start_idx:end_idx, :]
        
        # æœ¬åœ°æ±‚è§£
        local_x = spsolve(local_A, local_b)
        
        # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„è§£
        all_x = self.mpi.gather(local_x, root=0)
        
        if self.mpi.is_root_process():
            # æ ¹è¿›ç¨‹åˆå¹¶è§£
            x = np.concatenate(all_x)
        else:
            x = None
        
        # å¹¿æ’­è§£åˆ°æ‰€æœ‰è¿›ç¨‹
        x = self.mpi.broadcast(x, root=0)
        
        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        solve_time = time.time() - start_time
        self.performance_stats['solve_time'] += solve_time
        self.performance_stats['iterations'] += 1
        
        return x
    
    def solve_jacobi_parallel(self, A, b: np.ndarray,
                            max_iterations: int = 1000,
                            tolerance: float = 1e-6) -> np.ndarray:
        """
        å¹¶è¡ŒJacobiè¿­ä»£æ±‚è§£
        
        Args:
            A: ç³»æ•°çŸ©é˜µ
            b: å³ç«¯å‘é‡
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            tolerance: æ”¶æ•›å®¹å·®
            
        Returns:
            x: è§£å‘é‡
        """
        start_time = time.time()
        
        # åŸŸåˆ†è§£
        n_points = len(b)
        decomposition_info = self.domain_decomp.decompose_1d(n_points)
        start_idx, end_idx, local_size = decomposition_info
        
        # åˆå§‹åŒ–è§£å‘é‡
        x = np.zeros(n_points)
        local_x = x[start_idx:end_idx]
        local_b = b[start_idx:end_idx]
        
        # æå–æœ¬åœ°çŸ©é˜µ
        if hasattr(A, 'tocsr'):
            A_csr = A.tocsr()
            local_A = A_csr[start_idx:end_idx, :]
        else:
            local_A = A[start_idx:end_idx, :]
        
        # Jacobiè¿­ä»£
        for iteration in range(max_iterations):
            # ä¿å­˜å‰ä¸€æ¬¡è¿­ä»£ç»“æœ
            x_old = local_x.copy()
            
            # æœ¬åœ°Jacobiæ›´æ–°
            for i in range(local_size):
                global_i = start_idx + i
                sum_ax = 0.0
                diag = 0.0
                
                for j in range(n_points):
                    if j != global_i:
                        sum_ax += local_A[i, j] * x[j]
                    else:
                        diag = local_A[i, j]
                
                if abs(diag) > 1e-12:
                    local_x[i] = (local_b[i] - sum_ax) / diag
            
            # æ›´æ–°å…¨å±€è§£å‘é‡
            all_x = self.mpi.allgather(local_x)
            x = np.concatenate(all_x)
            
            # æ£€æŸ¥æ”¶æ•›æ€§
            local_residual = np.linalg.norm(local_x - x_old)
            global_residual = self.mpi.allreduce(local_residual, op=MPI.SUM)
            
            if global_residual < tolerance:
                break
        
        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        solve_time = time.time() - start_time
        self.performance_stats['solve_time'] += solve_time
        self.performance_stats['iterations'] += iteration + 1
        
        return x
    
    def get_performance_stats(self) -> dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        stats = self.performance_stats.copy()
        stats['rank'] = self.mpi.get_rank()
        stats['size'] = self.mpi.get_size()
        return stats


def create_parallel_solver() -> ParallelSolver:
    """åˆ›å»ºå¹¶è¡Œæ±‚è§£å™¨"""
    mpi_manager = MPIManager()
    return ParallelSolver(mpi_manager)


def demo_parallel_computing():
    """æ¼”ç¤ºå¹¶è¡Œè®¡ç®—åŠŸèƒ½"""
    print("ğŸ”„ å¹¶è¡Œè®¡ç®—æ¼”ç¤º")
    print("=" * 50)
    
    try:
        # åˆ›å»ºå¹¶è¡Œæ±‚è§£å™¨
        solver = create_parallel_solver()
        
        # åˆ›å»ºæµ‹è¯•é—®é¢˜
        n_points = 1000
        A = np.random.rand(n_points, n_points)
        A = A + A.T + n_points * np.eye(n_points)  # ç¡®ä¿æ­£å®š
        b = np.random.rand(n_points)
        
        # å¹¶è¡Œæ±‚è§£
        print(f"ğŸ”§ ä½¿ç”¨ {solver.mpi.get_size()} ä¸ªè¿›ç¨‹æ±‚è§£ {n_points}x{n_points} çº¿æ€§ç³»ç»Ÿ...")
        
        start_time = time.time()
        x_parallel = solver.solve_parallel_linear_system(A, b)
        parallel_time = time.time() - start_time
        
        # ä¸²è¡Œæ±‚è§£ï¼ˆä»…åœ¨æ ¹è¿›ç¨‹ï¼‰
        if solver.mpi.is_root_process():
            start_time = time.time()
            x_serial = np.linalg.solve(A, b)
            serial_time = time.time() - start_time
            
            # éªŒè¯ç»“æœ
            error = np.linalg.norm(x_parallel - x_serial) / np.linalg.norm(x_serial)
            
            print(f"   ä¸²è¡Œæ—¶é—´: {serial_time:.4f} ç§’")
            print(f"   å¹¶è¡Œæ—¶é—´: {parallel_time:.4f} ç§’")
            print(f"   åŠ é€Ÿæ¯”: {serial_time / parallel_time:.2f}x")
            print(f"   è¯¯å·®: {error:.2e}")
        
        # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
        stats = solver.get_performance_stats()
        print(f"\nğŸ“ˆ è¿›ç¨‹ {stats['rank']} æ€§èƒ½ç»Ÿè®¡:")
        print(f"   æ±‚è§£æ—¶é—´: {stats['solve_time']:.4f} ç§’")
        print(f"   è¿­ä»£æ¬¡æ•°: {stats['iterations']}")
        
        print("\nâœ… å¹¶è¡Œè®¡ç®—æ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ å¹¶è¡Œè®¡ç®—æ¼”ç¤ºå¤±è´¥: {e}")


if __name__ == "__main__":
    demo_parallel_computing() 