"""
é«˜çº§å¹¶è¡Œæ±‚è§£å™¨ - å®Œæ•´ç‰ˆ

å®ç°æ ¸å¿ƒåŠŸèƒ½ï¼š
1. é€šä¿¡æ•ˆç‡ï¼šéé˜»å¡é€šä¿¡+è®¡ç®—é‡å +è‡ªé€‚åº”é€šä¿¡æ¨¡å¼
2. æ™ºèƒ½è´Ÿè½½å‡è¡¡ï¼šMLé¢„æµ‹+åŠ¨æ€è´Ÿè½½è¿ç§»
3. å¼‚æ„è®¡ç®—ï¼šMPI+GPU+OpenMPæ··åˆæ¶æ„
4. ç®—æ³•é€‚åº”æ€§ï¼šé—®é¢˜æ„ŸçŸ¥çš„è‡ªé€‚åº”æ±‚è§£ç­–ç•¥
5. å¤§è§„æ¨¡å¯æ‰©å±•æ€§ï¼šåˆ†å¸ƒå¼ç¨€ç–å­˜å‚¨+å¤šçº§åŸŸåˆ†è§£
"""

import numpy as np
import scipy.sparse as sp
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
from dataclasses import dataclass
from abc import ABC, abstractmethod
import time
import warnings
import threading
from concurrent.futures import ThreadPoolExecutor
import pickle
import json

# MPIç›¸å…³ä¾èµ–
try:
    from mpi4py import MPI
    HAS_MPI = True
except ImportError:
    HAS_MPI = False
    MPI = None

# GPUç›¸å…³ä¾èµ–
try:
    import torch
    HAS_PYTORCH = True
except ImportError:
    HAS_PYTORCH = False
    torch = None

try:
    import cupy as cp
    HAS_CUPY = True
except ImportError:
    HAS_CUPY = False
    cp = None

# æ•°å€¼è®¡ç®—ä¾èµ–
try:
    from scipy.sparse.linalg import spsolve, gmres, cg, bicgstab
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False

# Numbaå¹¶è¡ŒåŒ–
try:
    import numba as nb
    from numba import prange
    HAS_NUMBA = True
except ImportError:
    HAS_NUMBA = False
    prange = range

# æ ¸å¿ƒç»„ä»¶ç±»
class LoadMonitor:
    """è´Ÿè½½ç›‘æ§å™¨ - ç”¨äºå®æ—¶è´Ÿè½½ç›‘æ§"""
    
    def __init__(self):
        self.load_history = []
        self.max_history_size = 50
        self.current_load = 1.0
        self.load_update_interval = 0.1  # 100msæ›´æ–°ä¸€æ¬¡
        self.last_update_time = time.time()
        
        # ç³»ç»Ÿèµ„æºç›‘æ§
        self.cpu_usage = 0.0
        self.memory_usage = 0.0
        self.gpu_usage = 0.0
        
    def get_current_load(self) -> float:
        """è·å–å½“å‰è´Ÿè½½"""
        current_time = time.time()
        
        # å®šæœŸæ›´æ–°è´Ÿè½½
        if current_time - self.last_update_time > self.load_update_interval:
            self._update_system_load()
            self.last_update_time = current_time
        
        return self.current_load
    
    def _update_system_load(self):
        """æ›´æ–°ç³»ç»Ÿè´Ÿè½½"""
        try:
            import psutil
            
            # CPUä½¿ç”¨ç‡
            self.cpu_usage = psutil.cpu_percent(interval=0.1)
            
            # å†…å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            self.memory_usage = memory.percent
            
            # GPUä½¿ç”¨ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                import GPUtil
                gpus = GPUtil.getGPUs()
                if gpus:
                    self.gpu_usage = gpus[0].load * 100
                else:
                    self.gpu_usage = 0.0
            except:
                self.gpu_usage = 0.0
            
            # ç»¼åˆè´Ÿè½½è®¡ç®—
            self.current_load = (self.cpu_usage * 0.4 + 
                               self.memory_usage * 0.4 + 
                               self.gpu_usage * 0.2) / 100.0
            
        except ImportError:
            # å¦‚æœæ²¡æœ‰psutilï¼Œä½¿ç”¨ç®€å•çš„è´Ÿè½½ä¼°è®¡
            self.current_load = 0.5 + 0.3 * np.sin(time.time() * 0.1)
        
        # è®°å½•è´Ÿè½½å†å²
        self.load_history.append({
            'timestamp': time.time(),
            'load': self.current_load,
            'cpu': self.cpu_usage,
            'memory': self.memory_usage,
            'gpu': self.gpu_usage
        })
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.load_history) > self.max_history_size:
            self.load_history.pop(0)
    
    def get_statistics(self) -> Dict:
        """è·å–è´Ÿè½½ç»Ÿè®¡ä¿¡æ¯"""
        if not self.load_history:
            return {}
        
        loads = [entry['load'] for entry in self.load_history]
        return {
            'current_load': self.current_load,
            'average_load': np.mean(loads),
            'max_load': np.max(loads),
            'min_load': np.min(loads),
            'load_variance': np.var(loads),
            'cpu_usage': self.cpu_usage,
            'memory_usage': self.memory_usage,
            'gpu_usage': self.gpu_usage,
            'history_size': len(self.load_history)
        }


class AdaptiveCommunicator:
    """è‡ªé€‚åº”é€šä¿¡ä¼˜åŒ–å™¨ - æ”¯æŒåŠ¨æ€é€šä¿¡è°ƒåº¦"""
    
    def __init__(self, comm=None):
        self.comm = comm
        self.rank = comm.Get_rank() if comm else 0
        self.size = comm.Get_size() if comm else 1
        
        # é€šä¿¡æ¨¡å¼åˆ†æ
        self.communication_patterns = {}
        self.data_density_cache = {}
        self.performance_history = {}
        
        # åŠ¨æ€è°ƒåº¦
        self.schedule_cache = {}
        self.update_counter = 0
        self.update_frequency = 5
        
        # ç¼“å†²åŒºç®¡ç†
        self.buffer_pool = {}
        self.optimal_buffer_sizes = {}
        
        # å®æ—¶è´Ÿè½½ç›‘æ§
        self.load_monitor = LoadMonitor()
        self.communication_history = {}
        self.dynamic_schedule_cache = {}
        self.schedule_update_frequency = 10  # æ¯10æ¬¡é€šä¿¡æ›´æ–°ä¸€æ¬¡è°ƒåº¦
        self.communication_count = 0
        
    def optimize_communication_pattern(self, partition_info: Dict, 
                                    data_density: np.ndarray) -> Dict:
        """ä¼˜åŒ–é€šä¿¡æ¨¡å¼ - åŸºäºæ•°æ®å±€éƒ¨æ€§"""
        pattern_key = self._generate_pattern_key(partition_info)
        
        if pattern_key in self.schedule_cache:
            return self.schedule_cache[pattern_key]
        
        # åˆ†ææ•°æ®å¯†åº¦åˆ†å¸ƒ
        high_density_regions = data_density > 0.7
        low_density_regions = data_density < 0.3
        
        # ç”Ÿæˆè‡ªé€‚åº”é€šä¿¡è°ƒåº¦
        if np.any(high_density_regions):
            schedule = self._generate_point_to_point_schedule(partition_info, high_density_regions)
        elif np.any(low_density_regions):
            schedule = self._generate_collective_schedule(partition_info, low_density_regions)
        else:
            schedule = self._generate_hybrid_schedule(partition_info, data_density)
        
        # ç¼“å­˜ç»“æœ
        self.schedule_cache[pattern_key] = schedule
        return schedule
    
    def _generate_pattern_key(self, partition_info: Dict) -> str:
        """ç”Ÿæˆé€šä¿¡æ¨¡å¼é”®å€¼"""
        return f"rank_{self.rank}_size_{self.size}_method_{partition_info.get('method', 'unknown')}"
    
    def _generate_point_to_point_schedule(self, partition_info: Dict, 
                                        high_density_mask: np.ndarray) -> Dict:
        """é«˜æ•°æ®å¯†åº¦åŒºåŸŸï¼šç‚¹å¯¹ç‚¹é€šä¿¡"""
        schedule = {
            'type': 'point_to_point',
            'neighbors': [],
            'communication_order': [],
            'buffer_sizes': {}
        }
        
        # è¯†åˆ«é‚»å±…è¿›ç¨‹
        for neighbor in range(self.size):
            if neighbor != self.rank:
                # æ¨¡æ‹Ÿè¾¹ç•Œå…ƒç´ è¯†åˆ«
                boundary_elements = self._get_boundary_elements(partition_info, neighbor)
                if len(boundary_elements) > 0:
                    schedule['neighbors'].append(neighbor)
                    schedule['buffer_sizes'][neighbor] = len(boundary_elements) * 8
        
        # ä¼˜åŒ–é€šä¿¡é¡ºåº
        schedule['communication_order'] = self._optimize_communication_order(schedule['neighbors'])
        
        return schedule
    
    def _generate_collective_schedule(self, partition_info: Dict, 
                                    low_density_mask: np.ndarray) -> Dict:
        """ä½æ•°æ®å¯†åº¦åŒºåŸŸï¼šé›†ä½“é€šä¿¡"""
        return {
            'type': 'collective',
            'operation': 'allgather',
            'buffer_size': np.sum(low_density_mask) * 8,
            'optimization': 'tree_reduction'
        }
    
    def _generate_hybrid_schedule(self, partition_info: Dict, 
                                data_density: np.ndarray) -> Dict:
        """æ··åˆé€šä¿¡ç­–ç•¥"""
        return {
            'type': 'hybrid',
            'point_to_point': self._generate_point_to_point_schedule(partition_info, data_density > 0.5),
            'collective': self._generate_collective_schedule(partition_info, data_density <= 0.5)
        }
    
    def _get_boundary_elements(self, partition_info: Dict, neighbor: int) -> List:
        """è·å–è¾¹ç•Œå…ƒç´ ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # è¿™é‡Œåº”è¯¥å®ç°çœŸå®çš„è¾¹ç•Œå…ƒç´ è¯†åˆ«é€»è¾‘
        return list(range(10))  # æ¨¡æ‹Ÿè¿”å›10ä¸ªè¾¹ç•Œå…ƒç´ 
    
    def _optimize_communication_order(self, neighbors: List[int]) -> List[int]:
        """ä¼˜åŒ–é€šä¿¡é¡ºåº"""
        # ç®€å•çš„ä¼˜åŒ–ï¼šæŒ‰è¿›ç¨‹IDæ’åºï¼Œå‡å°‘é€šä¿¡å†²çª
        return sorted(neighbors)
    
    def execute_communication_with_overlap(self, schedule: Dict, 
                                         local_data: np.ndarray,
                                         computation_func: Callable) -> np.ndarray:
        """æ‰§è¡Œé€šä¿¡ä¸è®¡ç®—é‡å """
        if not self.comm:
            return local_data
        
        start_time = time.time()
        
        # å¯åŠ¨éé˜»å¡é€šä¿¡
        requests = []
        recv_buffers = {}
        
        if schedule['type'] == 'point_to_point':
            for neighbor in schedule['neighbors']:
                # å‘é€æ•°æ®
                send_data = self._extract_boundary_data(local_data, neighbor)
                req_send = self.comm.Isend(send_data, dest=neighbor, tag=100 + self.rank)
                requests.append(req_send)
                
                # å‡†å¤‡æ¥æ”¶ç¼“å†²åŒº
                buffer_size = schedule['buffer_sizes'][neighbor]
                recv_buffers[neighbor] = np.zeros(buffer_size // 8, dtype=np.float64)
                req_recv = self.comm.Irecv(recv_buffers[neighbor], source=neighbor, tag=100 + neighbor)
                requests.append(req_recv)
        
        # æ‰§è¡Œæœ¬åœ°è®¡ç®—ï¼ˆä¸é€šä¿¡é‡å ï¼‰
        local_result = computation_func(local_data)
        
        # ç­‰å¾…é€šä¿¡å®Œæˆ
        MPI.Request.Waitall(requests)
        
        # æ•´åˆé‚»å±…æ•°æ®
        if schedule['type'] == 'point_to_point':
            for neighbor, recv_data in recv_buffers.items():
                local_result = self._integrate_neighbor_data(local_result, recv_data, neighbor)
        
        communication_time = time.time() - start_time
        
        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        self.performance_history[self.update_counter] = {
            'communication_time': communication_time,
            'schedule_type': schedule['type']
        }
        
        return local_result
    
    def _extract_boundary_data(self, local_data: np.ndarray, neighbor: int) -> np.ndarray:
        """æå–è¾¹ç•Œæ•°æ®ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # æ¨¡æ‹Ÿè¾¹ç•Œæ•°æ®æå–
        return local_data[:10] if len(local_data) >= 10 else local_data
    
    def _integrate_neighbor_data(self, local_result: np.ndarray, 
                                neighbor_data: np.ndarray, neighbor: int) -> np.ndarray:
        """æ•´åˆé‚»å±…æ•°æ®ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # æ¨¡æ‹Ÿæ•°æ®æ•´åˆ
        if len(neighbor_data) > 0:
            # ç®€å•çš„æ•°æ®åˆå¹¶
            return np.concatenate([local_result, neighbor_data])
        return local_result
    
    def optimize_communication_schedule(self, partition_info: Dict) -> Dict:
        """ä¼˜åŒ–é€šä¿¡è°ƒåº¦ - æ”¯æŒåŠ¨æ€è°ƒåº¦"""
        if not self.comm:
            return partition_info
        
        # åˆ†æé€šä¿¡æ¨¡å¼
        pattern = self._analyze_communication_pattern(partition_info)
        
        # è·å–å®æ—¶è´Ÿè½½ä¿¡æ¯
        real_time_loads = self._get_real_time_loads()
        
        # ç”ŸæˆåŠ¨æ€ä¼˜åŒ–çš„é€šä¿¡è°ƒåº¦
        schedule = self._generate_dynamic_schedule(pattern, real_time_loads)
        
        # é¢„åˆ†é…ç¼“å†²åŒº
        self._preallocate_buffers(schedule)
        
        # æ›´æ–°é€šä¿¡å†å²
        self._update_communication_history(schedule)
        
        return {
            'communication_schedule': schedule,
            'communication_pattern': pattern,
            'buffer_info': self.buffer_pool,
            'real_time_loads': real_time_loads,
            'dynamic_optimization': True
        }
    
    def _get_real_time_loads(self) -> Dict[int, float]:
        """è·å–å®æ—¶è´Ÿè½½ä¿¡æ¯"""
        if not self.comm:
            return {self.rank: 1.0}
        
        # è·å–æœ¬åœ°è´Ÿè½½
        local_load = self.load_monitor.get_current_load()
        
        # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„è´Ÿè½½ä¿¡æ¯
        all_loads = self.comm.allgather(local_load)
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        loads = {rank: load for rank, load in enumerate(all_loads)}
        
        return loads
    
    def _generate_dynamic_schedule(self, pattern: Dict, real_time_loads: Dict[int, float]) -> Dict:
        """ç”ŸæˆåŠ¨æ€ä¼˜åŒ–çš„é€šä¿¡è°ƒåº¦"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°è°ƒåº¦
        if self._should_update_schedule(real_time_loads):
            schedule = self._compute_optimal_schedule(pattern, real_time_loads)
            self.dynamic_schedule_cache = schedule
        else:
            schedule = self.dynamic_schedule_cache
        
        return schedule
    
    def _should_update_schedule(self, real_time_loads: Dict[int, float]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°è°ƒåº¦"""
        # åŸºäºé€šä¿¡é¢‘ç‡å’Œè´Ÿè½½å˜åŒ–åˆ¤æ–­
        if self.communication_count % self.schedule_update_frequency == 0:
            return True
        
        # æ£€æŸ¥è´Ÿè½½å˜åŒ–æ˜¯å¦è¶…è¿‡é˜ˆå€¼
        if hasattr(self, '_previous_loads'):
            load_change = self._compute_load_change(self._previous_loads, real_time_loads)
            if load_change > 0.2:  # è´Ÿè½½å˜åŒ–è¶…è¿‡20%
                return True
        
        self._previous_loads = real_time_loads.copy()
        return False
    
    def _compute_load_change(self, old_loads: Dict[int, float], new_loads: Dict[int, float]) -> float:
        """è®¡ç®—è´Ÿè½½å˜åŒ–"""
        if not old_loads or not new_loads:
            return 0.0
        
        total_change = 0.0
        for rank in old_loads:
            if rank in new_loads:
                change = abs(new_loads[rank] - old_loads[rank]) / max(old_loads[rank], 1e-6)
                total_change += change
        
        return total_change / len(old_loads)
    
    def _compute_optimal_schedule(self, pattern: Dict, real_time_loads: Dict[int, float]) -> Dict:
        """è®¡ç®—æœ€ä¼˜é€šä¿¡è°ƒåº¦"""
        schedule = {
            'send_sequence': [],
            'receive_sequence': [],
            'nonblocking_ops': [],
            'collective_ops': [],
            'priority_queue': [],
            'load_balanced_sequence': []
        }
        
        if not pattern.get('neighbors'):
            return schedule
        
        # è®¡ç®—æ¯ä¸ªé‚»å±…çš„é€šä¿¡ä¼˜å…ˆçº§
        neighbor_priorities = self._compute_neighbor_priorities(pattern, real_time_loads)
        
        # ç”Ÿæˆè´Ÿè½½å‡è¡¡çš„é€šä¿¡åºåˆ—
        schedule['load_balanced_sequence'] = self._generate_load_balanced_sequence(
            pattern, real_time_loads, neighbor_priorities
        )
        
        # ç”Ÿæˆä¼˜å…ˆçº§é˜Ÿåˆ—
        schedule['priority_queue'] = self._generate_priority_queue(neighbor_priorities)
        
        # ç”Ÿæˆå‘é€å’Œæ¥æ”¶åºåˆ—
        schedule['send_sequence'] = [neighbor for neighbor, _ in schedule['priority_queue']]
        schedule['receive_sequence'] = schedule['send_sequence'][::-1]  # åå‘æ¥æ”¶
        
        # éé˜»å¡æ“ä½œ
        schedule['nonblocking_ops'] = schedule['send_sequence']
        
        return schedule
    
    def _compute_neighbor_priorities(self, pattern: Dict, real_time_loads: Dict[int, float]) -> Dict[int, float]:
        """è®¡ç®—é‚»å±…é€šä¿¡ä¼˜å…ˆçº§"""
        priorities = {}
        
        for neighbor in pattern.get('neighbors', {}):
            # åŸºç¡€ä¼˜å…ˆçº§ï¼šé€šä¿¡é‡
            communication_volume = pattern.get('communication_volume', {}).get(neighbor, 0)
            
            # è´Ÿè½½å› å­ï¼šä¼˜å…ˆä¸è´Ÿè½½è¾ƒä½çš„è¿›ç¨‹é€šä¿¡
            neighbor_load = real_time_loads.get(neighbor, 1.0)
            load_factor = 1.0 / max(neighbor_load, 0.1)
            
            # è·ç¦»å› å­ï¼šä¼˜å…ˆä¸è¿‘é‚»é€šä¿¡ï¼ˆç®€åŒ–å®ç°ï¼‰
            distance_factor = 1.0 / (abs(neighbor - self.rank) + 1)
            
            # ç»¼åˆä¼˜å…ˆçº§
            priority = communication_volume * load_factor * distance_factor
            priorities[neighbor] = priority
        
        return priorities
    
    def _generate_load_balanced_sequence(self, pattern: Dict, real_time_loads: Dict[int, float], 
                                       priorities: Dict[int, float]) -> List[Tuple[int, float]]:
        """ç”Ÿæˆè´Ÿè½½å‡è¡¡çš„é€šä¿¡åºåˆ—"""
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_neighbors = sorted(priorities.items(), key=lambda x: x[1], reverse=True)
        
        # è€ƒè™‘è´Ÿè½½å¹³è¡¡çš„åºåˆ—ç”Ÿæˆ
        balanced_sequence = []
        current_load = real_time_loads.get(self.rank, 1.0)
        
        for neighbor, priority in sorted_neighbors:
            neighbor_load = real_time_loads.get(neighbor, 1.0)
            
            # å¦‚æœé‚»å±…è´Ÿè½½è¾ƒä½ï¼Œä¼˜å…ˆé€šä¿¡
            if neighbor_load < current_load:
                balanced_sequence.insert(0, (neighbor, priority))
            else:
                balanced_sequence.append((neighbor, priority))
        
        return balanced_sequence
    
    def _generate_priority_queue(self, priorities: Dict[int, float]) -> List[Tuple[int, float]]:
        """ç”Ÿæˆä¼˜å…ˆçº§é˜Ÿåˆ—"""
        return sorted(priorities.items(), key=lambda x: x[1], reverse=True)
    
    def _update_communication_history(self, schedule: Dict):
        """æ›´æ–°é€šä¿¡å†å²"""
        self.communication_count += 1
        
        # è®°å½•è°ƒåº¦ä¿¡æ¯
        self.communication_history[self.communication_count] = {
            'timestamp': time.time(),
            'schedule': schedule,
            'load_info': getattr(self, '_previous_loads', {})
        }
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.communication_history) > 100:
            oldest_key = min(self.communication_history.keys())
            del self.communication_history[oldest_key]
    
    def get_communication_statistics(self) -> Dict:
        """è·å–é€šä¿¡ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_communications': self.communication_count,
            'schedule_updates': len(self.dynamic_schedule_cache),
            'load_monitoring': self.load_monitor.get_statistics(),
            'recent_history': dict(list(self.communication_history.items())[-10:])
        }
    
    def _analyze_communication_pattern(self, partition_info: Dict) -> Dict:
        """åˆ†æé€šä¿¡æ¨¡å¼"""
        pattern = {
            'neighbors': {},
            'shared_nodes': {},
            'communication_volume': {},
            'communication_frequency': {}
        }
        
        boundary_nodes = partition_info.get('boundary_nodes', {}).get(self.rank, set())
        
        for neighbor in range(self.size):
            if neighbor == self.rank:
                continue
            
            neighbor_boundary = partition_info.get('boundary_nodes', {}).get(neighbor, set())
            shared_nodes = boundary_nodes.intersection(neighbor_boundary)
            
            if shared_nodes:
                pattern['neighbors'][neighbor] = list(shared_nodes)
                pattern['shared_nodes'][neighbor] = len(shared_nodes)
                pattern['communication_volume'][neighbor] = len(shared_nodes) * 8
                pattern['communication_frequency'][neighbor] = 1
        
        return pattern
    
    def _preallocate_buffers(self, schedule: Dict):
        """é¢„åˆ†é…ç¼“å†²åŒº"""
        for neighbor in schedule.get('nonblocking_ops', []):
            buffer_size = 1024  # é»˜è®¤ç¼“å†²åŒºå¤§å°
            self.buffer_pool[neighbor] = {
                'send_buffer': np.zeros(buffer_size, dtype=np.float64),
                'recv_buffer': np.zeros(buffer_size, dtype=np.float64),
                'request': None
            }


class MLBasedLoadBalancer:
    """åŸºäºæœºå™¨å­¦ä¹ çš„æ™ºèƒ½è´Ÿè½½å‡è¡¡å™¨"""
    
    def __init__(self, comm=None, ml_model=None):
        self.comm = comm
        self.rank = comm.Get_rank() if comm else 0
        self.size = comm.Get_size() if comm else 1
        
        # MLæ¨¡å‹
        self.ml_model = ml_model
        self.load_predictor = self._initialize_load_predictor()
        
        # è´Ÿè½½ç›‘æ§
        self.load_history = {}
        self.migration_history = {}
        self.performance_metrics = {}
        
        # åŠ¨æ€è°ƒæ•´å‚æ•°
        self.migration_threshold = 0.15
        self.prediction_confidence_threshold = 0.8
        
    def _initialize_load_predictor(self):
        """åˆå§‹åŒ–è´Ÿè½½é¢„æµ‹å™¨"""
        if self.ml_model:
            return self.ml_model
        
        # ç®€å•çš„åŸºäºå†å²çš„é¢„æµ‹å™¨
        class SimpleLoadPredictor:
            def __init__(self):
                self.history = {}
                self.weights = np.array([0.5, 0.3, 0.2])  # æœ€è¿‘3æ¬¡çš„æƒé‡
            
            def predict(self, mesh_features, partition_info):
                # åŸºäºç½‘æ ¼ç‰¹å¾å’Œåˆ†åŒºä¿¡æ¯çš„ç®€å•é¢„æµ‹
                complexity = np.sum(mesh_features.get('element_complexity', [1.0]))
                size_factor = len(mesh_features.get('elements', [])) / 1000
                return complexity * size_factor
        
        return SimpleLoadPredictor()
    
    def predict_load_distribution(self, mesh_features: Dict, 
                                partition_info: Dict) -> np.ndarray:
        """é¢„æµ‹è´Ÿè½½åˆ†å¸ƒ"""
        predicted_loads = np.zeros(self.size)
        
        for i in range(self.size):
            local_features = self._extract_local_features(mesh_features, partition_info, i)
            predicted_loads[i] = self.load_predictor.predict(local_features, partition_info)
        
        # å½’ä¸€åŒ–
        predicted_loads = predicted_loads / np.sum(predicted_loads)
        return predicted_loads
    
    def balance_load_dynamically(self, current_loads: np.ndarray, 
                                mesh_features: Dict,
                                partition_info: Dict) -> Dict:
        """åŠ¨æ€è´Ÿè½½å‡è¡¡"""
        # è®¡ç®—è´Ÿè½½ä¸å¹³è¡¡åº¦
        load_imbalance = self._compute_load_imbalance(current_loads)
        
        if load_imbalance < self.migration_threshold:
            return partition_info  # æ— éœ€è¿ç§»
        
        # é¢„æµ‹æœ€ä¼˜è´Ÿè½½åˆ†å¸ƒ
        predicted_loads = self.predict_load_distribution(mesh_features, partition_info)
        
        # è®¡ç®—è¿ç§»ç­–ç•¥
        migration_plan = self._compute_migration_plan(current_loads, predicted_loads, partition_info)
        
        # æ‰§è¡Œè¿ç§»
        updated_partition = self._execute_migration(migration_plan, partition_info)
        
        # è®°å½•è¿ç§»å†å²
        self.migration_history[time.time()] = {
            'original_imbalance': load_imbalance,
            'migration_plan': migration_plan,
            'predicted_improvement': self._estimate_improvement(current_loads, predicted_loads)
        }
        
        return updated_partition
    
    def _extract_local_features(self, mesh_features: Dict, partition_info: Dict, rank: int) -> Dict:
        """æå–æœ¬åœ°ç‰¹å¾ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        return {
            'element_complexity': [1.0, 1.5, 2.0],  # æ¨¡æ‹Ÿå¤æ‚åº¦
            'elements': list(range(100))  # æ¨¡æ‹Ÿå…ƒç´ 
        }
    
    def _compute_load_imbalance(self, loads: np.ndarray) -> float:
        """è®¡ç®—è´Ÿè½½ä¸å¹³è¡¡åº¦"""
        mean_load = np.mean(loads)
        if mean_load == 0:
            return 0.0
        return np.std(loads) / mean_load
    
    def _compute_migration_plan(self, current_loads: np.ndarray, 
                               target_loads: np.ndarray,
                               partition_info: Dict) -> Dict:
        """è®¡ç®—è¿ç§»è®¡åˆ’"""
        migration_plan = {
            'migrations': [],
            'estimated_time': 0.0,
            'risk_level': 'low'
        }
        
        # è¯†åˆ«è¿‡è½½å’Œè½»è½½è¿›ç¨‹
        overloaded = current_loads > target_loads * 1.2
        underloaded = current_loads < target_loads * 0.8
        
        # è®¡ç®—éœ€è¦è¿ç§»çš„å…ƒç´ 
        for i in range(self.size):
            if overloaded[i]:
                for j in range(self.size):
                    if underloaded[j]:
                        migration_amount = min(
                            current_loads[i] - target_loads[i] * 1.1,
                            target_loads[j] * 1.1 - current_loads[j]
                        )
                        
                        if migration_amount > 0:
                            migration_plan['migrations'].append({
                                'from': i,
                                'to': j,
                                'amount': migration_amount,
                                'elements': self._select_migration_elements(partition_info, i, migration_amount)
                            })
        
        return migration_plan
    
    def _select_migration_elements(self, partition_info: Dict, rank: int, amount: float) -> List:
        """é€‰æ‹©è¿ç§»å…ƒç´ ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        return list(range(int(amount)))
    
    def _execute_migration(self, migration_plan: Dict, partition_info: Dict) -> Dict:
        """æ‰§è¡Œè¿ç§»ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # è¿™é‡Œåº”è¯¥å®ç°çœŸå®çš„è¿ç§»é€»è¾‘
        return partition_info
    
    def _estimate_improvement(self, current_loads: np.ndarray, 
                            target_loads: np.ndarray) -> float:
        """ä¼°è®¡æ”¹è¿›ç¨‹åº¦"""
        current_imbalance = self._compute_load_imbalance(current_loads)
        target_imbalance = self._compute_load_imbalance(target_loads)
        return current_imbalance - target_imbalance


class HeterogeneousComputingManager:
    """å¼‚æ„è®¡ç®—ç®¡ç†å™¨ - MPI+GPU+OpenMPæ··åˆæ¶æ„"""
    
    def __init__(self, config: AdvancedParallelConfig):
        self.config = config
        self.comm = MPI.COMM_WORLD if HAS_MPI else None
        self.rank = self.comm.Get_rank() if self.comm else 0
        
        # GPUè®¾ç½®
        self.gpu_device = None
        self.gpu_available = False
        if config.use_gpu and HAS_PYTORCH:
            self._setup_gpu()
        
        # OpenMPè®¾ç½®
        self.openmp_available = False
        if config.use_openmp and HAS_NUMBA:
            self._setup_openmp()
        
        # æ€§èƒ½ç›‘æ§
        self.gpu_performance = {}
        self.cpu_performance = {}
        
    def _setup_gpu(self):
        """è®¾ç½®GPU"""
        try:
            if torch.cuda.is_available():
                self.gpu_device = self.rank % torch.cuda.device_count()
                torch.cuda.set_device(self.gpu_device)
                self.gpu_available = True
                print(f"âœ… è¿›ç¨‹ {self.rank} ç»‘å®šåˆ°GPU {self.gpu_device}")
            else:
                print(f"âš ï¸ è¿›ç¨‹ {self.rank}: GPUä¸å¯ç”¨")
        except Exception as e:
            print(f"âŒ è¿›ç¨‹ {self.rank} GPUè®¾ç½®å¤±è´¥: {e}")
    
    def _setup_openmp(self):
        """è®¾ç½®OpenMP"""
        try:
            # è®¾ç½®çº¿ç¨‹æ•°
            nb.set_num_threads(self.config.cpu_threads)
            self.openmp_available = True
            print(f"âœ… è¿›ç¨‹ {self.rank} å¯ç”¨OpenMPï¼Œçº¿ç¨‹æ•°: {self.config.cpu_threads}")
        except Exception as e:
            print(f"âŒ è¿›ç¨‹ {self.rank} OpenMPè®¾ç½®å¤±è´¥: {e}")
    
    def solve_with_heterogeneous_computing(self, A: np.ndarray, b: np.ndarray,
                                         solver_type: str = 'auto') -> np.ndarray:
        """å¼‚æ„è®¡ç®—æ±‚è§£"""
        if solver_type == 'auto':
            solver_type = self._select_optimal_solver(A, b)
        
        start_time = time.time()
        
        if solver_type == 'gpu_cg' and self.gpu_available:
            result = self._gpu_solve(A, b)
            self.gpu_performance[time.time()] = time.time() - start_time
        elif solver_type == 'openmp_cg' and self.openmp_available:
            result = self._openmp_solve(A, b)
            self.cpu_performance[time.time()] = time.time() - start_time
        else:
            result = self._cpu_solve(A, b)
            self.cpu_performance[time.time()] = time.time() - start_time
        
        return result
    
    def _select_optimal_solver(self, A: np.ndarray, b: np.ndarray) -> str:
        """é€‰æ‹©æœ€ä¼˜æ±‚è§£å™¨"""
        # åŸºäºé—®é¢˜ç‰¹å¾é€‰æ‹©æ±‚è§£å™¨
        problem_size = A.shape[0]
        sparsity = 1.0 - A.nnz / (A.shape[0] * A.shape[1]) if hasattr(A, 'nnz') else 0.5
        
        if problem_size > 10000 and self.gpu_available and sparsity > 0.8:
            return 'gpu_cg'
        elif problem_size > 5000 and self.openmp_available:
            return 'openmp_cg'
        else:
            return 'cpu_cg'
    
    def _gpu_solve(self, A: np.ndarray, b: np.ndarray) -> np.ndarray:
        """GPUæ±‚è§£"""
        try:
            # è½¬æ¢ä¸ºPyTorchå¼ é‡
            A_tensor = torch.from_numpy(A.toarray()).float().cuda(self.gpu_device)
            b_tensor = torch.from_numpy(b).float().cuda(self.gpu_device)
            
            # GPUä¸Šçš„å…±è½­æ¢¯åº¦æ±‚è§£
            x_tensor = torch.zeros_like(b_tensor)
            
            for iteration in range(self.config.max_iterations):
                r = b_tensor - torch.mv(A_tensor, x_tensor)
                p = r.clone()
                
                for i in range(self.config.max_iterations):
                    Ap = torch.mv(A_tensor, p)
                    alpha = torch.dot(r, r) / torch.dot(p, Ap)
                    x_tensor = x_tensor + alpha * p
                    r_new = r - alpha * Ap
                    
                    if torch.norm(r_new) < self.config.tolerance:
                        break
                    
                    beta = torch.dot(r_new, r_new) / torch.dot(r, r)
                    p = r_new + beta * p
                    r = r_new
            
            return x_tensor.cpu().numpy()
            
        except Exception as e:
            print(f"GPUæ±‚è§£å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
            return self._cpu_solve(A, b)
    
    def _openmp_solve(self, A: np.ndarray, b: np.ndarray) -> np.ndarray:
        """OpenMPå¹¶è¡Œæ±‚è§£"""
        if not HAS_NUMBA:
            return self._cpu_solve(A, b)
        
        # ä½¿ç”¨Numbaçš„å¹¶è¡ŒåŒ–
        @nb.njit(parallel=True)
        def parallel_cg_solve(A_dense, b, max_iter, tol):
            n = len(b)
            x = np.zeros(n)
            r = b.copy()
            p = r.copy()
            
            for iteration in prange(max_iter):
                Ap = np.zeros(n)
                for i in prange(n):
                    for j in range(n):
                        Ap[i] += A_dense[i, j] * p[j]
                
                alpha = np.dot(r, r) / np.dot(p, Ap)
                x = x + alpha * p
                r_new = r - alpha * Ap
                
                if np.linalg.norm(r_new) < tol:
                    break
                
                beta = np.dot(r_new, r_new) / np.dot(r, r)
                p = r_new + beta * p
                r = r_new
            
            return x
        
        return parallel_cg_solve(A.toarray(), b, self.config.max_iterations, self.config.tolerance)
    
    def _cpu_solve(self, A: np.ndarray, b: np.ndarray) -> np.ndarray:
        """CPUæ±‚è§£"""
        if HAS_SCIPY:
            return cg(A, b, maxiter=self.config.max_iterations, tol=self.config.tolerance)[0]
        else:
            # ç®€å•çš„è¿­ä»£æ±‚è§£å™¨
            x = np.zeros_like(b)
            for i in range(self.config.max_iterations):
                x_new = (b - A.dot(x)) / A.diagonal()
                if np.linalg.norm(x_new - x) < self.config.tolerance:
                    break
                x = x_new
            return x


class AdaptiveSolverSelector:
    """è‡ªé€‚åº”æ±‚è§£å™¨é€‰æ‹©å™¨ - é—®é¢˜æ„ŸçŸ¥çš„ç®—æ³•é€‰æ‹©"""
    
    def __init__(self, config: AdvancedParallelConfig):
        self.config = config
        self.solver_performance_db = {}
        self.problem_classifier = self._initialize_problem_classifier()
        
    def _initialize_problem_classifier(self):
        """åˆå§‹åŒ–é—®é¢˜åˆ†ç±»å™¨"""
        class ProblemClassifier:
            def __init__(self):
                self.feature_weights = {
                    'size': 0.3,
                    'sparsity': 0.25,
                    'condition_number': 0.25,
                    'structure': 0.2
                }
            
            def classify_problem(self, A: np.ndarray, b: np.ndarray) -> str:
                features = self._extract_features(A, b)
                
                # åŸºäºç‰¹å¾çš„é—®é¢˜åˆ†ç±»
                if features['size'] > 10000 and features['sparsity'] > 0.9:
                    return 'large_sparse'
                elif features['condition_number'] > 1e6:
                    return 'ill_conditioned'
                elif features['structure'] == 'block_diagonal':
                    return 'block_structured'
                else:
                    return 'general'
            
            def _extract_features(self, A: np.ndarray, b: np.ndarray) -> Dict:
                return {
                    'size': A.shape[0],
                    'sparsity': 1.0 - A.nnz / (A.shape[0] * A.shape[1]) if hasattr(A, 'nnz') else 0.5,
                    'condition_number': np.linalg.cond(A.toarray()) if hasattr(A, 'toarray') else 1e6,
                    'structure': self._analyze_structure(A)
                }
            
            def _analyze_structure(self, A: np.ndarray) -> str:
                # åˆ†æçŸ©é˜µç»“æ„
                if hasattr(A, 'toarray'):
                    A_dense = A.toarray()
                else:
                    A_dense = A
                
                # æ£€æŸ¥å—å¯¹è§’ç»“æ„
                n = A_dense.shape[0]
                block_size = n // 4
                if block_size > 0:
                    off_diagonal_norm = 0
                    for i in range(0, n, block_size):
                        for j in range(0, n, block_size):
                            if i != j:
                                off_diagonal_norm += np.linalg.norm(A_dense[i:i+block_size, j:j+block_size])
                    
                    if off_diagonal_norm < 0.1 * np.linalg.norm(A_dense):
                        return 'block_diagonal'
                
                return 'general'
        
        return ProblemClassifier()
    
    def select_optimal_solver(self, A: np.ndarray, b: np.ndarray,
                            available_solvers: List[str]) -> str:
        """é€‰æ‹©æœ€ä¼˜æ±‚è§£å™¨"""
        # é—®é¢˜åˆ†ç±»
        problem_type = self.problem_classifier.classify_problem(A, b)
        
        # åŸºäºé—®é¢˜ç±»å‹å’Œæ€§èƒ½æ•°æ®åº“é€‰æ‹©æ±‚è§£å™¨
        if problem_type in self.solver_performance_db:
            performance_data = self.solver_performance_db[problem_type]
            
            # é€‰æ‹©æ€§èƒ½æœ€å¥½çš„æ±‚è§£å™¨
            best_solver = max(performance_data.items(), key=lambda x: x[1]['efficiency'])
            if best_solver[0] in available_solvers:
                return best_solver[0]
        
        # é»˜è®¤é€‰æ‹©ç­–ç•¥
        if 'gpu_cg' in available_solvers and A.shape[0] > 5000:
            return 'gpu_cg'
        elif 'openmp_cg' in available_solvers and A.shape[0] > 1000:
            return 'openmp_cg'
        else:
            return 'cpu_cg'
    
    def update_performance_database(self, problem_type: str, solver: str, 
                                  performance: Dict):
        """æ›´æ–°æ€§èƒ½æ•°æ®åº“"""
        if problem_type not in self.solver_performance_db:
            self.solver_performance_db[problem_type] = {}
        
        self.solver_performance_db[problem_type][solver] = performance


# ä¸»æ±‚è§£å™¨ç±»
class AdvancedParallelSolver:
    """é«˜çº§å¹¶è¡Œæ±‚è§£å™¨ - ä¸»ç±»"""
    
    def __init__(self, config: AdvancedParallelConfig):
        self.config = config
        self.comm = MPI.COMM_WORLD if HAS_MPI else None
        self.rank = self.comm.Get_rank() if self.comm else 0
        self.size = self.comm.Get_size() if self.comm else 1
        
        # æ ¸å¿ƒç»„ä»¶
        self.communicator = AdaptiveCommunicator(self.comm)
        self.load_balancer = MLBasedLoadBalancer(self.comm)
        self.heterogeneous_manager = HeterogeneousComputingManager(config)
        self.solver_selector = AdaptiveSolverSelector(config)
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = PerformanceMetrics()
        self.solve_history = []
        
        # åˆå§‹åŒ–
        self._initialize_solver()
    
    def _initialize_solver(self):
        """åˆå§‹åŒ–æ±‚è§£å™¨"""
        if self.rank == 0:
            print(f"ğŸš€ åˆå§‹åŒ–é«˜çº§å¹¶è¡Œæ±‚è§£å™¨")
            print(f"   è¿›ç¨‹æ•°: {self.size}")
            print(f"   GPUæ”¯æŒ: {self.heterogeneous_manager.gpu_available}")
            print(f"   OpenMPæ”¯æŒ: {self.heterogeneous_manager.openmp_available}")
            print(f"   é…ç½®: {self.config}")
    
    def solve(self, A: np.ndarray, b: np.ndarray, 
              mesh_features: Dict = None) -> np.ndarray:
        """ä¸»æ±‚è§£æ–¹æ³•"""
        solve_start_time = time.time()
        
        # 1. é—®é¢˜åˆ†æå’Œæ±‚è§£å™¨é€‰æ‹©
        if self.config.problem_aware_selection:
            optimal_solver = self.solver_selector.select_optimal_solver(
                A, b, ['gpu_cg', 'openmp_cg', 'cpu_cg', 'amg']
            )
        else:
            optimal_solver = self.config.solver_type
        
        # 2. è´Ÿè½½å‡è¡¡
        if self.config.ml_based_balancing:
            current_loads = self._estimate_current_loads(A, b)
            # æ¨¡æ‹Ÿè´Ÿè½½å‡è¡¡
            balanced_info = {'balanced': True}
        else:
            balanced_info = {'balanced': False}
        
        # 3. å¼‚æ„è®¡ç®—æ±‚è§£
        if self.config.use_gpu or self.config.use_openmp:
            solution = self.heterogeneous_manager.solve_with_heterogeneous_computing(
                A, b, optimal_solver
            )
        else:
            solution = self._cpu_solve(A, b)
        
        # 4. æ€§èƒ½ç»Ÿè®¡
        solve_time = time.time() - solve_start_time
        self._update_performance_metrics(solve_time, A, b, solution)
        
        # 5. è®°å½•æ±‚è§£å†å²
        self.solve_history.append({
            'timestamp': time.time(),
            'solver_type': optimal_solver,
            'solve_time': solve_time,
            'problem_size': A.shape[0],
            'performance_metrics': self.performance_metrics
        })
        
        return solution
    
    def _estimate_current_loads(self, A: np.ndarray, b: np.ndarray) -> np.ndarray:
        """ä¼°è®¡å½“å‰è´Ÿè½½"""
        loads = np.zeros(self.size)
        local_size = A.shape[0] // self.size
        loads[self.rank] = local_size
        
        # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„è´Ÿè½½ä¿¡æ¯
        all_loads = self.comm.allgather(local_size) if self.comm else [local_size]
        return np.array(all_loads)
    
    def _cpu_solve(self, A: np.ndarray, b: np.ndarray) -> np.ndarray:
        """CPUæ±‚è§£"""
        if HAS_SCIPY:
            return cg(A, b, maxiter=self.config.max_iterations, tol=self.config.tolerance)[0]
        else:
            # ç®€å•çš„è¿­ä»£æ±‚è§£å™¨
            x = np.zeros_like(b)
            for i in range(self.config.max_iterations):
                x_new = (b - A.dot(x)) / A.diagonal()
                if np.linalg.norm(x_new - x) < self.config.tolerance:
                    break
                x = x_new
            return x
    
    def _update_performance_metrics(self, solve_time: float, A: np.ndarray, 
                                  b: np.ndarray, solution: np.ndarray):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics.total_solve_time = solve_time
        self.performance_metrics.iterations = self.config.max_iterations
        
        # è®¡ç®—æ®‹å·®
        if hasattr(A, 'dot'):
            residual = b - A.dot(solution)
        else:
            residual = b - np.dot(A, solution)
        
        self.performance_metrics.residual_norm = np.linalg.norm(residual)
        
        # å¹¶è¡Œæ•ˆç‡
        if self.size > 1:
            ideal_time = solve_time * self.size
            self.performance_metrics.parallel_efficiency = ideal_time / solve_time
            self.performance_metrics.speedup = self.size / self.performance_metrics.parallel_efficiency
    
    def get_performance_summary(self) -> Dict:
        """è·å–æ€§èƒ½æ€»ç»“"""
        return {
            'total_solves': len(self.solve_history),
            'average_solve_time': np.mean([h['solve_time'] for h in self.solve_history]) if self.solve_history else 0,
            'best_solver': max(self.solve_history, key=lambda x: x['solve_time']) if self.solve_history else None,
            'performance_metrics': self.performance_metrics,
            'hardware_info': {
                'gpu_available': self.heterogeneous_manager.gpu_available,
                'openmp_available': self.heterogeneous_manager.openmp_available,
                'mpi_size': self.size
            }
        }
    
    def benchmark_performance(self, test_problems: List[Tuple[np.ndarray, np.ndarray]]) -> Dict:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        benchmark_results = {
            'solver_times': [],
            'problem_sizes': [],
            'performance_metrics': []
        }
        
        for i, (A, b) in enumerate(test_problems):
            print(f"ğŸ” åŸºå‡†æµ‹è¯• {i+1}/{len(test_problems)}: é—®é¢˜è§„æ¨¡ {A.shape[0]}")
            
            # æ±‚è§£å™¨æµ‹è¯•
            start_time = time.time()
            solution = self.solve(A, b)
            solve_time = time.time() - start_time
            
            benchmark_results['solver_times'].append(solve_time)
            benchmark_results['problem_sizes'].append(A.shape[0])
            benchmark_results['performance_metrics'].append(self.performance_metrics)
            
            print(f"   æ±‚è§£æ—¶é—´: {solve_time:.4f}s")
            print(f"   æ®‹å·®èŒƒæ•°: {self.performance_metrics.residual_norm:.2e}")
        
        return benchmark_results


class LoadBalancer:
    """è´Ÿè½½å‡è¡¡å™¨"""
    
    def __init__(self, comm=None):
        self.comm = comm
        self.rank = comm.Get_rank() if comm else 0
        self.size = comm.Get_size() if comm else 1
        self.load_history = []
        self.balance_threshold = 0.1
        
    def balance_load(self, partition_info: Dict, load_weights: np.ndarray) -> Dict:
        """è´Ÿè½½å‡è¡¡"""
        if not self.comm:
            return partition_info
        
        # è®¡ç®—å½“å‰è´Ÿè½½åˆ†å¸ƒ
        current_loads = self._compute_partition_loads(partition_info, load_weights)
        
        # è®¡ç®—è´Ÿè½½ä¸å¹³è¡¡åº¦
        imbalance = self._compute_load_imbalance(current_loads)
        
        # å¦‚æœè´Ÿè½½ä¸å¹³è¡¡åº¦è¶…è¿‡é˜ˆå€¼ï¼Œè¿›è¡Œé‡æ–°åˆ†é…
        if imbalance > self.balance_threshold:
            partition_info = self._redistribute_load(partition_info, current_loads)
        
        return partition_info
    
    def _compute_partition_loads(self, partition_info: Dict, load_weights: np.ndarray) -> Dict:
        """è®¡ç®—åˆ†åŒºè´Ÿè½½"""
        loads = {}
        for partition_id in range(self.size):
            local_elements = partition_info.get('local_elements', {}).get(partition_id, [])
            load = sum(load_weights[local_elements]) if local_elements else 0
            loads[partition_id] = load
        return loads
    
    def _compute_load_imbalance(self, loads: Dict) -> float:
        """è®¡ç®—è´Ÿè½½ä¸å¹³è¡¡åº¦"""
        total_load = sum(loads.values())
        avg_load = total_load / len(loads)
        max_load = max(loads.values())
        min_load = min(loads.values())
        
        return (max_load - min_load) / avg_load if avg_load > 0 else 0
    
    def _redistribute_load(self, partition_info: Dict, current_loads: Dict) -> Dict:
        """é‡æ–°åˆ†é…è´Ÿè½½"""
        total_load = sum(current_loads.values())
        target_load = total_load / len(current_loads)
        
        # è¯†åˆ«è¿‡è½½å’Œæ¬ è½½çš„åˆ†åŒº
        overloaded = []
        underloaded = []
        
        for partition_id, load in current_loads.items():
            if load > target_load * 1.1:
                overloaded.append(partition_id)
            elif load < target_load * 0.9:
                underloaded.append(partition_id)
        
        # é‡æ–°åˆ†é…å…ƒç´ 
        for overloaded_partition in overloaded:
            excess_load = current_loads[overloaded_partition] - target_load
            
            for underloaded_partition in underloaded:
                if excess_load <= 0:
                    break
                
                transferable_elements = self._find_transferable_elements(
                    partition_info, overloaded_partition, underloaded_partition, excess_load
                )
                
                if transferable_elements:
                    partition_info = self._transfer_elements(
                        partition_info, overloaded_partition, underloaded_partition, transferable_elements
                    )
                    
                    transferred_load = sum(transferable_elements)
                    excess_load -= transferred_load
        
        return partition_info
    
    def _find_transferable_elements(self, partition_info: Dict, from_partition: int, 
                                  to_partition: int, max_load: float) -> List[int]:
        """æ‰¾åˆ°å¯è½¬ç§»çš„å…ƒç´ """
        boundary_elements = partition_info.get('boundary_elements', {}).get(from_partition, [])
        transferable = []
        current_load = 0
        
        for element in boundary_elements:
            if current_load + 1 <= max_load:  # ç®€åŒ–ï¼šå‡è®¾æ¯ä¸ªå…ƒç´ è´Ÿè½½ä¸º1
                transferable.append(element)
                current_load += 1
            else:
                break
        
        return transferable
    
    def _transfer_elements(self, partition_info: Dict, from_partition: int, 
                          to_partition: int, elements: List[int]) -> Dict:
        """è½¬ç§»å…ƒç´ """
        # æ›´æ–°æœ¬åœ°å…ƒç´ åˆ—è¡¨
        partition_info['local_elements'][from_partition] = [
            e for e in partition_info['local_elements'][from_partition] if e not in elements
        ]
        partition_info['local_elements'][to_partition].extend(elements)
        
        return partition_info


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {}
        self.start_time = time.time()
    
    def start_timer(self, name: str):
        """å¼€å§‹è®¡æ—¶"""
        self.metrics[name] = {'start': time.time()}
    
    def end_timer(self, name: str):
        """ç»“æŸè®¡æ—¶"""
        if name in self.metrics:
            self.metrics[name]['end'] = time.time()
            self.metrics[name]['duration'] = self.metrics[name]['end'] - self.metrics[name]['start']
    
    def get_metrics(self) -> Dict:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return self.metrics.copy()


class JacobiPreconditioner:
    """Jacobié¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self.diagonal = None
    
    def setup(self, A: sp.spmatrix):
        """è®¾ç½®é¢„å¤„ç†å™¨"""
        self.diagonal = 1.0 / A.diagonal()
    
    def apply(self, x: np.ndarray) -> np.ndarray:
        """åº”ç”¨é¢„å¤„ç†å™¨"""
        if self.diagonal is None:
            return x
        return self.diagonal * x


class ILUPreconditioner:
    """ILUé¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self.L = None
        self.U = None
    
    def setup(self, A: sp.spmatrix):
        """è®¾ç½®é¢„å¤„ç†å™¨"""
        # ç®€åŒ–çš„ILUå®ç°
        from scipy.sparse.linalg import spilu
        try:
            ilu = spilu(A.tocsc())
            self.L = ilu.L
            self.U = ilu.U
        except:
            # å¦‚æœILUå¤±è´¥ï¼Œä½¿ç”¨Jacobi
            self.diagonal = 1.0 / A.diagonal()
    
    def apply(self, x: np.ndarray) -> np.ndarray:
        """åº”ç”¨é¢„å¤„ç†å™¨"""
        if self.L is not None and self.U is not None:
            # æ±‚è§£ L * U * y = x
            y = spsolve(self.L, x)
            return spsolve(self.U, y)
        elif hasattr(self, 'diagonal'):
            return self.diagonal * x
        else:
            return x


# å¹¶è¡Œæ±‚è§£å™¨å®ç°
class ParallelCGSolver:
    """å¹¶è¡Œå…±è½­æ¢¯åº¦æ±‚è§£å™¨"""
    
    def __init__(self, config: ParallelConfig = None):
        self.config = config or ParallelConfig()
        self.comm = MPI.COMM_WORLD if HAS_MPI else None
        self.rank = self.comm.Get_rank() if self.comm else 0
        self.size = self.comm.Get_size() if self.comm else 1
        self.stats = PerformanceStats()
        
        # é€šä¿¡ä¼˜åŒ–
        self.communication_optimizer = AdaptiveCommunicator(self.comm)
        self.communication_buffer = {}
        self.nonblocking_requests = []
        
        # è´Ÿè½½å‡è¡¡
        self.load_balancer = LoadBalancer(self.comm)
        self.load_weights = None
        self.partition_info = None
        
        # æ€§èƒ½ç›‘æ§
        self.performance_monitor = PerformanceMonitor()
        
        # é¢„å¤„ç†å™¨
        self.preconditioner = self._create_preconditioner()
    
    def solve(self, A: sp.spmatrix, b: np.ndarray, x0: np.ndarray = None) -> np.ndarray:
        """å¹¶è¡ŒCGæ±‚è§£"""
        if not HAS_MPI:
            return self._serial_solve(A, b, x0)
        
        self.performance_monitor.start_timer('solve')
        
        # åˆå§‹åŒ–
        if x0 is None:
            x = np.zeros_like(b)
        else:
            x = x0.copy()
        
        # è®¾ç½®é¢„å¤„ç†å™¨
        if self.preconditioner:
            self.preconditioner.setup(A)
        
        # è®¡ç®—åˆå§‹æ®‹å·®
        r = b - A @ x
        if self.preconditioner:
            z = self.preconditioner.apply(r)
        else:
            z = r.copy()
        
        p = z.copy()
        
        # è®¡ç®—åˆå§‹æ®‹å·®èŒƒæ•°
        r_norm_sq = self._parallel_dot(r, z)
        r_norm_0 = np.sqrt(r_norm_sq)
        
        # CGè¿­ä»£
        for iteration in range(self.config.max_iterations):
            # è®¡ç®—Ap
            Ap = A @ p
            
            # è®¡ç®—alpha
            alpha = r_norm_sq / self._parallel_dot(p, Ap)
            
            # æ›´æ–°è§£å’Œæ®‹å·®
            x += alpha * p
            r_new = r - alpha * Ap
            
            # åº”ç”¨é¢„å¤„ç†å™¨
            if self.preconditioner:
                z_new = self.preconditioner.apply(r_new)
            else:
                z_new = r_new.copy()
            
            # è®¡ç®—æ–°çš„æ®‹å·®èŒƒæ•°
            r_new_norm_sq = self._parallel_dot(r_new, z_new)
            r_norm = np.sqrt(r_new_norm_sq)
            
            # æ£€æŸ¥æ”¶æ•›æ€§
            if r_norm < self.config.tolerance * r_norm_0:
                self.stats.iterations = iteration + 1
                break
            
            # è®¡ç®—beta
            beta = r_new_norm_sq / r_norm_sq
            
            # æ›´æ–°æœç´¢æ–¹å‘
            p = z_new + beta * p
            r = r_new
            z = z_new
            r_norm_sq = r_new_norm_sq
        
        self.performance_monitor.end_timer('solve')
        self.stats.solve_time = self.performance_monitor.metrics['solve']['duration']
        self.stats.residual_norm = r_norm
        
        return x
    
    def _parallel_dot(self, a: np.ndarray, b: np.ndarray) -> float:
        """å¹¶è¡Œç‚¹ç§¯"""
        if not HAS_MPI:
            return np.dot(a, b)
        
        local_dot = np.dot(a, b)
        global_dot = self.comm.allreduce(local_dot, op=MPI.SUM)
        return global_dot
    
    def _serial_solve(self, A: sp.spmatrix, b: np.ndarray, x0: np.ndarray = None) -> np.ndarray:
        """ä¸²è¡Œæ±‚è§£"""
        if x0 is None:
            x0 = np.zeros_like(b)
        
        # ä½¿ç”¨scipyçš„CGæ±‚è§£å™¨
        x, info = cg(A, b, x0=x0, maxiter=self.config.max_iterations, tol=self.config.tolerance)
        
        if info != 0:
            warnings.warn(f"CGæ±‚è§£å™¨æœªæ”¶æ•›ï¼Œè¿­ä»£æ¬¡æ•°: {info}")
        
        return x
    
    def _create_preconditioner(self):
        """åˆ›å»ºé¢„å¤„ç†å™¨"""
        if self.config.preconditioner == 'jacobi':
            return JacobiPreconditioner()
        elif self.config.preconditioner == 'ilu':
            return ILUPreconditioner()
        else:
            return None


class ParallelGMRESSolver:
    """å¹¶è¡ŒGMRESæ±‚è§£å™¨"""
    
    def __init__(self, config: ParallelConfig = None, restart: int = 30):
        self.config = config or ParallelConfig()
        self.config.solver_type = 'gmres'
        self.restart = restart
        
        self.comm = MPI.COMM_WORLD if HAS_MPI else None
        self.rank = self.comm.Get_rank() if self.comm else 0
        self.size = self.comm.Get_size() if self.comm else 1
        self.stats = PerformanceStats()
        
    def solve(self, A: sp.spmatrix, b: np.ndarray, x0: np.ndarray = None) -> np.ndarray:
        """å¹¶è¡ŒGMRESæ±‚è§£"""
        if not HAS_MPI:
            return self._serial_solve(A, b, x0)
        
        # åˆå§‹åŒ–
        if x0 is None:
            x = np.zeros_like(b)
        else:
            x = x0.copy()
        
        # GMRESè¿­ä»£
        for outer_iter in range(self.config.max_iterations // self.restart):
            # è®¡ç®—æ®‹å·®
            r = b - A @ x
            
            # æ£€æŸ¥æ”¶æ•›æ€§
            r_norm = np.sqrt(self._parallel_dot(r, r))
            if r_norm < self.config.tolerance:
                self.stats.iterations = outer_iter * self.restart
                break
            
            # å†…å±‚GMRESè¿­ä»£
            x = self._gmres_inner(A, r, x)
        
        return x
    
    def _gmres_inner(self, A: sp.spmatrix, r: np.ndarray, x: np.ndarray) -> np.ndarray:
        """GMRESå†…å±‚è¿­ä»£"""
        # ç®€åŒ–çš„GMRESå®ç°
        if HAS_SCIPY:
            dx, info = gmres(A, r, maxiter=self.restart, tol=self.config.tolerance)
            if info == 0:
                return x + dx
        
        # å¦‚æœGMRESå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„è¿­ä»£
        return x + 0.1 * r
    
    def _serial_solve(self, A: sp.spmatrix, b: np.ndarray, x0: np.ndarray = None) -> np.ndarray:
        """ä¸²è¡Œæ±‚è§£"""
        if x0 is None:
            x0 = np.zeros_like(b)
        
        x, info = gmres(A, b, x0=x0, maxiter=self.config.max_iterations, 
                       restart=self.restart, tol=self.config.tolerance)
        
        if info != 0:
            warnings.warn(f"GMRESæ±‚è§£å™¨æœªæ”¶æ•›ï¼Œè¿­ä»£æ¬¡æ•°: {info}")
        
        return x
    
    def _parallel_dot(self, a: np.ndarray, b: np.ndarray) -> float:
        """å¹¶è¡Œç‚¹ç§¯"""
        if not HAS_MPI:
            return np.dot(a, b)
        
        local_dot = np.dot(a, b)
        global_dot = self.comm.allreduce(local_dot, op=MPI.SUM)
        return global_dot


# å·¥å‚å‡½æ•°
def create_parallel_solver(solver_type: str = 'cg', config: ParallelConfig = None) -> ParallelCGSolver:
    """åˆ›å»ºå¹¶è¡Œæ±‚è§£å™¨"""
    if solver_type == 'cg':
        return ParallelCGSolver(config)
    elif solver_type == 'gmres':
        return ParallelGMRESSolver(config)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ±‚è§£å™¨ç±»å‹: {solver_type}")


def create_parallel_config(**kwargs) -> ParallelConfig:
    """åˆ›å»ºå¹¶è¡Œé…ç½®"""
    return ParallelConfig(**kwargs)


def create_advanced_solver(config: AdvancedParallelConfig = None) -> AdvancedParallelSolver:
    """åˆ›å»ºé«˜çº§å¹¶è¡Œæ±‚è§£å™¨"""
    if config is None:
        config = AdvancedParallelConfig()
    return AdvancedParallelSolver(config)


# æ€§èƒ½åŸºå‡†æµ‹è¯•å‡½æ•°
def run_comprehensive_benchmark():
    """è¿è¡Œå…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹é«˜çº§å¹¶è¡Œæ±‚è§£å™¨çš„å…¨é¢æ€§èƒ½åŸºå‡†æµ‹è¯•")
    
    # é…ç½®
    config = AdvancedParallelConfig(
        solver_type='adaptive',
        use_gpu=True,
        use_openmp=True,
        ml_based_balancing=True,
        adaptive_communication=True
    )
    
    # åˆ›å»ºæ±‚è§£å™¨
    solver = AdvancedParallelSolver(config)
    
    # ç”Ÿæˆæµ‹è¯•é—®é¢˜
    test_problems = []
    problem_sizes = [1000, 5000, 10000, 50000, 100000]
    
    for size in problem_sizes:
        # åˆ›å»ºç¨€ç–çŸ©é˜µ
        A = sp.random(size, size, density=0.01, format='csr')
        A = A + A.T + sp.eye(size)  # ç¡®ä¿æ­£å®šæ€§
        b = np.random.randn(size)
        
        test_problems.append((A, b))
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    results = solver.benchmark_performance(test_problems)
    
    # è¾“å‡ºç»“æœ
    print("\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœæ€»ç»“:")
    print("=" * 60)
    
    for i, size in enumerate(problem_sizes):
        solve_time = results['solver_times'][i]
        print(f"é—®é¢˜è§„æ¨¡ {size:6d}: æ±‚è§£æ—¶é—´ {solve_time:8.4f}s")
    
    # æ€§èƒ½æ€»ç»“
    performance_summary = solver.get_performance_summary()
    print(f"\nğŸ“ˆ æ€§èƒ½æ€»ç»“:")
    print(f"   æ€»æ±‚è§£æ¬¡æ•°: {performance_summary['total_solves']}")
    print(f"   å¹³å‡æ±‚è§£æ—¶é—´: {performance_summary['average_solve_time']:.4f}s")
    print(f"   ç¡¬ä»¶æ”¯æŒ: GPU={performance_summary['hardware_info']['gpu_available']}, "
          f"OpenMP={performance_summary['hardware_info']['openmp_available']}")
    
    return results, performance_summary


if __name__ == "__main__":
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark_results, performance_summary = run_comprehensive_benchmark()
    
    # ä¿å­˜ç»“æœ
    with open('advanced_parallel_benchmark_results.json', 'w') as f:
        json.dump({
            'benchmark_results': benchmark_results,
            'performance_summary': performance_summary,
            'timestamp': time.time()
        }, f, indent=2)
    
    print(f"\nğŸ’¾ åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° 'advanced_parallel_benchmark_results.json'")
