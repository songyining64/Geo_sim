"""
å¤šä¿çœŸåº¦å»ºæ¨¡æ¨¡å—

å®ç°"ä½ä¿çœŸåº¦æ•°æ®é¢„è®­ç»ƒ + é«˜ä¿çœŸåº¦æ•°æ®å¾®è°ƒ"çš„ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œ
ä½¿ç”¨ååŒè®­ç»ƒè®©ä½æˆæœ¬æ¨¡å‹è¾…åŠ©é«˜æˆæœ¬æ¨¡å‹å­¦ä¹ ï¼Œå‡å°‘å¤§è§„æ¨¡ä»¿çœŸçš„è®¡ç®—æˆæœ¬ã€‚
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
from abc import ABC, abstractmethod
import warnings
import time
from dataclasses import dataclass, field
import json
from pathlib import Path

# å¯é€‰ä¾èµ–
try:
    import torch.nn.functional as F
    HAS_TORCH_FUNCTIONAL = True
except ImportError:
    HAS_TORCH_FUNCTIONAL = False
    warnings.warn("torch.nn.functional not available. Some features will be limited.")

try:
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.metrics import mean_squared_error, r2_score
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    warnings.warn("scikit-learn not available. Traditional ML models will be limited.")


@dataclass
class FidelityLevel:
    """ä¿çœŸåº¦çº§åˆ«å®šä¹‰"""
    
    name: str
    level: int  # 0=æœ€ä½ä¿çœŸåº¦, 1,2,3...=é€’å¢ä¿çœŸåº¦
    description: str
    computational_cost: float  # ç›¸å¯¹è®¡ç®—æˆæœ¬
    accuracy: float  # é¢„æœŸç²¾åº¦ (0-1)
    data_requirements: int  # æ‰€éœ€æ•°æ®é‡
    training_time: float  # é¢„æœŸè®­ç»ƒæ—¶é—´ (ç§’)
    
    # æ¨¡å‹å‚æ•°
    model_type: str  # 'neural_network', 'random_forest', 'gradient_boosting'
    model_params: Dict[str, Any] = field(default_factory=dict)
    
    # è®­ç»ƒå‚æ•°
    training_params: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """è®¾ç½®é»˜è®¤å€¼"""
        if not self.model_params:
            if self.model_type == 'neural_network':
                self.model_params = {
                    'hidden_layers': [64, 32],
                    'activation': 'relu',
                    'dropout': 0.1
                }
            elif self.model_type == 'random_forest':
                self.model_params = {
                    'n_estimators': 100,
                    'max_depth': 10,
                    'random_state': 42
                }
            elif self.model_type == 'gradient_boosting':
                self.model_params = {
                    'n_estimators': 100,
                    'learning_rate': 0.1,
                    'max_depth': 6,
                    'random_state': 42
                }
        
        if not self.training_params:
            self.training_params = {
                'epochs': 100,
                'batch_size': 32,
                'learning_rate': 0.001,
                'validation_split': 0.2
            }


@dataclass
class MultiFidelityConfig:
    """å¤šä¿çœŸåº¦é…ç½®"""
    
    name: str
    description: str
    fidelity_levels: List[FidelityLevel]
    
    # ååŒè®­ç»ƒå‚æ•°
    co_training:
        enabled: bool = True
        transfer_learning: bool = True
        knowledge_distillation: bool = True
        ensemble_method: str = 'weighted_average'  # 'weighted_average', 'stacking', 'boosting'
    
    # è®­ç»ƒç­–ç•¥
    training_strategy:
        stage1_epochs: int = 1000  # ä½ä¿çœŸåº¦é¢„è®­ç»ƒ
        stage2_epochs: int = 500   # é«˜ä¿çœŸåº¦å¾®è°ƒ
        transfer_epochs: int = 200 # çŸ¥è¯†è¿ç§»
        distillation_epochs: int = 100  # çŸ¥è¯†è’¸é¦
    
    # æ€§èƒ½ä¼˜åŒ–
    performance:
        parallel_training: bool = True
        early_stopping: bool = True
        adaptive_learning_rate: bool = True
        memory_optimization: bool = True
    
    def get_fidelity_level(self, level: int) -> Optional[FidelityLevel]:
        """è·å–æŒ‡å®šä¿çœŸåº¦çº§åˆ«"""
        for fidelity in self.fidelity_levels:
            if fidelity.level == level:
                return fidelity
        return None
    
    def get_lowest_fidelity(self) -> Optional[FidelityLevel]:
        """è·å–æœ€ä½ä¿çœŸåº¦çº§åˆ«"""
        if not self.fidelity_levels:
            return None
        return min(self.fidelity_levels, key=lambda x: x.level)
    
    def get_highest_fidelity(self) -> Optional[FidelityLevel]:
        """è·å–æœ€é«˜ä¿çœŸåº¦çº§åˆ«"""
        if not self.fidelity_levels:
            return None
        return max(self.fidelity_levels, key=lambda x: x.level)


class BaseFidelityModel(ABC):
    """ä¿çœŸåº¦æ¨¡å‹åŸºç±»"""
    
    def __init__(self, fidelity_level: FidelityLevel):
        self.fidelity_level = fidelity_level
        self.model = None
        self.is_trained = False
        self.training_history = []
        self.validation_metrics = {}
        
    @abstractmethod
    def build_model(self, input_dim: int, output_dim: int):
        """æ„å»ºæ¨¡å‹"""
        pass
    
    @abstractmethod
    def train(self, X_train: np.ndarray, y_train: np.ndarray,
              X_val: Optional[np.ndarray] = None, y_val: Optional[np.ndarray] = None,
              **kwargs) -> Dict[str, Any]:
        """è®­ç»ƒæ¨¡å‹"""
        pass
    
    @abstractmethod
    def predict(self, X: np.ndarray) -> np.ndarray:
        """æ¨¡å‹é¢„æµ‹"""
        pass
    
    @abstractmethod
    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        pass
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•ä¿å­˜")
        
        model_data = {
            'fidelity_level': self.fidelity_level.name,
            'model_type': self.fidelity_level.model_type,
            'model_params': self.fidelity_level.model_params,
            'training_history': self.training_history,
            'validation_metrics': self.validation_metrics,
            'is_trained': self.is_trained
        }
        
        # ä¿å­˜æ¨¡å‹æƒé‡
        if hasattr(self.model, 'state_dict'):
            model_data['model_weights'] = self.model.state_dict()
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if filepath.endswith('.json'):
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(model_data, f, indent=2, ensure_ascii=False)
        else:
            torch.save(model_data, filepath)
        
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        if filepath.endswith('.json'):
            with open(filepath, 'r', encoding='utf-8') as f:
                model_data = json.load(f)
        else:
            model_data = torch.load(filepath, map_location='cpu')
        
        # æ¢å¤æ¨¡å‹çŠ¶æ€
        self.training_history = model_data.get('training_history', [])
        self.validation_metrics = model_data.get('validation_metrics', {})
        self.is_trained = model_data.get('is_trained', False)
        
        # æ¢å¤æ¨¡å‹æƒé‡
        if 'model_weights' in model_data and hasattr(self.model, 'load_state_dict'):
            self.model.load_state_dict(model_data['model_weights'])
        
        print(f"æ¨¡å‹å·²ä» {filepath} åŠ è½½")


class NeuralNetworkFidelityModel(BaseFidelityModel):
    """ç¥ç»ç½‘ç»œä¿çœŸåº¦æ¨¡å‹"""
    
    def __init__(self, fidelity_level: FidelityLevel):
        super().__init__(fidelity_level)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def build_model(self, input_dim: int, output_dim: int):
        """æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹"""
        layers = []
        prev_dim = input_dim
        
        # éšè—å±‚
        for hidden_dim in self.fidelity_level.model_params['hidden_layers']:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            if self.fidelity_level.model_params.get('dropout', 0) > 0:
                layers.append(nn.Dropout(self.fidelity_level.model_params['dropout']))
            prev_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.model = nn.Sequential(*layers).to(self.device)
        
    def train(self, X_train: np.ndarray, y_train: np.ndarray,
              X_val: Optional[np.ndarray] = None, y_val: Optional[np.ndarray] = None,
              **kwargs) -> Dict[str, Any]:
        """è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹"""
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_model")
        
        # è½¬æ¢ä¸ºå¼ é‡
        X_train_tensor = torch.FloatTensor(X_train).to(self.device)
        y_train_tensor = torch.FloatTensor(y_train).to(self.device)
        
        if X_val is not None and y_val is not None:
            X_val_tensor = torch.FloatTensor(X_val).to(self.device)
            y_val_tensor = torch.FloatTensor(y_val).to(self.device)
        
        # ä¼˜åŒ–å™¨
        optimizer = optim.Adam(
            self.model.parameters(),
            lr=self.fidelity_level.training_params.get('learning_rate', 0.001)
        )
        
        # æŸå¤±å‡½æ•°
        criterion = nn.MSELoss()
        
        # è®­ç»ƒå‚æ•°
        epochs = kwargs.get('epochs', self.fidelity_level.training_params['epochs'])
        batch_size = self.fidelity_level.training_params.get('batch_size', 32)
        
        # è®­ç»ƒå¾ªç¯
        self.model.train()
        for epoch in range(epochs):
            # æ‰¹æ¬¡è®­ç»ƒ
            total_loss = 0.0
            num_batches = 0
            
            for i in range(0, len(X_train), batch_size):
                batch_X = X_train_tensor[i:i+batch_size]
                batch_y = y_train_tensor[i:i+batch_size]
                
                optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
            
            # è®°å½•è®­ç»ƒå†å²
            epoch_info = {
                'epoch': epoch,
                'train_loss': total_loss / num_batches if num_batches > 0 else 0.0,
                'timestamp': time.time()
            }
            
            # éªŒè¯
            if X_val is not None and y_val is not None:
                self.model.eval()
                with torch.no_grad():
                    val_outputs = self.model(X_val_tensor)
                    val_loss = criterion(val_outputs, y_val_tensor).item()
                    epoch_info['val_loss'] = val_loss
                self.model.train()
            
            self.training_history.append(epoch_info)
            
            # æ—©åœæ£€æŸ¥
            if (self.fidelity_level.performance.early_stopping and 
                len(self.training_history) > 10):
                recent_val_losses = [h['val_loss'] for h in self.training_history[-10:] 
                                   if 'val_loss' in h]
                if len(recent_val_losses) >= 5:
                    if all(recent_val_losses[i] <= recent_val_losses[i-1] 
                           for i in range(1, len(recent_val_losses))):
                        print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
                        break
        
        self.is_trained = True
        
        # è®¡ç®—æœ€ç»ˆéªŒè¯æŒ‡æ ‡
        if X_val is not None and y_val is not None:
            self.validation_metrics = self.evaluate(X_val, y_val)
        
        return {
            'total_epochs': len(self.training_history),
            'final_train_loss': self.training_history[-1]['train_loss'],
            'final_val_loss': self.training_history[-1].get('val_loss', 0.0),
            'validation_metrics': self.validation_metrics
        }
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """æ¨¡å‹é¢„æµ‹"""
        if not self.is_trained:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•é¢„æµ‹")
        
        self.model.eval()
        X_tensor = torch.FloatTensor(X).to(self.device)
        
        with torch.no_grad():
            predictions = self.model(X_tensor)
        
        return predictions.cpu().numpy()
    
    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        if not self.is_trained:
            return {}
        
        predictions = self.predict(X)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        mse = mean_squared_error(y, predictions)
        rmse = np.sqrt(mse)
        r2 = r2_score(y, predictions)
        
        # è®¡ç®—ç›¸å¯¹è¯¯å·®
        relative_error = np.mean(np.abs(predictions - y) / (np.abs(y) + 1e-8))
        
        return {
            'mse': mse,
            'rmse': rmse,
            'r2': r2,
            'relative_error': relative_error
        }


class TraditionalMLFidelityModel(BaseFidelityModel):
    """ä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¿çœŸåº¦æ¨¡å‹"""
    
    def __init__(self, fidelity_level: FidelityLevel):
        super().__init__(fidelity_level)
        
        if not HAS_SKLEARN:
            raise ImportError("éœ€è¦scikit-learnæ¥ä½¿ç”¨ä¼ ç»ŸMLæ¨¡å‹")
    
    def build_model(self, input_dim: int, output_dim: int):
        """æ„å»ºä¼ ç»ŸMLæ¨¡å‹"""
        if self.fidelity_level.model_type == 'random_forest':
            self.model = RandomForestRegressor(**self.fidelity_level.model_params)
        elif self.fidelity_level.model_type == 'gradient_boosting':
            self.model = GradientBoostingRegressor(**self.fidelity_level.model_params)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.fidelity_level.model_type}")
    
    def train(self, X_train: np.ndarray, y_train: np.ndarray,
              X_val: Optional[np.ndarray] = None, y_val: Optional[np.ndarray] = None,
              **kwargs) -> Dict[str, Any]:
        """è®­ç»ƒä¼ ç»ŸMLæ¨¡å‹"""
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_model")
        
        start_time = time.time()
        
        # è®­ç»ƒæ¨¡å‹
        if y_train.ndim == 1:
            y_train = y_train.reshape(-1, 1)
        
        self.model.fit(X_train, y_train)
        
        training_time = time.time() - start_time
        
        # è®°å½•è®­ç»ƒå†å²
        self.training_history.append({
            'training_time': training_time,
            'timestamp': time.time()
        })
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        if X_val is not None and y_val is not None:
            self.validation_metrics = self.evaluate(X_val, y_val)
        
        self.is_trained = True
        
        return {
            'training_time': training_time,
            'validation_metrics': self.validation_metrics
        }
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """æ¨¡å‹é¢„æµ‹"""
        if not self.is_trained:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•é¢„æµ‹")
        
        predictions = self.model.predict(X)
        
        # ç¡®ä¿è¾“å‡ºæ˜¯2Dæ•°ç»„
        if predictions.ndim == 1:
            predictions = predictions.reshape(-1, 1)
        
        return predictions
    
    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        if not self.is_trained:
            return {}
        
        predictions = self.predict(X)
        
        # ç¡®ä¿yæ˜¯2Dæ•°ç»„
        if y.ndim == 1:
            y = y.reshape(-1, 1)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = {}
        for i in range(y.shape[1]):
            mse = mean_squared_error(y[:, i], predictions[:, i])
            rmse = np.sqrt(mse)
            r2 = r2_score(y[:, i], predictions[:, i])
            relative_error = np.mean(np.abs(predictions[:, i] - y[:, i]) / (np.abs(y[:, i]) + 1e-8))
            
            metrics[f'mse_output_{i}'] = mse
            metrics[f'rmse_output_{i}'] = rmse
            metrics[f'r2_output_{i}'] = r2
            metrics[f'relative_error_output_{i}'] = relative_error
        
        # æ€»ä½“æŒ‡æ ‡
        metrics['overall_mse'] = np.mean([metrics[f'mse_output_{i}'] for i in range(y.shape[1])])
        metrics['overall_r2'] = np.mean([metrics[f'r2_output_{i}'] for i in range(y.shape[1])])
        
        return metrics


class MultiFidelityTrainer:
    """å¤šä¿çœŸåº¦è®­ç»ƒå™¨"""
    
    def __init__(self, config: MultiFidelityConfig):
        self.config = config
        self.models: Dict[int, BaseFidelityModel] = {}
        self.training_data: Dict[int, Tuple[np.ndarray, np.ndarray]] = {}
        self.validation_data: Dict[int, Tuple[np.ndarray, np.ndarray]] = {}
        
        # è®­ç»ƒçŠ¶æ€
        self.current_stage = 1
        self.training_progress = {}
        self.ensemble_predictions = {}
        
    def add_training_data(self, fidelity_level: int, X: np.ndarray, y: np.ndarray):
        """æ·»åŠ è®­ç»ƒæ•°æ®"""
        self.training_data[fidelity_level] = (X, y)
        print(f"âœ… æ·»åŠ ä¿çœŸåº¦çº§åˆ« {fidelity_level} çš„è®­ç»ƒæ•°æ®: {X.shape}")
    
    def add_validation_data(self, fidelity_level: int, X: np.ndarray, y: np.ndarray):
        """æ·»åŠ éªŒè¯æ•°æ®"""
        self.validation_data[fidelity_level] = (X, y)
        print(f"âœ… æ·»åŠ ä¿çœŸåº¦çº§åˆ« {fidelity_level} çš„éªŒè¯æ•°æ®: {X.shape}")
    
    def create_models(self, input_dim: int, output_dim: int):
        """åˆ›å»ºæ‰€æœ‰ä¿çœŸåº¦çº§åˆ«çš„æ¨¡å‹"""
        for fidelity in self.config.fidelity_levels:
            if fidelity.model_type == 'neural_network':
                model = NeuralNetworkFidelityModel(fidelity)
            elif fidelity.model_type in ['random_forest', 'gradient_boosting']:
                model = TraditionalMLFidelityModel(fidelity)
            else:
                warnings.warn(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {fidelity.model_type}")
                continue
            
            model.build_model(input_dim, output_dim)
            self.models[fidelity.level] = model
            
            print(f"âœ… åˆ›å»ºä¿çœŸåº¦çº§åˆ« {fidelity.level} çš„æ¨¡å‹: {fidelity.model_type}")
    
    def stage1_low_fidelity_pretraining(self) -> Dict[str, Any]:
        """é˜¶æ®µ1ï¼šä½ä¿çœŸåº¦é¢„è®­ç»ƒ"""
        print("\n=== é˜¶æ®µ1ï¼šä½ä¿çœŸåº¦é¢„è®­ç»ƒ ===")
        
        # è·å–æœ€ä½ä¿çœŸåº¦çº§åˆ«
        lowest_fidelity = self.config.get_lowest_fidelity()
        if not lowest_fidelity:
            raise ValueError("æœªæ‰¾åˆ°æœ€ä½ä¿çœŸåº¦çº§åˆ«")
        
        if lowest_fidelity.level not in self.training_data:
            raise ValueError(f"ç¼ºå°‘ä¿çœŸåº¦çº§åˆ« {lowest_fidelity.level} çš„è®­ç»ƒæ•°æ®")
        
        X_train, y_train = self.training_data[lowest_fidelity.level]
        X_val, y_val = self.validation_data.get(lowest_fidelity.level, (None, None))
        
        # è®­ç»ƒæ¨¡å‹
        model = self.models[lowest_fidelity.level]
        training_result = model.train(
            X_train, y_train, X_val, y_val,
            epochs=self.config.training_strategy.stage1_epochs
        )
        
        self.training_progress['stage1'] = training_result
        print(f"âœ… ä½ä¿çœŸåº¦é¢„è®­ç»ƒå®Œæˆ: {training_result}")
        
        return training_result
    
    def stage2_high_fidelity_finetuning(self) -> Dict[str, Any]:
        """é˜¶æ®µ2ï¼šé«˜ä¿çœŸåº¦å¾®è°ƒ"""
        print("\n=== é˜¶æ®µ2ï¼šé«˜ä¿çœŸåº¦å¾®è°ƒ ===")
        
        # è·å–æœ€é«˜ä¿çœŸåº¦çº§åˆ«
        highest_fidelity = self.config.get_highest_fidelity()
        if not highest_fidelity:
            raise ValueError("æœªæ‰¾åˆ°æœ€é«˜ä¿çœŸåº¦çº§åˆ«")
        
        if highest_fidelity.level not in self.training_data:
            raise ValueError(f"ç¼ºå°‘ä¿çœŸåº¦çº§åˆ« {highest_fidelity.level} çš„è®­ç»ƒæ•°æ®")
        
        X_train, y_train = self.training_data[highest_fidelity.level]
        X_val, y_val = self.validation_data.get(highest_fidelity.level, (None, None))
        
        # çŸ¥è¯†è¿ç§»ï¼šä»ä½ä¿çœŸåº¦æ¨¡å‹è¿ç§»æƒé‡
        if self.config.co_training.transfer_learning:
            self._transfer_knowledge(highest_fidelity.level)
        
        # è®­ç»ƒæ¨¡å‹
        model = self.models[highest_fidelity.level]
        training_result = model.train(
            X_train, y_train, X_val, y_val,
            epochs=self.config.training_strategy.stage2_epochs
        )
        
        self.training_progress['stage2'] = training_result
        print(f"âœ… é«˜ä¿çœŸåº¦å¾®è°ƒå®Œæˆ: {training_result}")
        
        return training_result
    
    def _transfer_knowledge(self, target_fidelity_level: int):
        """çŸ¥è¯†è¿ç§»ï¼šä»ä½ä¿çœŸåº¦æ¨¡å‹è¿ç§»åˆ°é«˜ä¿çœŸåº¦æ¨¡å‹"""
        if not self.config.co_training.transfer_learning:
            return
        
        # æ‰¾åˆ°æœ€ä½ä¿çœŸåº¦çº§åˆ«
        lowest_fidelity = self.config.get_lowest_fidelity()
        if not lowest_fidelity or lowest_fidelity.level not in self.models:
            return
        
        source_model = self.models[lowest_fidelity.level]
        target_model = self.models[target_fidelity_level]
        
        # æ£€æŸ¥æ˜¯å¦éƒ½æ˜¯ç¥ç»ç½‘ç»œæ¨¡å‹
        if (isinstance(source_model, NeuralNetworkFidelityModel) and 
            isinstance(target_model, NeuralNetworkFidelityModel)):
            
            print(f"ğŸ”„ ä»ä¿çœŸåº¦çº§åˆ« {lowest_fidelity.level} è¿ç§»çŸ¥è¯†åˆ°çº§åˆ« {target_fidelity_level}")
            
            # è¿ç§»æƒé‡ï¼ˆå¦‚æœæ¶æ„å…¼å®¹ï¼‰
            try:
                if (hasattr(source_model.model, 'state_dict') and 
                    hasattr(target_model.model, 'state_dict')):
                    
                    source_state = source_model.model.state_dict()
                    target_state = target_model.model.state_dict()
                    
                    # è¿ç§»å…¼å®¹çš„å±‚æƒé‡
                    transferred_count = 0
                    for key in source_state:
                        if key in target_state and source_state[key].shape == target_state[key].shape:
                            target_state[key] = source_state[key]
                            transferred_count += 1
                    
                    target_model.model.load_state_dict(target_state)
                    print(f"âœ… æˆåŠŸè¿ç§» {transferred_count} å±‚æƒé‡")
                    
            except Exception as e:
                warnings.warn(f"çŸ¥è¯†è¿ç§»å¤±è´¥: {str(e)}")
    
    def co_training(self) -> Dict[str, Any]:
        """ååŒè®­ç»ƒï¼šè®©ä½ä¿çœŸåº¦æ¨¡å‹è¾…åŠ©é«˜ä¿çœŸåº¦æ¨¡å‹å­¦ä¹ """
        if not self.config.co_training.enabled:
            return {}
        
        print("\n=== ååŒè®­ç»ƒ ===")
        
        # çŸ¥è¯†è’¸é¦
        if self.config.co_training.knowledge_distillation:
            self._knowledge_distillation()
        
        # é›†æˆé¢„æµ‹
        ensemble_result = self._ensemble_prediction()
        
        self.training_progress['co_training'] = ensemble_result
        print(f"âœ… ååŒè®­ç»ƒå®Œæˆ: {ensemble_result}")
        
        return ensemble_result
    
    def _knowledge_distillation(self):
        """çŸ¥è¯†è’¸é¦ï¼šä»é«˜ä¿çœŸåº¦æ¨¡å‹å‘ä½ä¿çœŸåº¦æ¨¡å‹ä¼ é€’çŸ¥è¯†"""
        if not self.config.co_training.knowledge_distillation:
            return
        
        print("ğŸ”„ æ‰§è¡ŒçŸ¥è¯†è’¸é¦...")
        
        # è·å–æ‰€æœ‰ä¿çœŸåº¦çº§åˆ«
        fidelity_levels = sorted(self.config.fidelity_levels, key=lambda x: x.level)
        
        for i in range(len(fidelity_levels) - 1):
            source_level = fidelity_levels[i + 1]  # é«˜ä¿çœŸåº¦
            target_level = fidelity_levels[i]      # ä½ä¿çœŸåº¦
            
            if (source_level.level in self.models and 
                target_level.level in self.models and
                source_level.level in self.training_data):
                
                source_model = self.models[source_level.level]
                target_model = self.models[target_level.level]
                
                # ä½¿ç”¨é«˜ä¿çœŸåº¦æ¨¡å‹çš„é¢„æµ‹ä½œä¸ºè½¯æ ‡ç­¾
                X_train, _ = self.training_data[source_level.level]
                soft_labels = source_model.predict(X_train)
                
                # åœ¨ä½ä¿çœŸåº¦æ•°æ®ä¸Šå¾®è°ƒ
                if target_level.level in self.training_data:
                    X_target, y_target = self.training_data[target_level.level]
                    
                    # æ··åˆç¡¬æ ‡ç­¾å’Œè½¯æ ‡ç­¾
                    alpha = 0.7  # è½¯æ ‡ç­¾æƒé‡
                    mixed_labels = alpha * soft_labels[:len(y_target)] + (1 - alpha) * y_target
                    
                    # å¾®è°ƒæ¨¡å‹
                    target_model.train(
                        X_target, mixed_labels,
                        epochs=self.config.training_strategy.distillation_epochs
                    )
                    
                    print(f"âœ… å®Œæˆä»çº§åˆ« {source_level.level} åˆ°çº§åˆ« {target_level.level} çš„çŸ¥è¯†è’¸é¦")
    
    def _ensemble_prediction(self) -> Dict[str, Any]:
        """é›†æˆé¢„æµ‹ï¼šç»“åˆå¤šä¸ªä¿çœŸåº¦çº§åˆ«çš„é¢„æµ‹"""
        print("ğŸ”„ æ‰§è¡Œé›†æˆé¢„æµ‹...")
        
        if not self.models:
            return {}
        
        # è·å–éªŒè¯æ•°æ®
        highest_fidelity = self.config.get_highest_fidelity()
        if not highest_fidelity or highest_fidelity.level not in self.validation_data:
            return {}
        
        X_val, y_val = self.validation_data[highest_fidelity.level]
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        predictions = {}
        for level, model in self.models.items():
            if model.is_trained:
                try:
                    pred = model.predict(X_val)
                    predictions[level] = pred
                except Exception as e:
                    warnings.warn(f"ä¿çœŸåº¦çº§åˆ« {level} é¢„æµ‹å¤±è´¥: {str(e)}")
        
        if not predictions:
            return {}
        
        # é›†æˆé¢„æµ‹
        if self.config.co_training.ensemble_method == 'weighted_average':
            ensemble_pred = self._weighted_average_ensemble(predictions)
        elif self.config.co_training.ensemble_method == 'stacking':
            ensemble_pred = self._stacking_ensemble(predictions, y_val)
        else:
            ensemble_pred = self._simple_average_ensemble(predictions)
        
        # è¯„ä¼°é›†æˆæ€§èƒ½
        ensemble_metrics = self._evaluate_ensemble(ensemble_pred, y_val)
        
        # ä¿å­˜é›†æˆé¢„æµ‹ç»“æœ
        self.ensemble_predictions = {
            'individual_predictions': predictions,
            'ensemble_prediction': ensemble_pred,
            'ensemble_metrics': ensemble_metrics
        }
        
        return ensemble_metrics
    
    def _weighted_average_ensemble(self, predictions: Dict[int, np.ndarray]) -> np.ndarray:
        """åŠ æƒå¹³å‡é›†æˆ"""
        # åŸºäºä¿çœŸåº¦çº§åˆ«åˆ†é…æƒé‡
        total_weight = 0
        weighted_sum = None
        
        for level, pred in predictions.items():
            fidelity = self.config.get_fidelity_level(level)
            if fidelity:
                weight = fidelity.accuracy  # ä½¿ç”¨ç²¾åº¦ä½œä¸ºæƒé‡
                total_weight += weight
                
                if weighted_sum is None:
                    weighted_sum = weight * pred
                else:
                    weighted_sum += weight * pred
        
        if total_weight > 0:
            return weighted_sum / total_weight
        else:
            return np.mean(list(predictions.values()), axis=0)
    
    def _stacking_ensemble(self, predictions: Dict[int, np.ndarray], y_true: np.ndarray) -> np.ndarray:
        """å †å é›†æˆ"""
        # ä½¿ç”¨çº¿æ€§å›å½’ä½œä¸ºå…ƒå­¦ä¹ å™¨
        if not HAS_SKLEARN:
            return self._simple_average_ensemble(predictions)
        
        try:
            from sklearn.linear_model import LinearRegression
            
            # å‡†å¤‡å…ƒç‰¹å¾
            meta_features = np.column_stack(list(predictions.values()))
            
            # è®­ç»ƒå…ƒå­¦ä¹ å™¨
            meta_learner = LinearRegression()
            meta_learner.fit(meta_features, y_true)
            
            # é¢„æµ‹
            ensemble_pred = meta_learner.predict(meta_features)
            
            return ensemble_pred
            
        except Exception as e:
            warnings.warn(f"å †å é›†æˆå¤±è´¥: {str(e)}")
            return self._simple_average_ensemble(predictions)
    
    def _simple_average_ensemble(self, predictions: Dict[int, np.ndarray]) -> np.ndarray:
        """ç®€å•å¹³å‡é›†æˆ"""
        return np.mean(list(predictions.values()), axis=0)
    
    def _evaluate_ensemble(self, ensemble_pred: np.ndarray, y_true: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°é›†æˆæ€§èƒ½"""
        if not HAS_SKLEARN:
            return {}
        
        try:
            mse = mean_squared_error(y_true, ensemble_pred)
            rmse = np.sqrt(mse)
            r2 = r2_score(y_true, ensemble_pred)
            relative_error = np.mean(np.abs(ensemble_pred - y_true) / (np.abs(y_true) + 1e-8))
            
            return {
                'ensemble_mse': mse,
                'ensemble_rmse': rmse,
                'ensemble_r2': r2,
                'ensemble_relative_error': relative_error
            }
        except Exception as e:
            warnings.warn(f"é›†æˆè¯„ä¼°å¤±è´¥: {str(e)}")
            return {}
    
    def run_full_training(self, input_dim: int, output_dim: int) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„å¤šä¿çœŸåº¦è®­ç»ƒæµç¨‹"""
        print("ğŸš€ å¼€å§‹å¤šä¿çœŸåº¦è®­ç»ƒæµç¨‹")
        
        # 1. åˆ›å»ºæ¨¡å‹
        self.create_models(input_dim, output_dim)
        
        # 2. é˜¶æ®µ1ï¼šä½ä¿çœŸåº¦é¢„è®­ç»ƒ
        stage1_result = self.stage1_low_fidelity_pretraining()
        
        # 3. é˜¶æ®µ2ï¼šé«˜ä¿çœŸåº¦å¾®è°ƒ
        stage2_result = self.stage2_high_fidelity_finetuning()
        
        # 4. ååŒè®­ç»ƒ
        co_training_result = self.co_training()
        
        # 5. æœ€ç»ˆè¯„ä¼°
        final_evaluation = self._final_evaluation()
        
        # æ±‡æ€»ç»“æœ
        training_summary = {
            'stage1': stage1_result,
            'stage2': stage2_result,
            'co_training': co_training_result,
            'final_evaluation': final_evaluation,
            'total_training_time': sum([
                stage1_result.get('training_time', 0),
                stage2_result.get('training_time', 0)
            ]),
            'model_performance': self._get_model_performance_summary()
        }
        
        print("\nğŸ‰ å¤šä¿çœŸåº¦è®­ç»ƒå®Œæˆï¼")
        print(f"è®­ç»ƒæ‘˜è¦: {training_summary}")
        
        return training_summary
    
    def _final_evaluation(self) -> Dict[str, Any]:
        """æœ€ç»ˆè¯„ä¼°"""
        print("\n=== æœ€ç»ˆè¯„ä¼° ===")
        
        evaluation_results = {}
        
        # è¯„ä¼°æ¯ä¸ªæ¨¡å‹
        for level, model in self.models.items():
            if model.is_trained and level in self.validation_data:
                X_val, y_val = self.validation_data[level]
                metrics = model.evaluate(X_val, y_val)
                evaluation_results[f'level_{level}'] = metrics
                
                print(f"ä¿çœŸåº¦çº§åˆ« {level} æ€§èƒ½: {metrics}")
        
        # é›†æˆæ¨¡å‹æ€§èƒ½
        if self.ensemble_predictions:
            evaluation_results['ensemble'] = self.ensemble_predictions['ensemble_metrics']
            print(f"é›†æˆæ¨¡å‹æ€§èƒ½: {self.ensemble_predictions['ensemble_metrics']}")
        
        return evaluation_results
    
    def _get_model_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹æ€§èƒ½æ‘˜è¦"""
        summary = {}
        
        for level, model in self.models.items():
            if model.is_trained:
                summary[f'level_{level}'] = {
                    'model_type': model.fidelity_level.model_type,
                    'training_completed': model.is_trained,
                    'validation_metrics': model.validation_metrics,
                    'training_history_length': len(model.training_history)
                }
        
        return summary
    
    def predict_with_ensemble(self, X: np.ndarray) -> Dict[str, np.ndarray]:
        """ä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        if not self.models:
            raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        
        predictions = {}
        
        # è·å–å„ä¸ªæ¨¡å‹çš„é¢„æµ‹
        for level, model in self.models.items():
            if model.is_trained:
                try:
                    pred = model.predict(X)
                    predictions[f'level_{level}'] = pred
                except Exception as e:
                    warnings.warn(f"ä¿çœŸåº¦çº§åˆ« {level} é¢„æµ‹å¤±è´¥: {str(e)}")
        
        # é›†æˆé¢„æµ‹
        if len(predictions) > 1:
            if self.config.co_training.ensemble_method == 'weighted_average':
                ensemble_pred = self._weighted_average_ensemble(predictions)
            elif self.config.co_training.ensemble_method == 'stacking':
                # å¯¹äºæ–°æ•°æ®ï¼Œä½¿ç”¨ç®€å•å¹³å‡
                ensemble_pred = self._simple_average_ensemble(predictions)
            else:
                ensemble_pred = self._simple_average_ensemble(predictions)
            
            predictions['ensemble'] = ensemble_pred
        
        return predictions


def create_multi_fidelity_system(config: MultiFidelityConfig) -> MultiFidelityTrainer:
    """åˆ›å»ºå¤šä¿çœŸåº¦ç³»ç»Ÿ"""
    return MultiFidelityTrainer(config)


def demo_multi_fidelity():
    """æ¼”ç¤ºå¤šä¿çœŸåº¦å»ºæ¨¡åŠŸèƒ½"""
    print("=== å¤šä¿çœŸåº¦å»ºæ¨¡æ¼”ç¤º ===")
    
    # 1. åˆ›å»ºé…ç½®
    try:
        # å®šä¹‰ä¿çœŸåº¦çº§åˆ«
        low_fidelity = FidelityLevel(
            name="ä½ç²¾åº¦å¿«é€Ÿä»¿çœŸ",
            level=0,
            description="ä½¿ç”¨ç®€åŒ–PDEçš„å¿«é€Ÿä»¿çœŸ",
            computational_cost=1.0,
            accuracy=0.7,
            data_requirements=1000,
            training_time=60.0,
            model_type="neural_network",
            model_params={
                'hidden_layers': [32, 16],
                'activation': 'relu',
                'dropout': 0.1
            },
            training_params={
                'epochs': 100,
                'batch_size': 32,
                'learning_rate': 0.001
            }
        )
        
        high_fidelity = FidelityLevel(
            name="é«˜ç²¾åº¦å®Œæ•´ä»¿çœŸ",
            level=1,
            description="ä½¿ç”¨å®Œæ•´ç‰©ç†æ¨¡å‹çš„ç²¾ç¡®ä»¿çœŸ",
            computational_cost=10.0,
            accuracy=0.95,
            data_requirements=5000,
            training_time=300.0,
            model_type="neural_network",
            model_params={
                'hidden_layers': [128, 64, 32],
                'activation': 'relu',
                'dropout': 0.2
            },
            training_params={
                'epochs': 200,
                'batch_size': 16,
                'learning_rate': 0.0005
            }
        )
        
        config = MultiFidelityConfig(
            name="æ²¹è—æ¨¡æ‹Ÿå¤šä¿çœŸåº¦ç³»ç»Ÿ",
            description="ç»“åˆå¿«é€Ÿå’Œç²¾ç¡®ä»¿çœŸçš„æ²¹è—é¢„æµ‹ç³»ç»Ÿ",
            fidelity_levels=[low_fidelity, high_fidelity],
            co_training=MultiFidelityConfig.co_training(
                enabled=True,
                transfer_learning=True,
                knowledge_distillation=True,
                ensemble_method='weighted_average'
            ),
            training_strategy=MultiFidelityConfig.training_strategy(
                stage1_epochs=100,
                stage2_epochs=50,
                transfer_epochs=20,
                distillation_epochs=10
            )
        )
        
        print("âœ… åˆ›å»ºå¤šä¿çœŸåº¦é…ç½®")
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºé…ç½®å¤±è´¥: {e}")
        return
    
    # 2. åˆ›å»ºç³»ç»Ÿ
    try:
        trainer = create_multi_fidelity_system(config)
        print("âœ… åˆ›å»ºå¤šä¿çœŸåº¦è®­ç»ƒå™¨")
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºè®­ç»ƒå™¨å¤±è´¥: {e}")
        return
    
    # 3. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    try:
        np.random.seed(42)
        
        # ä½ä¿çœŸåº¦æ•°æ®
        X_low = np.random.randn(1000, 5)  # 5ä¸ªè¾“å…¥ç‰¹å¾
        y_low = np.random.randn(1000, 2)  # 2ä¸ªè¾“å‡ºç›®æ ‡
        
        # é«˜ä¿çœŸåº¦æ•°æ®
        X_high = np.random.randn(5000, 5)
        y_high = np.random.randn(5000, 2)
        
        # æ·»åŠ éªŒè¯æ•°æ®
        X_val_low = np.random.randn(200, 5)
        y_val_low = np.random.randn(200, 2)
        
        X_val_high = np.random.randn(1000, 5)
        y_val_high = np.random.randn(1000, 2)
        
        # æ·»åŠ åˆ°è®­ç»ƒå™¨
        trainer.add_training_data(0, X_low, y_low)
        trainer.add_training_data(1, X_high, y_high)
        trainer.add_validation_data(0, X_val_low, y_val_low)
        trainer.add_validation_data(1, X_val_high, y_val_high)
        
        print("âœ… æ·»åŠ è®­ç»ƒå’ŒéªŒè¯æ•°æ®")
        
    except Exception as e:
        print(f"âŒ æ·»åŠ æ•°æ®å¤±è´¥: {e}")
        return
    
    # 4. è¿è¡Œè®­ç»ƒ
    try:
        training_summary = trainer.run_full_training(input_dim=5, output_dim=2)
        print("âœ… è®­ç»ƒå®Œæˆ")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        return
    
    # 5. æµ‹è¯•é¢„æµ‹
    try:
        X_test = np.random.randn(100, 5)
        predictions = trainer.predict_with_ensemble(X_test)
        
        print(f"âœ… é›†æˆé¢„æµ‹å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {predictions['ensemble'].shape}")
        
    except Exception as e:
        print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
    
    print("\nğŸ‰ å¤šä¿çœŸåº¦å»ºæ¨¡æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    demo_multi_fidelity()
